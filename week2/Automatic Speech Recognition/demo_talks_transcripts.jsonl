{"video_id": "talk_01", "url": "https://www.youtube.com/watch?v=EXAMPLE1", "timestamp": "2024-01-15T10:30:00", "asr_transcript": {"video_id": "talk_01", "language": "en", "text": "Hello everyone, welcome to today's presentation on transformer architectures. In this talk, we'll explore the attention mechanism and its applications in natural language processing.", "segments": [{"start": 0.0, "end": 3.5, "text": "Hello everyone, welcome to today's presentation on transformer architectures."}, {"start": 3.5, "end": 8.2, "text": "In this talk, we'll explore the attention mechanism and its applications in natural language processing."}, {"start": 8.2, "end": 12.1, "text": "The transformer model has revolutionized the field of machine learning."}]}, "ocr_extractions": [{"image_path": "/tmp/talk_01_frame_0001.png", "text": "Transformer Architecture\n• Self-Attention Mechanism\n• Multi-Head Attention\n• Position Encoding", "timestamp": "0001"}, {"image_path": "/tmp/talk_01_frame_0002.png", "text": "Applications in NLP\n• Machine Translation\n• Text Summarization\n• Question Answering", "timestamp": "0002"}], "processing_info": {"audio_duration": 180.5, "total_segments": 45, "total_ocr_extractions": 6}}
{"video_id": "talk_02", "url": "https://www.youtube.com/watch?v=EXAMPLE2", "timestamp": "2024-01-15T10:45:00", "asr_transcript": {"video_id": "talk_02", "language": "en", "text": "Today we're discussing BERT and the importance of pre-training in language models. BERT uses bidirectional training to better understand context.", "segments": [{"start": 0.0, "end": 4.2, "text": "Today we're discussing BERT and the importance of pre-training in language models."}, {"start": 4.2, "end": 8.7, "text": "BERT uses bidirectional training to better understand context."}, {"start": 8.7, "end": 13.1, "text": "This approach has shown significant improvements in various NLP tasks."}]}, "ocr_extractions": [{"image_path": "/tmp/talk_02_frame_0001.png", "text": "BERT: Bidirectional Encoder Representations from Transformers\n• Masked Language Modeling\n• Next Sentence Prediction", "timestamp": "0001"}], "processing_info": {"audio_duration": 175.3, "total_segments": 42, "total_ocr_extractions": 4}}
