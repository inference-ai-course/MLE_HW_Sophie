{"video_id": "talk_01", "url": "https://www.youtube.com/watch?v=aircAruvnKk  # 1. But what is a neural network? | Deep learning chapter 1", "timestamp": "2025-07-31T16:59:01.785934", "asr_transcript": {"video_id": "talk_01", "language": "en", "text": " This is a 3. It's slobily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this, and outputs a single number between 0 and 10, telling you what it thinks the digit is, while the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing. Not as a buzzword, but as a piece of math. My hope is just that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic. And I'm happy to stick with the status quo here, because at the end of the two videos, I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many, many variants of neural networks, and in recent years, there's been sort of a boom in research towards these variants. But in these two introductory videos, you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me, it still has plenty of complexity for us to wrap our minds around. But even in this simplest form, it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time, you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests, neural networks are inspired by the brain, but let's break that down. What are the neurons and in what sense are they linked together? Right now, when I say neuron, all I want you to think about is a thing that holds a number, specifically a number between zero and one. It's really not more than that. For example, the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the gray scale value of the corresponding pixel, ranging from zero for black pixels up to one for white pixels. This number inside the neuron is called its activation. And the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again, some number that's between zero and one, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark. For how on earth this process of recognizing digits is going to be handled. In this network, I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest, I chose two layers based on how I want to motivate the structure in just a moment. And 16, well that was just a nice number to fit on the screen. In practice, there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer, determine the activations of the next layer. And of course, the heart of the network, as an information processing mechanism, comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits. And let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer, according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer, which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. And before jumping into the math for how one layer influences the next or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently. What are we expecting here? What is the best hope for what those middle layers might be doing? Well, when you or I recognize digits, you piece together various components. A 9 has a loop up top and a line on the right. And 8 also has a loop up top, but it's paired with another loop down low. A 4 basically breaks down into 3 specific lines and things like that. Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents. That anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1. And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron. That way, going from the third layer to the last one, just requires learning which combination of subcomponents corresponds to which digits. Of course, that just kicks the problem down the road because how would you recognize these subcomponents or even learn what the right subcomponents should be? And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment. Recognizing a loop can also break down into subproblems. One reasonable way to do this would be to first recognize the various little edges that make it up. Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, well that's really just a long edge, or maybe you think of it as a certain pattern of several smaller edges. So maybe, our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges. Maybe, when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9. Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network. But this is a hope that we might have, a sort of goal with the layered structure like this. Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks. And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction. Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc. But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the activations in the next. The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here. The question at hand is what parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop and other such things? Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer. These weights are just numbers. Then take all of those activations from the first layer and compute their weighted sum according to these weights. I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weights value. Now if we made the weights associated with almost all of the pixels zero, except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about. And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels. Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker. When you compute a weighted sum like this, you might come out with any number, but for this network, what we want is for activations to be some value between zero and one. So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between zero and one. And a common function that does this is called the sigmoid function, also known as a logistic curve, basically very negative inputs end up close to zero, very positive inputs end up close to one, and it just steadily increases around the input zero. So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is. But maybe it's not that you want the neuron to light up when the weighted sum is bigger than zero. Maybe you only want it to be active when the sum is bigger than say 10. That is, you want some bias for it to be inactive. What we'll do then is just add in some other number, like negative 10, to this weighted sum, before plugging it through the sigmoid squishing function. That additional number is called the bias. So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active. And that is just one neuron. Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it. Also, each one has some bias, some other number that you add onto the weighted sum before squishing it with the sigmoid. And that's a lot to think about. With this hidden layer of 16 neurons, that's a total of 784 x 16 weights, along with 16 biases. And all of that is just the connections from the first layer to the second. The connections between the other layers also have a bunch of weights and biases associated with them. All said and done, this network has almost exactly 13,000 total weights and biases. 13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways. So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many, many numbers so that it'll actually solve the problem at hand. One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc. I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve. Or when the network does work, but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions. By the way, the actual function here is a little cumbersome to write down, don't you think? So let me show you a more notationally compact way that these connections are represented. This is how you'd see it if you choose to read out more about neural networks. Organize all of the activations from one layer into a column as a vector. Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer. What that means is that taking the weighted sum of the activations in the first layer, according to these weights, corresponds to one of the terms in the matrix vector product of everything we have on the left here. By the way, so much of machine learning just comes down to having a good grasp of linear algebra. So for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter three. Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector and adding the entire vector to the previous matrix vector product. Then as a final step, I'll wrap a sigmoid around the outside here. And what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside. So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression. And this makes the relevant code both a lot simpler and a lot faster since many libraries optimize the heck out of matrix multiplication. Remember how earlier I said these neurons are simply things that hold numbers? Well, of course, the specific numbers that they hold depends on the image you feed in. So it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1. Really, the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output. It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up uncertain patterns and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless. And in a way, it's kind of reassuring that it looks complicated. I mean, if there were any simpler, what hope would we have that it could take on the challenge of recognizing digits? And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh, that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really going to be doing. Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically, most of you don't actually receive notifications from YouTube, do you? Maybe more honestly, I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you. Anyway, stay posted for more. Thank you very much to everyone supporting these videos on Patreon. I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons, you can look out for updates there. To close things off here, I have with me, Lisa Lee, who did her PhD work on the theoretical side of deep learning, and who currently works at a venture capital firm called Amplify Partners, who kindly provided some of the funding for this video. So, Lisa, one thing I think we should quickly bring up is this sigmoid function. As I understand it, early networks use this to squish the relevant weighted sum into that interval between zero and one, kind of motivated by this biological analogy of neurons either being inactive or active. Exactly. But relatively few modern networks actually use sigmoid anymore. It's kind of old school, right? Yeah, or rather, rel U seems to be much easier to train. And rel U stands for rectified linear unit? Yes, it's this kind of function where you're just taking a max of zero and A, where A is given by what you were explaining in the video. And what this was sort of motivated from, I think, was partially by a biological analogy with how neurons would either be activated or not. And so if it passes a certain threshold, it would be the identity function. But if it did not, then it would just not be activated, so be zero. So it's kind of a simplification. Using sigmoids didn't help training or it was very difficult to train. It's at some point and people just tried rel U and it happened to work very well for these incredibly deep neural networks. All right. Thank you, Lisa.", "segments": [{"start": 0.0, "end": 11.16, "text": "This is a 3. It's slobily written and rendered at an extremely low resolution of 28x28 pixels,"}, {"start": 11.16, "end": 15.36, "text": "but your brain has no trouble recognizing it as a 3. And I want you to take a moment to"}, {"start": 15.36, "end": 20.5, "text": "appreciate how crazy it is that brains can do this so effortlessly. I mean this, this"}, {"start": 20.5, "end": 26.16, "text": "and this are also recognizable as 3s, even though the specific values of each pixel"}, {"start": 26.16, "end": 31.04, "text": "is very different from one image to the next. The particular light-sensitive cells in"}, {"start": 31.04, "end": 35.72, "text": "your eye that are firing when you see this 3 are very different from the ones firing"}, {"start": 35.72, "end": 41.6, "text": "when you see this 3. But something in that crazy smart visual cortex of yours resolves"}, {"start": 41.6, "end": 46.74, "text": "these as representing the same idea, while at the same time recognizing other images as"}, {"start": 46.74, "end": 52.96, "text": "their own distinct ideas. But if I told you, hey, sit down and write for me a program"}, {"start": 52.96, "end": 59.14, "text": "that takes in a grid of 28x28 pixels like this, and outputs a single number between 0"}, {"start": 59.14, "end": 64.64, "text": "and 10, telling you what it thinks the digit is, while the task goes from comically trivial"}, {"start": 64.64, "end": 69.44, "text": "to dauntingly difficult. Unless you've been living under a rock, I think I hardly need"}, {"start": 69.44, "end": 73.5, "text": "to motivate the relevance and importance of machine learning and neural networks to"}, {"start": 73.5, "end": 77.72, "text": "the present and to the future. But what I want to do here is show you what a neural network"}, {"start": 77.72, "end": 82.64, "text": "actually is, assuming no background, and to help visualize what it's doing. Not as a"}, {"start": 82.64, "end": 87.0, "text": "buzzword, but as a piece of math. My hope is just that you come away feeling like the"}, {"start": 87.0, "end": 91.6, "text": "structure itself is motivated, and to feel like you know what it means when you read, or"}, {"start": 91.6, "end": 96.8, "text": "you hear about a neural network quote unquote learning. This video is just going to be devoted"}, {"start": 96.8, "end": 101.0, "text": "to the structure component of that, and the following one is going to tackle learning."}, {"start": 101.0, "end": 105.48, "text": "What we're going to do is put together a neural network that can learn to recognize handwritten"}, {"start": 105.48, "end": 111.96000000000001, "text": "digits. This is a somewhat classic example for introducing"}, {"start": 111.96, "end": 115.64, "text": "the topic. And I'm happy to stick with the status quo here, because at the end of the"}, {"start": 115.64, "end": 119.52, "text": "two videos, I want to point you to a couple good resources where you can learn more, and"}, {"start": 119.52, "end": 125.24, "text": "where you can download the code that does this and play with it on your own computer."}, {"start": 125.24, "end": 129.64, "text": "There are many, many variants of neural networks, and in recent years, there's been sort"}, {"start": 129.64, "end": 135.0, "text": "of a boom in research towards these variants. But in these two introductory videos, you and"}, {"start": 135.0, "end": 139.84, "text": "I are just going to look at the simplest plain vanilla form with no added frills. This"}, {"start": 139.84, "end": 144.88, "text": "is kind of a necessary prerequisite for understanding any of the more powerful modern variants,"}, {"start": 144.88, "end": 149.36, "text": "and trust me, it still has plenty of complexity for us to wrap our minds around. But even"}, {"start": 149.36, "end": 154.16, "text": "in this simplest form, it can learn to recognize handwritten digits, which is a pretty cool"}, {"start": 154.16, "end": 159.64000000000001, "text": "thing for a computer to be able to do. And at the same time, you'll see how it does fall"}, {"start": 159.64000000000001, "end": 163.64000000000001, "text": "short of a couple hopes that we might have for it."}, {"start": 163.64000000000001, "end": 168.08, "text": "As the name suggests, neural networks are inspired by the brain, but let's break that"}, {"start": 168.08, "end": 173.20000000000002, "text": "down. What are the neurons and in what sense are they linked together? Right now, when"}, {"start": 173.20000000000002, "end": 178.92000000000002, "text": "I say neuron, all I want you to think about is a thing that holds a number, specifically"}, {"start": 178.92000000000002, "end": 183.88000000000002, "text": "a number between zero and one. It's really not more than that."}, {"start": 183.88000000000002, "end": 188.72000000000003, "text": "For example, the network starts with a bunch of neurons corresponding to each of the 28"}, {"start": 188.72000000000003, "end": 195.48000000000002, "text": "times 28 pixels of the input image, which is 784 neurons in total. Each one of these"}, {"start": 195.48, "end": 201.2, "text": "holds a number that represents the gray scale value of the corresponding pixel, ranging"}, {"start": 201.2, "end": 207.39999999999998, "text": "from zero for black pixels up to one for white pixels. This number inside the neuron is called"}, {"start": 207.39999999999998, "end": 211.92, "text": "its activation. And the image you might have in mind here is that each neuron is lit"}, {"start": 211.92, "end": 216.72, "text": "up when its activation is a high number."}, {"start": 216.72, "end": 226.44, "text": "So all of these 784 neurons make up the first layer of our network."}, {"start": 226.44, "end": 230.88, "text": "Now jumping over to the last layer, this has 10 neurons, each representing one of the"}, {"start": 230.88, "end": 237.16, "text": "digits. The activation in these neurons, again, some number that's between zero and one,"}, {"start": 237.16, "end": 243.0, "text": "represents how much the system thinks that a given image corresponds with a given digit."}, {"start": 243.0, "end": 248.12, "text": "There's also a couple layers in between called the hidden layers, which for the time being"}, {"start": 248.12, "end": 252.56, "text": "should just be a giant question mark. For how on earth this process of recognizing digits"}, {"start": 252.56, "end": 258.32, "text": "is going to be handled. In this network, I chose two hidden layers, each one with 16 neurons,"}, {"start": 258.32, "end": 262.56, "text": "and admittedly that's kind of an arbitrary choice. To be honest, I chose two layers based"}, {"start": 262.56, "end": 266.88, "text": "on how I want to motivate the structure in just a moment. And 16, well that was just"}, {"start": 266.88, "end": 271.04, "text": "a nice number to fit on the screen. In practice, there is a lot of room for experiment with"}, {"start": 271.04, "end": 276.88, "text": "a specific structure here. The way the network operates, activations in one layer, determine"}, {"start": 276.88, "end": 281.72, "text": "the activations of the next layer. And of course, the heart of the network, as an information"}, {"start": 281.72, "end": 286.88, "text": "processing mechanism, comes down to exactly how those activations from one layer bring"}, {"start": 286.88, "end": 292.44, "text": "about activations in the next layer. It's meant to be loosely analogous to how in biological"}, {"start": 292.44, "end": 298.08000000000004, "text": "networks of neurons, some groups of neurons firing cause certain others to fire. Now"}, {"start": 298.08, "end": 302.15999999999997, "text": "the network I'm showing here has already been trained to recognize digits. And let me"}, {"start": 302.15999999999997, "end": 307.8, "text": "show you what I mean by that. It means if you feed in an image, lighting up all 784"}, {"start": 307.8, "end": 312.96, "text": "neurons of the input layer, according to the brightness of each pixel in the image, that"}, {"start": 312.96, "end": 318.28, "text": "pattern of activations causes some very specific pattern in the next layer, which causes some"}, {"start": 318.28, "end": 322.76, "text": "pattern in the one after it, which finally gives some pattern in the output layer. And"}, {"start": 322.76, "end": 327.8, "text": "the brightest neuron of that output layer is the network's choice, so to speak, for"}, {"start": 327.8, "end": 335.36, "text": "what digit this image represents. And before jumping into the math for how one layer influences"}, {"start": 335.36, "end": 340.32, "text": "the next or how training works, let's just talk about why it's even reasonable to expect"}, {"start": 340.32, "end": 346.08, "text": "a layered structure like this to behave intelligently. What are we expecting here? What is the best"}, {"start": 346.08, "end": 351.96, "text": "hope for what those middle layers might be doing? Well, when you or I recognize digits,"}, {"start": 351.96, "end": 357.44, "text": "you piece together various components. A 9 has a loop up top and a line on the right."}, {"start": 357.44, "end": 363.38, "text": "And 8 also has a loop up top, but it's paired with another loop down low. A 4 basically"}, {"start": 363.38, "end": 369.28, "text": "breaks down into 3 specific lines and things like that. Now in a perfect world, we might"}, {"start": 369.28, "end": 375.4, "text": "hope that each neuron in the second to last layer corresponds with one of these subcomponents."}, {"start": 375.4, "end": 380.4, "text": "That anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's"}, {"start": 380.4, "end": 385.21999999999997, "text": "some specific neuron whose activation is going to be close to 1. And I don't mean this"}, {"start": 385.21999999999997, "end": 389.84, "text": "specific loop of pixels, the hope would be that any generally loopy pattern towards the"}, {"start": 389.84, "end": 395.56, "text": "top sets off this neuron. That way, going from the third layer to the last one, just"}, {"start": 395.56, "end": 401.59999999999997, "text": "requires learning which combination of subcomponents corresponds to which digits. Of course, that"}, {"start": 401.59999999999997, "end": 405.44, "text": "just kicks the problem down the road because how would you recognize these subcomponents"}, {"start": 405.44, "end": 408.91999999999996, "text": "or even learn what the right subcomponents should be? And I still haven't even talked"}, {"start": 408.92, "end": 414.04, "text": "about how one layer influences the next, but run with me on this one for a moment."}, {"start": 414.04, "end": 418.8, "text": "Recognizing a loop can also break down into subproblems. One reasonable way to do this"}, {"start": 418.8, "end": 424.72, "text": "would be to first recognize the various little edges that make it up. Similarly, a long"}, {"start": 424.72, "end": 430.0, "text": "line, like the kind you might see in the digits 1 or 4 or 7, well that's really just a long"}, {"start": 430.0, "end": 436.84000000000003, "text": "edge, or maybe you think of it as a certain pattern of several smaller edges. So maybe,"}, {"start": 436.84, "end": 441.0, "text": "our hope is that each neuron in the second layer of the network corresponds with the"}, {"start": 441.0, "end": 447.0, "text": "various relevant little edges. Maybe, when an image like this one comes in, it lights"}, {"start": 447.0, "end": 453.02, "text": "up all of the neurons associated with around 8 to 10 specific little edges, which in turn"}, {"start": 453.02, "end": 457.84, "text": "lights up the neurons associated with the upper loop and a long vertical line, and those"}, {"start": 457.84, "end": 462.59999999999997, "text": "light up the neuron associated with a 9. Whether or not this is what our final network actually"}, {"start": 462.6, "end": 467.84000000000003, "text": "does is another question, one that I'll come back to once we see how to train the network."}, {"start": 467.84000000000003, "end": 471.96000000000004, "text": "But this is a hope that we might have, a sort of goal with the layered structure like"}, {"start": 471.96000000000004, "end": 477.32000000000005, "text": "this. Moreover, you can imagine how being able to detect edges and patterns like this"}, {"start": 477.32000000000005, "end": 482.68, "text": "would be really useful for other image recognition tasks. And even beyond image recognition,"}, {"start": 482.68, "end": 486.48, "text": "there are all sorts of intelligent things you might want to do that break down into layers"}, {"start": 486.48, "end": 492.08000000000004, "text": "of abstraction. Parsing speech, for example, involves taking raw audio and picking out distinct"}, {"start": 492.08, "end": 497.47999999999996, "text": "sounds, which combine to make certain syllables, which combine to form words, which combine"}, {"start": 497.47999999999996, "end": 503.08, "text": "to make up phrases and more abstract thoughts, etc. But getting back to how any of this actually"}, {"start": 503.08, "end": 508.56, "text": "works, picture yourself right now designing how exactly the activations in one layer might"}, {"start": 508.56, "end": 513.64, "text": "determine the activations in the next. The goal is to have some mechanism that could"}, {"start": 513.64, "end": 519.4399999999999, "text": "conceivably combine pixels into edges, or edges into patterns, or patterns into digits."}, {"start": 519.44, "end": 524.44, "text": "And to zoom in on one very specific example, let's say the hope is for one particular"}, {"start": 524.44, "end": 530.12, "text": "neuron in the second layer to pick up on whether or not the image has an edge in this region"}, {"start": 530.12, "end": 537.0, "text": "here. The question at hand is what parameters should the network have? What dials and knobs"}, {"start": 537.0, "end": 541.6400000000001, "text": "should you be able to tweak so that it's expressive enough to potentially capture this"}, {"start": 541.6400000000001, "end": 546.6, "text": "pattern, or any other pixel pattern, or the pattern that several edges can make a loop"}, {"start": 546.6, "end": 552.52, "text": "and other such things? Well, what we'll do is assign a weight to each one of the connections"}, {"start": 552.52, "end": 558.72, "text": "between our neuron and the neurons from the first layer. These weights are just numbers."}, {"start": 558.72, "end": 564.28, "text": "Then take all of those activations from the first layer and compute their weighted sum"}, {"start": 564.28, "end": 570.0400000000001, "text": "according to these weights. I find it helpful to think of these weights as being organized"}, {"start": 570.0400000000001, "end": 574.64, "text": "into a little grid of their own, and I'm going to use green pixels to indicate positive"}, {"start": 574.64, "end": 579.08, "text": "weights and red pixels to indicate negative weights, where the brightness of that pixel"}, {"start": 579.08, "end": 584.48, "text": "is some loose depiction of the weights value. Now if we made the weights associated with"}, {"start": 584.48, "end": 588.8, "text": "almost all of the pixels zero, except for some positive weights in this region that we"}, {"start": 588.8, "end": 593.88, "text": "care about, then taking the weighted sum of all the pixel values really just amounts to"}, {"start": 593.88, "end": 599.64, "text": "adding up the values of the pixel just in the region that we care about. And if you really"}, {"start": 599.64, "end": 604.16, "text": "wanted to pick up on whether there's an edge here, what you might do is have some negative"}, {"start": 604.16, "end": 609.76, "text": "weights associated with the surrounding pixels. Then the sum is largest when those middle"}, {"start": 609.76, "end": 615.6, "text": "pixels are bright, but the surrounding pixels are darker. When you compute a weighted"}, {"start": 615.6, "end": 620.24, "text": "sum like this, you might come out with any number, but for this network, what we want"}, {"start": 620.24, "end": 625.4, "text": "is for activations to be some value between zero and one. So a common thing to do is to"}, {"start": 625.4, "end": 630.6, "text": "pump this weighted sum into some function that squishes the real number line into the range"}, {"start": 630.6, "end": 635.9599999999999, "text": "between zero and one. And a common function that does this is called the sigmoid function,"}, {"start": 635.9599999999999, "end": 641.52, "text": "also known as a logistic curve, basically very negative inputs end up close to zero, very"}, {"start": 641.52, "end": 646.0799999999999, "text": "positive inputs end up close to one, and it just steadily increases around the input"}, {"start": 646.0799999999999, "end": 654.64, "text": "zero. So the activation of the neuron here is basically a measure of how positive the"}, {"start": 654.64, "end": 660.4, "text": "relevant weighted sum is. But maybe it's not that you want the neuron to light up when"}, {"start": 660.4, "end": 664.76, "text": "the weighted sum is bigger than zero. Maybe you only want it to be active when the sum"}, {"start": 664.76, "end": 671.4399999999999, "text": "is bigger than say 10. That is, you want some bias for it to be inactive. What we'll"}, {"start": 671.4399999999999, "end": 677.04, "text": "do then is just add in some other number, like negative 10, to this weighted sum, before"}, {"start": 677.04, "end": 682.12, "text": "plugging it through the sigmoid squishing function. That additional number is called the"}, {"start": 682.12, "end": 687.76, "text": "bias. So the weights tell you what pixel pattern this neuron in the second layer is picking"}, {"start": 687.76, "end": 693.72, "text": "up on, and the bias tells you how high the weighted sum needs to be before the neuron starts"}, {"start": 693.72, "end": 700.64, "text": "getting meaningfully active. And that is just one neuron. Every other neuron in this layer"}, {"start": 700.64, "end": 706.28, "text": "is going to be connected to all 784 pixel neurons from the first layer, and each one of"}, {"start": 706.28, "end": 713.6, "text": "those 784 connections has its own weight associated with it. Also, each one has some bias,"}, {"start": 713.6, "end": 718.4, "text": "some other number that you add onto the weighted sum before squishing it with the sigmoid. And"}, {"start": 718.4, "end": 723.28, "text": "that's a lot to think about. With this hidden layer of 16 neurons, that's a total of"}, {"start": 723.28, "end": 730.64, "text": "784 x 16 weights, along with 16 biases. And all of that is just the connections from"}, {"start": 730.64, "end": 735.16, "text": "the first layer to the second. The connections between the other layers also have a bunch"}, {"start": 735.16, "end": 740.52, "text": "of weights and biases associated with them. All said and done, this network has almost"}, {"start": 740.52, "end": 747.1999999999999, "text": "exactly 13,000 total weights and biases. 13,000 knobs and dials that can be tweaked and"}, {"start": 747.1999999999999, "end": 753.04, "text": "turned to make this network behave in different ways. So when we talk about learning, what"}, {"start": 753.04, "end": 757.8399999999999, "text": "that's referring to is getting the computer to find a valid setting for all of these"}, {"start": 757.8399999999999, "end": 763.56, "text": "many, many numbers so that it'll actually solve the problem at hand. One thought experiment"}, {"start": 763.56, "end": 768.2399999999999, "text": "that is at once fun and kind of horrifying is to imagine sitting down and setting all"}, {"start": 768.2399999999999, "end": 773.0, "text": "of these weights and biases by hand, purposefully tweaking the numbers so that the second layer"}, {"start": 773.0, "end": 778.1999999999999, "text": "picks up on edges, the third layer picks up on patterns, etc. I personally find this"}, {"start": 778.1999999999999, "end": 782.8, "text": "satisfying rather than just treating the network as a total black box, because when the"}, {"start": 782.8, "end": 787.5999999999999, "text": "network doesn't perform the way you anticipate, if you've built up a little bit of a relationship"}, {"start": 787.5999999999999, "end": 792.1199999999999, "text": "with what those weights and biases actually mean, you have a starting place for experimenting"}, {"start": 792.2, "end": 796.96, "text": "with how to change the structure to improve. Or when the network does work, but not for"}, {"start": 796.96, "end": 801.52, "text": "the reasons you might expect, digging into what the weights and biases are doing is a good"}, {"start": 801.52, "end": 806.76, "text": "way to challenge your assumptions and really expose the full space of possible solutions."}, {"start": 806.76, "end": 812.8, "text": "By the way, the actual function here is a little cumbersome to write down, don't you think?"}, {"start": 812.8, "end": 817.76, "text": "So let me show you a more notationally compact way that these connections are represented."}, {"start": 817.76, "end": 821.72, "text": "This is how you'd see it if you choose to read out more about neural networks. Organize"}, {"start": 821.8000000000001, "end": 829.0400000000001, "text": "all of the activations from one layer into a column as a vector. Then organize all of the"}, {"start": 829.0400000000001, "end": 834.44, "text": "weights as a matrix, where each row of that matrix corresponds to the connections between"}, {"start": 834.44, "end": 839.88, "text": "one layer and a particular neuron in the next layer. What that means is that taking the"}, {"start": 839.88, "end": 844.48, "text": "weighted sum of the activations in the first layer, according to these weights, corresponds"}, {"start": 844.48, "end": 849.48, "text": "to one of the terms in the matrix vector product of everything we have on the left here."}, {"start": 852.72, "end": 857.84, "text": "By the way, so much of machine learning just comes down to having a good grasp of linear"}, {"start": 857.84, "end": 862.32, "text": "algebra. So for any of you who want a nice visual understanding for matrices and what"}, {"start": 862.32, "end": 867.32, "text": "matrix vector multiplication means, take a look at the series I did on linear algebra,"}, {"start": 867.32, "end": 872.08, "text": "especially chapter three. Back to our expression, instead of talking about adding the bias"}, {"start": 872.08, "end": 877.6, "text": "to each one of these values independently, we represent it by organizing all those biases"}, {"start": 877.6, "end": 883.48, "text": "into a vector and adding the entire vector to the previous matrix vector product. Then"}, {"start": 883.48, "end": 888.88, "text": "as a final step, I'll wrap a sigmoid around the outside here. And what that's supposed"}, {"start": 888.88, "end": 893.0, "text": "to represent is that you're going to apply the sigmoid function to each specific component"}, {"start": 893.0, "end": 898.96, "text": "of the resulting vector inside. So once you write down this weight matrix and these vectors"}, {"start": 898.96, "end": 903.84, "text": "as their own symbols, you can communicate the full transition of activations from one"}, {"start": 903.84, "end": 908.8000000000001, "text": "layer to the next in an extremely tight and neat little expression. And this makes the"}, {"start": 908.8000000000001, "end": 914.0400000000001, "text": "relevant code both a lot simpler and a lot faster since many libraries optimize the"}, {"start": 914.0400000000001, "end": 920.24, "text": "heck out of matrix multiplication. Remember how earlier I said these neurons are simply"}, {"start": 920.24, "end": 925.1600000000001, "text": "things that hold numbers? Well, of course, the specific numbers that they hold depends"}, {"start": 925.1600000000001, "end": 931.08, "text": "on the image you feed in. So it's actually more accurate to think of each neuron as a"}, {"start": 931.08, "end": 936.4200000000001, "text": "function, one that takes in the outputs of all the neurons in the previous layer and"}, {"start": 936.4200000000001, "end": 941.76, "text": "spits out a number between 0 and 1. Really, the entire network is just a function, one"}, {"start": 941.76, "end": 948.5600000000001, "text": "that takes in 784 numbers as an input and spits out 10 numbers as an output. It's an absurdly"}, {"start": 948.5600000000001, "end": 953.2800000000001, "text": "complicated function, one that involves 13,000 parameters in the forms of these weights and"}, {"start": 953.2800000000001, "end": 957.72, "text": "biases that pick up uncertain patterns and which involves iterating many matrix vector"}, {"start": 957.72, "end": 963.76, "text": "products and the sigmoid squishification function, but it's just a function nonetheless."}, {"start": 963.76, "end": 967.84, "text": "And in a way, it's kind of reassuring that it looks complicated. I mean, if there were"}, {"start": 967.84, "end": 971.8000000000001, "text": "any simpler, what hope would we have that it could take on the challenge of recognizing"}, {"start": 971.8000000000001, "end": 977.2, "text": "digits? And how does it take on that challenge? How does this network learn the appropriate"}, {"start": 977.2, "end": 982.24, "text": "weights and biases just by looking at data? Oh, that's what I'll show in the next video,"}, {"start": 982.24, "end": 985.88, "text": "and I'll also dig a little more into what this particular network we're seeing is really"}, {"start": 985.88, "end": 987.56, "text": "going to be doing."}, {"start": 987.56, "end": 991.72, "text": "Now is the point I suppose I should say subscribe to stay notified about when that video or any"}, {"start": 991.72, "end": 996.32, "text": "new videos come out, but realistically, most of you don't actually receive notifications"}, {"start": 996.32, "end": 1001.2, "text": "from YouTube, do you? Maybe more honestly, I should say subscribe so that the neural networks"}, {"start": 1001.2, "end": 1005.56, "text": "that underlie YouTube's recommendation algorithm are primed to believe that you want to see"}, {"start": 1005.56, "end": 1010.96, "text": "content from this channel get recommended to you. Anyway, stay posted for more. Thank you"}, {"start": 1010.96, "end": 1015.24, "text": "very much to everyone supporting these videos on Patreon. I've been a little slow to progress"}, {"start": 1015.24, "end": 1019.36, "text": "in the probability series this summer, but I'm jumping back into it after this project,"}, {"start": 1019.36, "end": 1023.76, "text": "so patrons, you can look out for updates there."}, {"start": 1023.76, "end": 1027.92, "text": "To close things off here, I have with me, Lisa Lee, who did her PhD work on the theoretical"}, {"start": 1027.92, "end": 1031.52, "text": "side of deep learning, and who currently works at a venture capital firm called Amplify"}, {"start": 1031.52, "end": 1036.76, "text": "Partners, who kindly provided some of the funding for this video. So, Lisa, one thing I think"}, {"start": 1036.76, "end": 1040.88, "text": "we should quickly bring up is this sigmoid function. As I understand it, early networks"}, {"start": 1040.88, "end": 1045.4, "text": "use this to squish the relevant weighted sum into that interval between zero and one,"}, {"start": 1045.4, "end": 1050.1200000000001, "text": "kind of motivated by this biological analogy of neurons either being inactive or active."}, {"start": 1050.1200000000001, "end": 1054.68, "text": "Exactly. But relatively few modern networks actually use sigmoid anymore. It's kind of"}, {"start": 1054.68, "end": 1059.44, "text": "old school, right? Yeah, or rather, rel U seems to be much easier to train."}, {"start": 1059.44, "end": 1064.7600000000002, "text": "And rel U stands for rectified linear unit? Yes, it's this kind of function where you're"}, {"start": 1064.76, "end": 1070.56, "text": "just taking a max of zero and A, where A is given by what you were explaining in the"}, {"start": 1070.56, "end": 1076.44, "text": "video. And what this was sort of motivated from, I think, was partially by a biological"}, {"start": 1076.44, "end": 1082.84, "text": "analogy with how neurons would either be activated or not. And so if it passes a certain"}, {"start": 1082.84, "end": 1088.12, "text": "threshold, it would be the identity function. But if it did not, then it would just not"}, {"start": 1088.12, "end": 1092.68, "text": "be activated, so be zero. So it's kind of a simplification. Using sigmoids didn't help"}, {"start": 1092.68, "end": 1097.1200000000001, "text": "training or it was very difficult to train. It's at some point and people just tried"}, {"start": 1097.1200000000001, "end": 1104.76, "text": "rel U and it happened to work very well for these incredibly deep neural networks."}, {"start": 1104.76, "end": 1105.88, "text": "All right. Thank you, Lisa."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0001.png", "text": "TC 3", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0002.png", "text": "OREO EE My cro eiSiesioi ares oO\n9", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0003.png", "text": "ay a(Wia, + by) Machine learning.\nes Reo 4 . .", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0004.png", "text": "Q\nH dQ. 2 Q\nZz eo gt ee\nCe Se Se)\na Be Om\nfe Ne ke 6 ae\n¢", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0005.png", "text": "Plain vanilla\n(aka cnultilayer perceptron’ }\n: ‘ ;\nro ae as\na py +e | Se\n‘ 3", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0006.png", "text": "Nevion — Thing that holds a number", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0007.png", "text": "28\na\nCREED GUIISSEIIIUEEL URES 28 x 28 — 784\n289 hehe Us SE Sees sees scene “Activation”", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0009.png", "text": "16 neurons cack", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0011.png", "text": "we: ee", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0012.png", "text": "qe. |\nFe", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0013.png", "text": "Upper loop neuron maybe.\n: ie cghs LTRS\nRE Se", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0014.png", "text": "vo", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0015.png", "text": "4\nme  &§ &\nq", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0017.png", "text": "What are these connections actually doing?\nON\nFu SC", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0018.png", "text": ". What pacirictess shorted exist?\n. nm O17 Tete\n: f: U.0% tial! aa.\n“ y 1 O90 Ase ct\ntd 7 L eg ee\n. P. O75 ne aE\n: ™ O90 Ew A\n: PU .\n: rs 99", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0019.png", "text": "Oyen rag gt yh ray de ee tty\n\ne\n\n* Y: 2.07\n\nes 231\n\n3. CN 3st =\nTd : ne\n\nS7/ 043\n\n+ 20 ,\n\n3 107\n\ne\n\nbad", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0020.png", "text": "Sree asd are ety\nq\nNG\ney\neX\ne :\ne\ne\ne\ncS", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0021.png", "text": "Gye Aa bray be ag +e Gy\nt -3 -2 -1 6 1 2 3 1\nip i\nt -3 mi -1 ¢ 1 2 3 1\n—“——\nActivations shonld be in this range", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0022.png", "text": "Sigimoid\nbe t How positive is this”\nran Ole a. e woag baay + bray)\nry\nee\nG6 Y\ne\ne\ne", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0023.png", "text": "Sigmoid >\nbe t How positive is this\na Ole as rag beds bay 10h)\nCc YY Only activate meaningfully 7\nTod J when wested sua > 10\ne.\ne\ne", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0024.png", "text": ", .-, 734 weights per neuron\n7344: a ! Que bias for each", "timestamp": "0024"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0025.png", "text": "AL ae 3 THEx lo él lfi # Hix LO\nQaIMy eh ch: op 16 +1y +10\n5. cans a aes “\nCp ee ee", "timestamp": "0025"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0026.png", "text": "--Tather ah er er\nG\nfe =\naa i it\nv", "timestamp": "0026"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0027.png", "text": "& 9 Se Se i Ser ea\nTht “dt\nle", "timestamp": "0027"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0028.png", "text": "@ Siginoid\n\nee) ; ‘\n\nay TY oe\n®", "timestamp": "0028"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0029.png", "text": "@ Siginoid\ntd", "timestamp": "0029"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0031.png", "text": "Neuron\nt\n3 Be ae. a munber\nTH 7 = ml", "timestamp": "0031"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0032.png", "text": "e Siguioid\n\neo omer\ngdh)\ne ° an", "timestamp": "0032"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0033.png", "text": "On learning\nns\nfuo3 °\n3 3 2\nSubscribe!", "timestamp": "0033"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0034.png", "text": "Tran Ge Mark Gove\n\nTou Smasnau PATREON | Robert Teed\n\nfesh ‘Tran Jason Hise\n\nAbe Busey Meshal Alshaamiat:\n\nMichael MeC:sffie Betsst Sing\n\nJatin Hales Jaaues Thomntouw\n\nAnkalagon Mustafa Mahdi\n\nFo: Laven't Mathew Bramyon\n\nBons Vewhnovich Jey Ling\n\nJalan Pulgarin Vet\n\nJeff Lanse Shun Kus,\n\nCouper Jones Kish Kundahe\n\nRan Dahl Achille Brighton\nRupta Poser", "timestamp": "0034"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0035.png", "text": "r ) Siginoid\n®", "timestamp": "0035"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0036.png", "text": "Rectified linear unit .\nReLUia} — max(0.a)\n1\na\nrs cr 1 2 3 4\n1", "timestamp": "0036"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_01_frame_0037.png", "text": "Clicky stuffs", "timestamp": "0037"}], "processing_info": {"audio_duration": 1105.88, "total_segments": 212, "total_ocr_extractions": 33}}
{"video_id": "talk_02", "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w  # 2. Gradient descent, how neural networks learn | Chapter 2", "timestamp": "2025-07-31T16:59:46.164650", "asr_transcript": {"video_id": "talk_02", "language": "en", "text": " Last video, I laid out the structure of a neural network. I'll give a quick recap here just so that it's fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, bad output should have activations which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have. And this is what we'll call the cost of a single training example. Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense it's parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input those 13,000 or so weights and biases and it spits out a single number describing how bad those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. And there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead of function with two inputs and one output. You might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It's important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it's a better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it's what I'm going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. And notice, one consequence of that is that it's important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let's zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process, how well does it actually perform on images that it's never seen before? Well the one that I've described here, with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is, I just think there's something incredible about any network doing this well on images that it's never seen before, given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing? Well, for this one at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained training setup. I mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen's efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It applies the entire data set of what the correct classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you're really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I've said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series.", "segments": [{"start": 0.0, "end": 7.36, "text": "Last video, I laid out the structure of a neural network."}, {"start": 7.36, "end": 11.52, "text": "I'll give a quick recap here just so that it's fresh in our minds and then I have two main"}, {"start": 11.52, "end": 13.120000000000001, "text": "goals for this video."}, {"start": 13.120000000000001, "end": 17.68, "text": "The first is to introduce the idea of gradient descent, which underlies not only how neural"}, {"start": 17.68, "end": 21.16, "text": "networks learn, but how a lot of other machine learning works as well."}, {"start": 21.16, "end": 25.0, "text": "Then after that, we're going to dig in a little more to how this particular network performs"}, {"start": 25.0, "end": 29.52, "text": "and what those hidden layers of neurons end up actually looking for."}, {"start": 29.52, "end": 34.68, "text": "As a reminder, our goal here is the classic example of handwritten digit recognition, the"}, {"start": 34.68, "end": 37.04, "text": "Hello World of Neural Networks."}, {"start": 37.04, "end": 42.519999999999996, "text": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between"}, {"start": 42.519999999999996, "end": 44.24, "text": "0 and 1."}, {"start": 44.24, "end": 51.239999999999995, "text": "Those are what determine the activations of 784 neurons in the input layer of the network."}, {"start": 51.239999999999995, "end": 55.480000000000004, "text": "And then the activation for each neuron in the following layers is based on a weighted"}, {"start": 55.48, "end": 62.199999999999996, "text": "sum of all the activations in the previous layer plus some special number called a bias."}, {"start": 62.199999999999996, "end": 66.47999999999999, "text": "Then you compose that sum with some other function, like the sigmoid squishification or"}, {"start": 66.47999999999999, "end": 69.64, "text": "a ray-loo, the way that I walked through last video."}, {"start": 69.64, "end": 74.96, "text": "In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each,"}, {"start": 74.96, "end": 80.96, "text": "the network has about 13,000 weights and biases that we can adjust, and it's these values"}, {"start": 80.96, "end": 85.28, "text": "that determine what exactly the network actually does."}, {"start": 85.28, "end": 89.76, "text": "And what we mean when we say that this network classifies a given digit is that the brightest"}, {"start": 89.76, "end": 94.08, "text": "of those 10 neurons in the final layer corresponds to that digit."}, {"start": 94.08, "end": 98.2, "text": "And remember, the motivation that we had in mind here for the layered structure was that"}, {"start": 98.2, "end": 103.24000000000001, "text": "maybe the second layer could pick up on the edges and the third layer might pick up on"}, {"start": 103.24000000000001, "end": 107.64, "text": "patterns like loops and lines, and the last one could just piece together those patterns"}, {"start": 107.64, "end": 109.8, "text": "to recognize digits."}, {"start": 109.8, "end": 112.88, "text": "So here we learn how the network learns."}, {"start": 112.88, "end": 116.92, "text": "What we want is an algorithm where you can show this network a whole bunch of training"}, {"start": 116.92, "end": 121.56, "text": "data, which comes in the form of a bunch of different images of handwritten digits, along"}, {"start": 121.56, "end": 126.47999999999999, "text": "with labels for what they're supposed to be, and it'll adjust those 13,000 weights and"}, {"start": 126.47999999999999, "end": 130.76, "text": "biases so as to improve its performance on the training data."}, {"start": 130.76, "end": 135.51999999999998, "text": "Hopefully, this layered structure will mean that what it learns generalizes to images"}, {"start": 135.51999999999998, "end": 137.8, "text": "beyond that training data."}, {"start": 137.8, "end": 141.76, "text": "And the way we test that is that after you train the network, you show it more labeled"}, {"start": 141.76, "end": 146.07999999999998, "text": "data, that it's never seen before, and you see how accurately it classifies those new"}, {"start": 146.07999999999998, "end": 149.07999999999998, "text": "images."}, {"start": 149.07999999999998, "end": 154.72, "text": "Fortunately for us, and what makes this such a common example to start with, is that"}, {"start": 154.72, "end": 159.48, "text": "the good people behind the M-NIST database have put together a collection of tens of thousands"}, {"start": 159.48, "end": 165.07999999999998, "text": "of handwritten digit images, each one labeled with the numbers that they're supposed to be."}, {"start": 165.07999999999998, "end": 168.39999999999998, "text": "And it's provocative as it is to describe a machine as learning."}, {"start": 168.4, "end": 172.76000000000002, "text": "Once you actually see how it works, it feels a lot less like some crazy sci-fi premise,"}, {"start": 172.76000000000002, "end": 175.88, "text": "and a lot more like, a calculus exercise."}, {"start": 175.88, "end": 180.88, "text": "I mean, basically it comes down to finding the minimum of a certain function."}, {"start": 180.88, "end": 186.44, "text": "Remember, conceptually, we're thinking of each neuron as being connected to all of"}, {"start": 186.44, "end": 191.44, "text": "the neurons in the previous layer, and the weights in the weighted sum defining its activation"}, {"start": 191.44, "end": 194.64000000000001, "text": "are kind of like the strengths of those connections."}, {"start": 194.64, "end": 199.76, "text": "The bias is some indication of whether that neuron tends to be active or inactive."}, {"start": 199.76, "end": 203.23999999999998, "text": "And to start things off, we're just going to initialize all of those weights and biases"}, {"start": 203.23999999999998, "end": 204.23999999999998, "text": "totally randomly."}, {"start": 204.23999999999998, "end": 208.39999999999998, "text": "Needless to say, this network is going to perform pretty horribly on a given training"}, {"start": 208.39999999999998, "end": 211.16, "text": "example, since it's just doing something random."}, {"start": 211.16, "end": 215.64, "text": "For example, you feed in this image of a three, and the output layer just looks like a"}, {"start": 215.64, "end": 216.79999999999998, "text": "mess."}, {"start": 216.79999999999998, "end": 223.32, "text": "So what you do is you define a cost function, a way of telling the computer, no, bad computer,"}, {"start": 223.32, "end": 228.95999999999998, "text": "bad output should have activations which are zero for most neurons, but one for this neuron."}, {"start": 228.95999999999998, "end": 231.72, "text": "What you gave me is utter trash."}, {"start": 231.72, "end": 236.72, "text": "To say that a little more mathematically, what you do is add up the squares of the differences"}, {"start": 236.72, "end": 241.88, "text": "between each of those trash output activations and the value that you want them to have."}, {"start": 241.88, "end": 246.16, "text": "And this is what we'll call the cost of a single training example."}, {"start": 246.16, "end": 252.64, "text": "Notice this sum is small when the network confidently classifies the image correctly,"}, {"start": 252.64, "end": 258.8, "text": "but it's large when the network seems like it doesn't really know what it's doing."}, {"start": 258.8, "end": 263.88, "text": "So then what you do is consider the average cost over all of the tens of thousands of"}, {"start": 263.88, "end": 267.56, "text": "training examples at your disposal."}, {"start": 267.56, "end": 272.28, "text": "This average cost is our measure for how lousy the network is and how bad the computer should"}, {"start": 272.28, "end": 273.28, "text": "feel."}, {"start": 273.28, "end": 275.28, "text": "And that's a complicated thing."}, {"start": 275.28, "end": 280.44, "text": "Remember how the network itself was basically a function, one that takes in 784 numbers"}, {"start": 280.44, "end": 285.4, "text": "as inputs, the pixel values, and spits out 10 numbers as its output."}, {"start": 285.4, "end": 289.6, "text": "And in a sense it's parameterized by all these weights and biases."}, {"start": 289.6, "end": 293.32, "text": "Well the cost function is a layer of complexity on top of that."}, {"start": 293.32, "end": 299.15999999999997, "text": "It takes as its input those 13,000 or so weights and biases and it spits out a single number"}, {"start": 299.15999999999997, "end": 302.68, "text": "describing how bad those weights and biases are."}, {"start": 302.68, "end": 307.6, "text": "And the way it's defined depends on the network's behavior over all the tens of thousands"}, {"start": 307.6, "end": 309.64, "text": "of pieces of training data."}, {"start": 309.64, "end": 312.44, "text": "That's a lot to think about."}, {"start": 312.44, "end": 316.36, "text": "But just telling the computer what a crappy job it's doing isn't very helpful."}, {"start": 316.36, "end": 321.28, "text": "You want to tell it how to change those weights and biases so that it gets better."}, {"start": 321.28, "end": 325.76, "text": "To make it easier, rather than struggling to imagine a function with 13,000 inputs,"}, {"start": 325.76, "end": 331.44, "text": "just imagine a simple function that has one number as an input and one number as an output."}, {"start": 331.44, "end": 336.44, "text": "How do you find an input that minimizes the value of this function?"}, {"start": 336.44, "end": 340.76, "text": "Circular students will know that you can sometimes figure out that minimum explicitly."}, {"start": 340.76, "end": 344.56, "text": "But that's not always feasible for really complicated functions."}, {"start": 344.56, "end": 349.76, "text": "Certainly not in the 13,000 input version of this situation for our crazy complicated"}, {"start": 349.76, "end": 351.64, "text": "neural network cost function."}, {"start": 351.64, "end": 356.72, "text": "A more flexible tactic is to start at any all input and figure out which direction"}, {"start": 356.72, "end": 360.24, "text": "you should step to make that output lower."}, {"start": 360.24, "end": 365.0, "text": "Specifically, if you can figure out the slope of the function where you are, then shift"}, {"start": 365.08, "end": 372.6, "text": "to the left if that slope is positive and shift the input to the right if that slope is negative."}, {"start": 372.6, "end": 376.52, "text": "If you do this repeatedly, at each point checking the new slope and taking the appropriate"}, {"start": 376.52, "end": 380.64, "text": "step, you're going to approach some local minimum of the function."}, {"start": 380.64, "end": 384.56, "text": "And the image you might have in mind here is a ball rolling down a hill."}, {"start": 384.56, "end": 389.36, "text": "And notice, even for this really simplified single input function, there are many possible"}, {"start": 389.36, "end": 393.88, "text": "valleys that you might land in, depending on which random input you start at."}, {"start": 393.88, "end": 397.56, "text": "And there's no guarantee that the local minimum you land in is going to be the smallest"}, {"start": 397.56, "end": 400.15999999999997, "text": "possible value of the cost function."}, {"start": 400.15999999999997, "end": 403.15999999999997, "text": "That's going to carry over to our neural network case as well."}, {"start": 403.15999999999997, "end": 408.15999999999997, "text": "And I also want you to notice how if you make your step sizes proportional to the slope,"}, {"start": 408.15999999999997, "end": 412.88, "text": "then when the slope is flattening out towards the minimum, your steps get smaller and smaller,"}, {"start": 412.88, "end": 416.12, "text": "and that kind of helps you from overshooting."}, {"start": 416.12, "end": 421.6, "text": "Bumping up the complexity a bit, imagine instead of function with two inputs and one output."}, {"start": 421.6, "end": 425.6, "text": "You might think of the input space as the x, y plane and the cost function as being"}, {"start": 425.6, "end": 428.72, "text": "graft as a surface above it."}, {"start": 428.72, "end": 433.20000000000005, "text": "Now instead of asking about the slope of the function, you have to ask which direction"}, {"start": 433.20000000000005, "end": 438.44, "text": "should you step in this input space so as to decrease the output of the function most"}, {"start": 438.44, "end": 439.76000000000005, "text": "quickly."}, {"start": 439.76000000000005, "end": 441.96000000000004, "text": "In other words, what's the downhill direction?"}, {"start": 441.96000000000004, "end": 446.76000000000005, "text": "And again, it's helpful to think of a ball rolling down that hill."}, {"start": 446.76, "end": 452.28, "text": "Most of you familiar with multivariable calculus will know that the gradient of a function gives"}, {"start": 452.28, "end": 457.48, "text": "you the direction of steepest ascent, basically which direction should you step to increase"}, {"start": 457.48, "end": 459.8, "text": "the function most quickly."}, {"start": 459.8, "end": 463.68, "text": "Naturally enough, taking the negative of that gradient gives you the direction to step"}, {"start": 463.68, "end": 467.28, "text": "that decreases the function most quickly."}, {"start": 467.28, "end": 471.12, "text": "And even more than that, the length of this gradient vector is actually an indication"}, {"start": 471.12, "end": 474.59999999999997, "text": "for just how steep that steepest slope is."}, {"start": 474.6, "end": 478.08000000000004, "text": "Now if you're unfamiliar with multivariable calculus and you want to learn more, check out"}, {"start": 478.08000000000004, "end": 481.0, "text": "some of the work that I did for Khan Academy on the topic."}, {"start": 481.0, "end": 485.68, "text": "Honestly though, all that matters for you and me right now is that in principle there"}, {"start": 485.68, "end": 488.36, "text": "exists a way to compute this vector."}, {"start": 488.36, "end": 492.40000000000003, "text": "This vector that tells you what the downhill direction is and how steep it is."}, {"start": 492.40000000000003, "end": 497.28000000000003, "text": "You'll be okay if that's all you know and you're not rock solid on the details."}, {"start": 497.28000000000003, "end": 501.56, "text": "Because if you can get that, the algorithm for minimizing the function is to compute this"}, {"start": 501.56, "end": 508.28000000000003, "text": "gradient direction, then take a small step downhill and just repeat that over and over."}, {"start": 508.28000000000003, "end": 513.68, "text": "It's the same basic idea for a function that has 13,000 inputs instead of two inputs."}, {"start": 513.68, "end": 520.16, "text": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector."}, {"start": 520.16, "end": 524.44, "text": "The negative gradient of the cost function is just a vector."}, {"start": 524.44, "end": 530.12, "text": "It's some direction inside this insanely huge input space that tells you which nudges"}, {"start": 530.12, "end": 535.88, "text": "to all of those numbers is going to cause the most rapid decrease to the cost function."}, {"start": 535.88, "end": 540.0, "text": "And of course, with our specially designed cost function, changing the weights and biases"}, {"start": 540.0, "end": 545.52, "text": "to decrease it means making the output of the network on each piece of training data"}, {"start": 545.52, "end": 550.2, "text": "look less like a random array of 10 values and more like an actual decision that we want"}, {"start": 550.2, "end": 551.52, "text": "it to make."}, {"start": 551.52, "end": 555.96, "text": "It's important to remember, this cost function involves an average over all of the training"}, {"start": 556.0400000000001, "end": 564.24, "text": "data. So if you minimize it, it means it's a better performance on all of those samples."}, {"start": 564.24, "end": 568.6, "text": "The algorithm for computing this gradient efficiently, which is effectively the heart of"}, {"start": 568.6, "end": 572.0400000000001, "text": "how a neural network learns, is called back propagation."}, {"start": 572.0400000000001, "end": 574.72, "text": "And it's what I'm going to be talking about next video."}, {"start": 574.72, "end": 579.0400000000001, "text": "There I really want to take the time to walk through what exactly happens to each weight"}, {"start": 579.0400000000001, "end": 582.24, "text": "and each bias for a given piece of training data."}, {"start": 582.24, "end": 586.32, "text": "Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus"}, {"start": 586.32, "end": 587.96, "text": "and formulas."}, {"start": 587.96, "end": 591.76, "text": "Right here, right now, the main thing I want you to know, independent of implementation"}, {"start": 591.76, "end": 596.8, "text": "details, is that what we mean when we talk about a network learning is that it's just"}, {"start": 596.8, "end": 599.28, "text": "minimizing a cost function."}, {"start": 599.28, "end": 602.76, "text": "And notice, one consequence of that is that it's important for this cost function to"}, {"start": 602.76, "end": 607.8, "text": "have a nice smooth output so that we can find a local minimum by taking little steps down"}, {"start": 607.8, "end": 609.32, "text": "hill."}, {"start": 609.32, "end": 614.12, "text": "This is why, by the way, artificial neurons have continuously ranging activations, rather"}, {"start": 614.12, "end": 618.4000000000001, "text": "than simply being active or inactive in a binary way, the way that biological neurons"}, {"start": 618.4000000000001, "end": 620.4000000000001, "text": "are."}, {"start": 620.4000000000001, "end": 624.5600000000001, "text": "This process of repeatedly nudging an input of a function by some multiple of the negative"}, {"start": 624.5600000000001, "end": 627.44, "text": "gradient is called gradient descent."}, {"start": 627.44, "end": 631.72, "text": "It's a way to converge toward some local minimum of a cost function, basically a valley"}, {"start": 631.72, "end": 633.4000000000001, "text": "in this graph."}, {"start": 633.4000000000001, "end": 637.0, "text": "I'm still showing the picture of a function with two inputs, of course, because nudges"}, {"start": 637.0, "end": 641.52, "text": "in a 13,000-dimensional input space are a little hard to wrap your mind around, but there"}, {"start": 641.52, "end": 645.2, "text": "is actually a nice non-spatial way to think about this."}, {"start": 645.2, "end": 649.08, "text": "Each component of the negative gradient tells us two things."}, {"start": 649.08, "end": 653.56, "text": "The sign, of course, tells us whether the corresponding component of the input vector"}, {"start": 653.56, "end": 655.84, "text": "should be nudged up or down."}, {"start": 655.84, "end": 661.28, "text": "But importantly, the relative magnitudes of all these components kind of tells you which"}, {"start": 661.28, "end": 663.84, "text": "changes matter more."}, {"start": 664.84, "end": 669.76, "text": "You see, in our network, an adjustment to one of the weights might have a much greater"}, {"start": 669.76, "end": 674.9200000000001, "text": "impact on the cost function than the adjustment to some other weight."}, {"start": 674.9200000000001, "end": 679.36, "text": "Some of these connections just matter more for our training data."}, {"start": 679.36, "end": 683.48, "text": "So a way that you can think about this gradient vector of our mind-warpingly massive cost"}, {"start": 683.48, "end": 689.48, "text": "function is that it encodes the relative importance of each weight and bias, that is, which"}, {"start": 689.48, "end": 694.04, "text": "of these changes is going to carry the most bang for your buck."}, {"start": 694.04, "end": 697.32, "text": "This really is just another way of thinking about direction."}, {"start": 697.32, "end": 701.6800000000001, "text": "To take a simpler example, if you have some function with two variables as an input, and"}, {"start": 701.6800000000001, "end": 707.9200000000001, "text": "you compute that it's gradient at some particular point, comes out as 3-1."}, {"start": 707.9200000000001, "end": 711.48, "text": "Then on the one hand, you can interpret that as saying that when you're standing at that"}, {"start": 711.48, "end": 716.0, "text": "input, moving along this direction increases the function most quickly."}, {"start": 716.0, "end": 720.4, "text": "But when you graph the function above the plane of input points, that vector is what's"}, {"start": 720.4, "end": 723.08, "text": "giving you the straight uphill direction."}, {"start": 723.08, "end": 727.56, "text": "But another way to read that is to say that changes to this first variable have three"}, {"start": 727.56, "end": 732.36, "text": "times the importance as changes to the second variable, that at least in the neighborhood"}, {"start": 732.36, "end": 737.64, "text": "of the relevant input, nudging the x-value carries a lot more bang for your buck."}, {"start": 739.64, "end": 742.84, "text": "Alright, let's zoom out and sum up where we are so far."}, {"start": 742.84, "end": 748.52, "text": "The network itself is this function with 784 inputs and 10 outputs, defined in terms"}, {"start": 748.52, "end": 750.8000000000001, "text": "of all of these weighted sums."}, {"start": 750.8000000000001, "end": 754.12, "text": "The cost function is a layer of complexity on top of that."}, {"start": 754.12, "end": 759.24, "text": "It takes the 13,000 weights and biases as inputs, and spits out a single measure of"}, {"start": 759.24, "end": 762.5600000000001, "text": "laziness based on the training examples."}, {"start": 762.5600000000001, "end": 767.48, "text": "And the gradient of the cost function is one more layer of complexity still."}, {"start": 767.48, "end": 772.76, "text": "It tells us what nudges to all of these weights and biases cause the fastest change to"}, {"start": 772.76, "end": 776.6, "text": "the value of the cost function, which you might interpret as saying which changes to which"}, {"start": 776.6, "end": 779.76, "text": "weights matter the most."}, {"start": 779.76, "end": 787.6, "text": "So, when you initialize the network with random weights and biases and adjust them many times"}, {"start": 787.6, "end": 792.12, "text": "based on this gradient descent process, how well does it actually perform on images that"}, {"start": 792.12, "end": 794.12, "text": "it's never seen before?"}, {"start": 794.12, "end": 798.24, "text": "Well the one that I've described here, with the two hidden layers of 16 neurons each,"}, {"start": 798.24, "end": 800.32, "text": "goes in mostly for aesthetic reasons."}, {"start": 800.32, "end": 802.2, "text": "Well, it's not bad."}, {"start": 802.2, "end": 806.84, "text": "It classifies about 96% of the new images that it sees correctly."}, {"start": 806.84, "end": 810.76, "text": "And honestly, if you look at some of the examples that it messes up on, you kind of feel"}, {"start": 810.76, "end": 816.2, "text": "compelled to cut it a little slack."}, {"start": 816.2, "end": 819.72, "text": "Now if you play around with the hidden layer structure and make a couple tweaks, you can"}, {"start": 819.72, "end": 821.72, "text": "get this up to 98%."}, {"start": 821.72, "end": 823.16, "text": "And that's pretty good."}, {"start": 823.16, "end": 824.16, "text": "It's not the best."}, {"start": 824.16, "end": 827.6, "text": "You can certainly get better performance by getting more sophisticated than this plain"}, {"start": 827.6, "end": 829.08, "text": "vanilla network."}, {"start": 829.08, "end": 833.88, "text": "But given how daunting the initial task is, I just think there's something incredible about"}, {"start": 833.88, "end": 838.36, "text": "any network doing this well on images that it's never seen before, given that we never"}, {"start": 838.36, "end": 842.28, "text": "specifically told it what patterns to look for."}, {"start": 842.28, "end": 846.64, "text": "Originally, the way that I motivated this structure was by describing a hope that we"}, {"start": 846.64, "end": 850.88, "text": "might have, that the second layer might pick up on little edges, that the third layer"}, {"start": 850.88, "end": 855.36, "text": "would piece together those edges to recognize loops in longer lines, and that those might"}, {"start": 855.36, "end": 858.16, "text": "be piece together to recognize digits."}, {"start": 858.16, "end": 861.08, "text": "So is this what our network is actually doing?"}, {"start": 861.08, "end": 865.0, "text": "Well, for this one at least, not at all."}, {"start": 865.0, "end": 868.76, "text": "Remember how last video we looked at how the weights of the connections from all of the"}, {"start": 868.76, "end": 873.4, "text": "neurons in the first layer to a given neuron in the second layer can be visualized as"}, {"start": 873.4, "end": 877.6, "text": "a given pixel pattern that that second layer neuron is picking up on?"}, {"start": 877.6, "end": 882.24, "text": "Well, when we actually do that, for the weights associated with these transitions from the"}, {"start": 882.24, "end": 884.2, "text": "first layer to the next."}, {"start": 884.2, "end": 890.88, "text": "Instead of picking up on isolated little edges here and there, they look almost random,"}, {"start": 890.88, "end": 894.1600000000001, "text": "just with some very loose patterns in the middle there."}, {"start": 894.1600000000001, "end": 898.96, "text": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights"}, {"start": 898.96, "end": 904.0, "text": "and biases, our network found itself a happy little local minimum that, despite successfully"}, {"start": 904.0, "end": 908.4000000000001, "text": "classifying most images, doesn't exactly pick up on the patterns that we might have hoped"}, {"start": 908.4000000000001, "end": 909.8000000000001, "text": "for."}, {"start": 909.8000000000001, "end": 914.1600000000001, "text": "And to really drive this point home, watch what happens when you input a random image."}, {"start": 914.48, "end": 919.1999999999999, "text": "If the system was smart, you might expect it to either feel uncertain, maybe, not really"}, {"start": 919.1999999999999, "end": 923.68, "text": "activating any of those 10 output neurons or activating them all evenly."}, {"start": 923.68, "end": 928.56, "text": "But instead, it confidently gives you some nonsense answer, as if it feels as sure"}, {"start": 928.56, "end": 934.64, "text": "that this random noise is a 5, as it does that an actual image of a 5 is a 5."}, {"start": 934.64, "end": 935.9599999999999, "text": "Fraze differently?"}, {"start": 935.9599999999999, "end": 941.8, "text": "Even if this network can recognize digits pretty well, it has no idea how to draw them."}, {"start": 941.8, "end": 945.76, "text": "A lot of this is because it's such a tightly constrained training setup."}, {"start": 945.76, "end": 948.16, "text": "I mean, put yourself in the network's shoes here."}, {"start": 948.16, "end": 953.24, "text": "From its point of view, the entire universe consists of nothing but clearly defined unmoving"}, {"start": 953.24, "end": 955.56, "text": "digits centered in a tiny grid."}, {"start": 955.56, "end": 960.12, "text": "And its cost function just never gave it any incentive to be anything but utterly confident"}, {"start": 960.12, "end": 962.12, "text": "in its decisions."}, {"start": 962.12, "end": 965.76, "text": "So with this is the image of what those second layer neurons are really doing, you might"}, {"start": 965.76, "end": 969.42, "text": "wonder why I would introduce this network with the motivation of picking up on edges and"}, {"start": 969.42, "end": 970.42, "text": "patterns."}, {"start": 970.5799999999999, "end": 973.5, "text": "That's just not at all what it ends up doing."}, {"start": 973.5, "end": 977.5, "text": "Well this is not meant to be our end goal, but instead a starting point."}, {"start": 977.5, "end": 982.02, "text": "Frankly, this is old technology, the kind researched in the 80s and 90s."}, {"start": 982.02, "end": 986.42, "text": "And you do need to understand it before you can understand more detailed modern variants,"}, {"start": 986.42, "end": 989.86, "text": "and it clearly is capable of solving some interesting problems."}, {"start": 989.86, "end": 993.6999999999999, "text": "But the more you dig in to what those hidden layers are really doing, the less intelligent"}, {"start": 993.6999999999999, "end": 995.9, "text": "it seems."}, {"start": 996.9, "end": 1003.22, "text": "Shifting the focus for a moment from how networks learn to how you learn, that'll only"}, {"start": 1003.22, "end": 1007.18, "text": "happen if you engage actively with the material here somehow."}, {"start": 1007.18, "end": 1011.78, "text": "One pretty simple thing that I want you to do is just pause right now and think deeply"}, {"start": 1011.78, "end": 1017.54, "text": "for a moment about what changes you might make to this system and how it perceives images"}, {"start": 1017.54, "end": 1021.86, "text": "if you wanted it to better pick up on things like edges and patterns."}, {"start": 1021.86, "end": 1026.3600000000001, "text": "But better than that, to actually engage with the material, I highly recommend the book"}, {"start": 1026.3600000000001, "end": 1029.72, "text": "by Michael Neilsen on deep learning and neural networks."}, {"start": 1029.72, "end": 1035.2, "text": "In it, you can find the code and the data to download and play with for this exact example,"}, {"start": 1035.2, "end": 1039.38, "text": "and the book will walk you through step by step what that code is doing."}, {"start": 1039.38, "end": 1043.8600000000001, "text": "What's awesome is that this book is free and publicly available, so if you do get something"}, {"start": 1043.8600000000001, "end": 1047.82, "text": "out of it, consider joining me in making a donation towards Neilsen's efforts."}, {"start": 1048.34, "end": 1052.02, "text": "I've also linked a couple other resources that I like a lot in the description, including"}, {"start": 1052.02, "end": 1058.6599999999999, "text": "the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill."}, {"start": 1058.6599999999999, "end": 1061.8999999999999, "text": "To close things off here for the last few minutes, I want to jump back into a snippet"}, {"start": 1061.8999999999999, "end": 1064.3799999999999, "text": "of the interview that I had with Lisha Lee."}, {"start": 1064.3799999999999, "end": 1068.34, "text": "You might remember her from the last video, she did her PhD work in deep learning."}, {"start": 1068.34, "end": 1071.76, "text": "And in this little snippet, she talks about two recent papers that really dig in to"}, {"start": 1071.76, "end": 1076.26, "text": "how some of the more modern image recognition networks are actually learning."}, {"start": 1076.26, "end": 1080.26, "text": "Just to set up where we were in the conversation, the first paper took one of these particularly"}, {"start": 1080.26, "end": 1084.3799999999999, "text": "deep neural networks that's really good at image recognition, and instead of training"}, {"start": 1084.3799999999999, "end": 1089.42, "text": "it on a properly labeled data set, it shuffled all of the labels around before training."}, {"start": 1089.42, "end": 1093.54, "text": "Obviously the testing accuracy here was going to be no better than random, since everything's"}, {"start": 1093.54, "end": 1095.26, "text": "just randomly labeled."}, {"start": 1095.26, "end": 1100.34, "text": "But it was still able to achieve the same training accuracy as you would on a properly labeled"}, {"start": 1100.34, "end": 1101.34, "text": "data set."}, {"start": 1101.34, "end": 1106.06, "text": "Basically, the millions of weights for this particular network were enough for it to just"}, {"start": 1106.06, "end": 1110.6799999999998, "text": "memorize the random data, which kind of raises the question for whether minimizing this"}, {"start": 1110.6799999999998, "end": 1115.9399999999998, "text": "cost function actually corresponds to any sort of structure in the image, or is it just"}, {"start": 1115.9399999999998, "end": 1116.9399999999998, "text": "memorization?"}, {"start": 1116.9399999999998, "end": 1120.1, "text": "It applies the entire data set of what the correct classification is."}, {"start": 1120.1, "end": 1126.24, "text": "And so a couple of half a year later at ICML this year, there was not exactly rebuttal"}, {"start": 1126.24, "end": 1130.6999999999998, "text": "paper, paper that addressed some aspects of like, hey, actually these networks are doing"}, {"start": 1130.7, "end": 1132.26, "text": "something a little bit smarter than that."}, {"start": 1132.26, "end": 1139.54, "text": "If you look at that accuracy curve, if you were just training on a random data set, that"}, {"start": 1139.54, "end": 1145.54, "text": "curve sort of went down very slowly in almost a linear fashion."}, {"start": 1145.54, "end": 1150.98, "text": "So you're really struggling to find that local minimum of possible, the right weights that"}, {"start": 1150.98, "end": 1152.3, "text": "would get you that accuracy."}, {"start": 1152.3, "end": 1156.94, "text": "Whereas if you're actually training on a structure data set, one that has the right labels, you"}, {"start": 1156.94, "end": 1160.9, "text": "fiddle around a little bit in the beginning, but then you kind of dropped very fast to"}, {"start": 1160.9, "end": 1163.3, "text": "get to that accuracy level."}, {"start": 1163.3, "end": 1168.5800000000002, "text": "And so in some sense, it was easier to find that local maxima."}, {"start": 1168.5800000000002, "end": 1172.9, "text": "And so it was also interesting about that, is it brings into light another paper from"}, {"start": 1172.9, "end": 1178.96, "text": "actually a couple of years ago, which has a lot more simplifications about the network"}, {"start": 1178.96, "end": 1179.96, "text": "layers."}, {"start": 1179.96, "end": 1184.3, "text": "But one of the results was saying how if you look at the optimization landscape, the local"}, {"start": 1184.3, "end": 1189.4199999999998, "text": "minima that these networks tend to learn are actually of equal quality."}, {"start": 1189.4199999999998, "end": 1193.8999999999999, "text": "So in some sense, if your data set a structure, you should be able to find that much more"}, {"start": 1193.8999999999999, "end": 1198.58, "text": "easily."}, {"start": 1198.58, "end": 1201.4199999999998, "text": "My thanks, as always, to those of you supporting on Patreon."}, {"start": 1201.4199999999998, "end": 1205.4199999999998, "text": "I've said before just what a game changer in Patreon is, but these videos really would"}, {"start": 1205.4199999999998, "end": 1207.5, "text": "not be possible without you."}, {"start": 1207.5, "end": 1211.5, "text": "I also want to give a special thanks to the VC firm Amplify Partners in their support"}, {"start": 1211.5, "end": 1213.18, "text": "of these initial videos in the series."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0001.png", "text": "(Recap)", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0002.png", "text": "Sigmoid\n: an How positive is this”\nOQ a. GQ. wea bas + birdy 19)\n@\nco SS Only activate mcaningfully\nTod J owhen vergired sua 10\nee.\ne\n.\ne", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0003.png", "text": "a wet oo\n: ?, 4", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0004.png", "text": "Training in uy |\nprogress. ..\n\n@ 2 2\n\n% few 8 ip", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0005.png", "text": "Testing data\nGe Nivube: correct wa\nGuess, = Sn9r4\n3-3 total aw\na ° it\n: oe mh a ‘s)\n2 e a", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0006.png", "text": "‘ y Finding ninima i Cor\n/\na cee, ’\n\\ Lo SN\nNS\nF:\n\nI re re\n1 Pogo vod 4 G Foe 4", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0007.png", "text": "e A a\n0S. Se\noe\nQQ", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0008.png", "text": "What's the ost\nbade wee of this ditference?\n‘Hos rope 6: 7\nfos wee fant va\nWs = 100i Pa wo?\nCoat ory ery °: e\nWWsh or wie 3: ve\n99 1 wate : u\n‘uss nome Utrer crash", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0009.png", "text": "Vennage caste: What's the ost\nell tracnicp Gale 1099 - 100% = of thus difference?\nre -\n; ; e. eo:\nSUS) ot wee eo at\nuw site e: v2\nTye wie e ae\n6s owe o aa\ney nye Utter trash", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0010.png", "text": "Types Cost nane tion\n3 a nt 3 [pit 13.002 weights. biases\ny, S88 ¥ Output LP number bese\nte FE Tacaneters\n: z ry\n3\nCon od", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0011.png", "text": "/\n‘ f\nCw,\nv\nSingle input\nwe\nee er |", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0012.png", "text": "i i\n; i\n1 \"Crys\n, :\n, of i\nWs « 7 ? ° uw\n' ' . r . ce. ' ' . 1\n*", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0013.png", "text": "| 1\n! ‘ius\n1 :", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0014.png", "text": "TTF ag", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0015.png", "text": "Input space ‘|¥ » 4", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0016.png", "text": "a wv\nfly) u ,\nthy) *\n\" Va! ; ett\nis", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0017.png", "text": "mpute TC\nBion step in -V¢* direction\nee, pent.\nMy\n7 ash\nt aS af\nFa Oa ak Ae ‘\ncer", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0018.png", "text": "& What's the ast\nS$ e of thus difference?\ne fa vu\nacre a ve\ney les @:\ne e: an\ne Utter trash", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0019.png", "text": "Backpropagation (next video)\n‘Training in\nprogress:\n: £8\n> RR OF\nPo Meth Ls", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0020.png", "text": "!\nRot aa\n| Taaaeeees", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0021.png", "text": "Gradient descent\n. oy ae\na\n; ry 6\nSey a\nroy\nL..b-L-*- nn °", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0022.png", "text": "TOW .\n[ ways thea os", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0023.png", "text": "wl,\non Cy | a ee\nte Pe eo Oe\n. 1 ol osbeuel aie\nTOLW}\n. wee ste ul poet\n‘ oy. gz seal .", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0024.png", "text": ". ¥ a", "timestamp": "0024"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0025.png", "text": "oad . .\nbyes Neural networks fimsction\n: ef bape TSd ansers episets1\n. 3 ‘ Output 0 nunchers\n: ° & Tavnetrs 13.002 wemkts ee ses\ne e . a\n+ ° e a\n: '@ Cc e\nmt ad ; e\n| e e\n2 e ia\n. . e e\na . °\n: e\n~ e e", "timestamp": "0025"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0026.png", "text": "Change by some small\naoe multiple of VCE)\nb.b7 i“ hy,\n\n158 4 :\n\n0.03 3 £  #\nAilwegits | va. mi yy 3 OE\nand biases | 7 . ann a : eo", "timestamp": "0026"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0027.png", "text": "qrartt T", "timestamp": "0027"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0028.png", "text": "Testing data\n~ Che Nivube: correct 268 .\nGuess _ = 0956\na — 9 total 275\nfa . 8\nns Cr .\nL aN? eas\noe PB", "timestamp": "0028"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0029.png", "text": "What second layer\n2 . neurons look for\nTed a", "timestamp": "0029"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0030.png", "text": "What second layer\n8 neurons look for\nao, uaz", "timestamp": "0030"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0031.png", "text": "poe 3", "timestamp": "0031"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0032.png", "text": "Average caste: What's the ost\nall tame Gale Coe mate of dus ditference?\n\nfans ite On ran\n\nSUYS site @: a2\n\n: , oe v4\n\n. fudy Sey oe: oy\n\n{669 = 100R= e: ve\n\nA) ee\noe 1 wa? Utrer trash", "timestamp": "0032"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0033.png", "text": "se Convolitional NN IST\n‘This surics\ny\\", "timestamp": "0033"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0034.png", "text": "ef \\ ponder! /", "timestamp": "0034"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0035.png", "text": "Neural Networks, Manifolds,\nand ‘Topology +", "timestamp": "0035"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0037.png", "text": "SONFRRT 0g ce\nror 7 ~™ ia oO 2", "timestamp": "0037"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0038.png", "text": "Velie ot 7 — Randoinly-leheledt data\ncost Hise tion 3\n™\n. oe\nae\n\n- 7\n\na ee ee ee\n\n. Number of gradient\n\ndescent steps.", "timestamp": "0038"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0039.png", "text": "Value of 7 — Randomly-labeled data\ncost function — Properly-labeled data\n—-.\n. ——\nNumber of gradient\ndescent steps", "timestamp": "0039"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0040.png", "text": "Desinas Damon Kistle\nBunt Hund PATREON Munene Perscon\nCrypoegsazm Yon: Nansrathy\nJuan Benet Ed Kettert\n\nAh Yahvs Joseph Jolin Coax\nWalla Lac Rot ine\nMavauk Mo Melietia Fic Chaw\n\nLocke Brewald Matias Jansson\ndomantha D- Supler Peto Peres Sache\nYam, Coernobilsky David Clash\nKaustuv: DeBrowns Michael Gardner\nKatlavn Sclimedbeke Haews Suigh\n\nYu tun Moats Fivtenn\nDave Nepunshi Fr.k Sandell", "timestamp": "0040"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_02_frame_0041.png", "text": "Clicky stuffs", "timestamp": "0041"}], "processing_info": {"audio_duration": 1213.18, "total_segments": 297, "total_ocr_extractions": 40}}
{"video_id": "talk_03", "url": "https://www.youtube.com/watch?v=Ilg3gGewQ5U  # 3. Backpropagation, intuitively | Chapter 3", "timestamp": "2025-07-31T17:00:16.932053", "asr_transcript": {"video_id": "talk_03", "language": "en", "text": " Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we're going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it's helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's kind of a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that's computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need, whether that's having people label tens of thousands of images or whatever other data type you might be dealing with.", "segments": [{"start": 0.0, "end": 9.6, "text": "Here we tackle back propagation, the core algorithm behind how neural networks learn."}, {"start": 9.6, "end": 13.280000000000001, "text": "After a quick recap for where we are, the first thing I'll do is an intuitive walk-through"}, {"start": 13.280000000000001, "end": 17.48, "text": "for what the algorithm is actually doing without any reference to the formulas."}, {"start": 17.48, "end": 21.36, "text": "Then, for those of you who do want to dive into the math, the next video goes into the"}, {"start": 21.36, "end": 24.0, "text": "calculus underlying all this."}, {"start": 24.0, "end": 27.96, "text": "If you watched the last two videos, or if you're just jumping in with the appropriate background,"}, {"start": 27.96, "end": 31.240000000000002, "text": "you know what a neural network is, and how it feeds forward information."}, {"start": 31.240000000000002, "end": 35.84, "text": "Here, we're doing the classic example of recognizing handwritten digits, whose pixel values"}, {"start": 35.84, "end": 40.92, "text": "get fed into the first layer of the network with 784 neurons, and I've been showing a network"}, {"start": 40.92, "end": 46.519999999999996, "text": "with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating"}, {"start": 46.519999999999996, "end": 49.24, "text": "which digit the network is choosing as a tensor."}, {"start": 49.24, "end": 54.400000000000006, "text": "I'm also expecting you to understand gradient descent, as described in the last video,"}, {"start": 54.4, "end": 59.32, "text": "and how what we mean by learning is that we want to find which weights and biases"}, {"start": 59.32, "end": 62.04, "text": "minimize a certain cost function."}, {"start": 62.04, "end": 66.36, "text": "As a quick reminder for the cost of a single training example, what you do is take the"}, {"start": 66.36, "end": 71.28, "text": "output that the network gives, along with the output that you wanted it to give, and"}, {"start": 71.28, "end": 75.48, "text": "you just add up the squares of the differences between each component."}, {"start": 75.48, "end": 80.12, "text": "Doing this for all of your tens of thousands of training examples and averaging the results,"}, {"start": 80.12, "end": 83.0, "text": "this gives you the total cost of the network."}, {"start": 83.0, "end": 86.52, "text": "As if that's not enough to think about, as described in the last video, the thing that"}, {"start": 86.52, "end": 91.68, "text": "we're looking for is the negative gradient of this cost function, which tells you how"}, {"start": 91.68, "end": 96.0, "text": "you need to change all of the weights and biases, all of these connections, so as to"}, {"start": 96.0, "end": 103.03999999999999, "text": "most efficiently decrease the cost."}, {"start": 103.03999999999999, "end": 108.16, "text": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated"}, {"start": 108.16, "end": 109.56, "text": "gradient."}, {"start": 109.56, "end": 113.16, "text": "And the one idea from the last video that I really want you to hold firmly in your mind"}, {"start": 113.16, "end": 117.60000000000001, "text": "right now, is that because thinking of the gradient vector as a direction in 13,000"}, {"start": 117.60000000000001, "end": 122.36, "text": "dimensions is to put it lightly beyond the scope of our imaginations, there's another"}, {"start": 122.36, "end": 124.60000000000001, "text": "way you can think about it."}, {"start": 124.60000000000001, "end": 129.72, "text": "The magnitude of each component here is telling you how sensitive the cost function is to"}, {"start": 129.72, "end": 131.8, "text": "each weight and bias."}, {"start": 131.8, "end": 135.16, "text": "For example, let's say you go through the process I'm about to describe when you compute"}, {"start": 135.16, "end": 140.12, "text": "the negative gradient and the component associated with the weight on this edge here comes out"}, {"start": 140.12, "end": 146.92, "text": "to be 3.2, while the component associated with this edge here comes out as 0.1."}, {"start": 146.92, "end": 151.51999999999998, "text": "The way you would interpret that is that the cost of the function is 32 times more sensitive"}, {"start": 151.51999999999998, "end": 153.64, "text": "to changes in that first weight."}, {"start": 153.64, "end": 157.04, "text": "So if you were to wiggle that value just a little bit, it's going to cause some change"}, {"start": 157.04, "end": 162.6, "text": "to the cost, and that change is 32 times greater than what the same wiggle to that second weight"}, {"start": 162.6, "end": 168.6, "text": "would give."}, {"start": 168.6, "end": 172.48, "text": "Personally, when I was first learning about backpropagation, I think the most confusing"}, {"start": 172.48, "end": 176.2, "text": "aspect was just the notation and the index chasing of it all."}, {"start": 176.2, "end": 180.28, "text": "But once you unwrap what each part of this algorithm is really doing, each individual"}, {"start": 180.28, "end": 183.24, "text": "effect that it's having is actually pretty intuitive."}, {"start": 183.24, "end": 187.72, "text": "It's just that there's a lot of little adjustments getting layered on top of each other."}, {"start": 187.72, "end": 191.35999999999999, "text": "So I'm going to start things off here with a complete disregard for the notation, and"}, {"start": 191.36, "end": 195.44000000000003, "text": "just step through those effects that each training example is having on the weights"}, {"start": 195.44000000000003, "end": 197.36, "text": "and biases."}, {"start": 197.36, "end": 201.56, "text": "Because the cost function involves averaging a certain cost per example, over all the"}, {"start": 201.56, "end": 206.20000000000002, "text": "tens of thousands of training examples, the way that we adjust the weights and biases"}, {"start": 206.20000000000002, "end": 212.68, "text": "for a single gradient descent step also depends on every single example, or rather, in principle"}, {"start": 212.68, "end": 216.16000000000003, "text": "it should, but for computational efficiency, we're going to do a little trick later to keep"}, {"start": 216.16000000000003, "end": 219.84, "text": "you from needing to hit every single example for every single step."}, {"start": 219.84, "end": 223.96, "text": "In other case, right now, all we're going to do is focus our attention on one single"}, {"start": 223.96, "end": 226.76, "text": "example, this image of a two."}, {"start": 226.76, "end": 232.8, "text": "What effect should this one training example have on how the weights and biases get adjusted?"}, {"start": 232.8, "end": 236.04, "text": "Let's say we're at a point where the network is not well trained yet, so the activations"}, {"start": 236.04, "end": 241.52, "text": "in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on"}, {"start": 241.52, "end": 242.52, "text": "and on."}, {"start": 242.52, "end": 245.24, "text": "Now, we can't directly change those activations."}, {"start": 245.24, "end": 247.84, "text": "We only have influence on the weights and biases."}, {"start": 247.84, "end": 251.68, "text": "But it is helpful to keep track of which adjustments we wish should take place to that"}, {"start": 251.68, "end": 253.32, "text": "output layer."}, {"start": 253.32, "end": 258.2, "text": "And since we want it to classify the image as a two, we want that third value to get nudged"}, {"start": 258.2, "end": 262.08, "text": "up, while all of the others get nudged down."}, {"start": 262.08, "end": 267.6, "text": "Moreover, the sizes of these nudges should be proportional to how far away each current"}, {"start": 267.6, "end": 270.24, "text": "value is from its target value."}, {"start": 270.24, "end": 276.08, "text": "For example, the increase to that number two neurons activation is in a sense more important"}, {"start": 276.08, "end": 279.91999999999996, "text": "than the decrease to the number eight neuron, which is already pretty close to where it"}, {"start": 279.91999999999996, "end": 282.03999999999996, "text": "should be."}, {"start": 282.03999999999996, "end": 286.4, "text": "So zooming in further, let's focus just on this one neuron, the one whose activation we"}, {"start": 286.4, "end": 288.2, "text": "wish to increase."}, {"start": 288.2, "end": 293.79999999999995, "text": "Remember, that activation is defined as a certain weighted sum of all of the activations in"}, {"start": 293.79999999999995, "end": 298.91999999999996, "text": "the previous layer, plus a bias, which is all then plugged into something like the sigmoid"}, {"start": 298.91999999999996, "end": 301.84, "text": "squishification function, or a ray-lew."}, {"start": 301.84, "end": 306.35999999999996, "text": "So there are three different avenues that can team up together to help increase that"}, {"start": 306.35999999999996, "end": 307.71999999999997, "text": "activation."}, {"start": 307.71999999999997, "end": 312.76, "text": "You can increase the bias, you can increase the weights, and you can change the activations"}, {"start": 312.76, "end": 315.23999999999995, "text": "from the previous layer."}, {"start": 315.23999999999995, "end": 317.96, "text": "Focusing just on how the weights should be adjusted?"}, {"start": 317.96, "end": 321.44, "text": "Notice how the weights actually have differing levels of influence."}, {"start": 321.44, "end": 325.84, "text": "The connections with the brightest neurons from the preceding layer have the biggest effect,"}, {"start": 325.84, "end": 331.4, "text": "since those weights are multiplied by larger activation values."}, {"start": 331.4, "end": 335.71999999999997, "text": "So if you were to increase one of those weights, it actually has a stronger influence on the"}, {"start": 335.71999999999997, "end": 340.88, "text": "ultimate cost function than increasing the weights of connections with dimmer neurons,"}, {"start": 340.88, "end": 344.03999999999996, "text": "at least as far as this one training example is concerned."}, {"start": 344.03999999999996, "end": 348.64, "text": "Remember, when we talk about gradient descent, we don't just care about whether each component"}, {"start": 348.64, "end": 352.88, "text": "should get nudged up or down, we care about which ones give you the most bang for your"}, {"start": 352.88, "end": 355.44, "text": "butt."}, {"start": 355.44, "end": 360.23999999999995, "text": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological"}, {"start": 360.24, "end": 363.24, "text": "networks of neurons learn, heavy in theory."}, {"start": 363.24, "end": 367.08, "text": "Often summed up in the phrase, neurons that fire together, wire together."}, {"start": 367.08, "end": 372.44, "text": "Here, the biggest increases to weights, the biggest strengthening of connections, happens"}, {"start": 372.44, "end": 378.08, "text": "between neurons which are the most active, and the ones which we wish to become more active."}, {"start": 378.08, "end": 382.28000000000003, "text": "In a sense, the neurons that are firing while seeing a two get more strongly linked to"}, {"start": 382.28000000000003, "end": 385.44, "text": "those firing when thinking about a two."}, {"start": 385.44, "end": 389.0, "text": "To be clear, I really am not in a position to make statements one way or another about"}, {"start": 389.0, "end": 393.32, "text": "whether artificial networks of neurons behave anything like biological brains, and this"}, {"start": 393.32, "end": 397.4, "text": "fires together, wire together idea comes with a couple meaningful asterisks."}, {"start": 397.4, "end": 401.56, "text": "But taken as a very loose analogy, I do find it interesting to note."}, {"start": 401.56, "end": 406.96, "text": "Anyway, the third way that we can help increase this neuron's activation is by changing all"}, {"start": 406.96, "end": 409.52, "text": "the activations in the previous layer."}, {"start": 409.52, "end": 415.04, "text": "Namely, if everything connected to that digit two neuron with a positive weight, got brighter,"}, {"start": 415.04, "end": 419.44, "text": "and if everything connected with a negative weight got dimmer, then that digit two neuron"}, {"start": 419.44, "end": 422.64000000000004, "text": "would become more active."}, {"start": 422.64000000000004, "end": 426.16, "text": "And similar to the weight changes, you're going to get the most bang for your buck by"}, {"start": 426.16, "end": 431.12, "text": "seeking changes that are proportional to the size of the corresponding weights."}, {"start": 431.12, "end": 435.40000000000003, "text": "Now, of course, we cannot directly influence those activations."}, {"start": 435.40000000000003, "end": 438.28000000000003, "text": "We only have control over the weights and biases."}, {"start": 438.28000000000003, "end": 442.52000000000004, "text": "But just as with the last layer, it's helpful to just keep a note of what those desired"}, {"start": 442.52000000000004, "end": 443.52000000000004, "text": "changes are."}, {"start": 444.47999999999996, "end": 448.76, "text": "But keep in mind, zooming out one step here, this is only what that digit two output neuron"}, {"start": 448.76, "end": 449.76, "text": "wants."}, {"start": 449.76, "end": 454.88, "text": "Remember, we also want all of the other neurons in the last layer to become less active,"}, {"start": 454.88, "end": 458.15999999999997, "text": "and each of those other output neurons has its own thoughts about what should happen"}, {"start": 458.15999999999997, "end": 463.15999999999997, "text": "to that second to last layer."}, {"start": 463.15999999999997, "end": 468.91999999999996, "text": "So the desire of this digit two neuron is added together with the desires of all the"}, {"start": 468.92, "end": 473.64000000000004, "text": "other output neurons for what should happen to this second to last layer."}, {"start": 473.64000000000004, "end": 479.04, "text": "Again, in proportion to the corresponding weights, and in proportion to how much each of"}, {"start": 479.04, "end": 481.68, "text": "those neurons needs to change."}, {"start": 481.68, "end": 486.0, "text": "This right here is where the idea of propagating backwards comes in."}, {"start": 486.0, "end": 491.04, "text": "By adding together all these desired effects, you basically get a list of nudges that you"}, {"start": 491.04, "end": 494.24, "text": "want to happen to this second to last layer."}, {"start": 494.24, "end": 498.68, "text": "And once you have those, you can recursively apply the same process to the relevant weights"}, {"start": 498.68, "end": 503.36, "text": "and biases that determine those values, repeating the same process I just walked through and"}, {"start": 503.36, "end": 509.16, "text": "moving backwards through the network."}, {"start": 509.16, "end": 513.92, "text": "And zooming out a bit further, remember that this is all just how a single training example"}, {"start": 513.92, "end": 517.48, "text": "wishes to nudge each one of those weights and biases."}, {"start": 517.48, "end": 521.36, "text": "If we only listen to what that two wanted, the network would ultimately be incentivized"}, {"start": 521.36, "end": 524.08, "text": "just to classify all images as a two."}, {"start": 524.08, "end": 529.48, "text": "What you do is you go through this same back property for every other training example,"}, {"start": 529.48, "end": 533.6800000000001, "text": "recording how each of them would like to change the weights and the biases."}, {"start": 533.6800000000001, "end": 542.2, "text": "And you average together those desired changes."}, {"start": 542.2, "end": 548.2, "text": "This collection here of the average to nudges to each weight and bias is, loosely speaking,"}, {"start": 548.2, "end": 552.6, "text": "the negative gradient of the cost function referenced in the last video, or at least something"}, {"start": 552.6, "end": 554.32, "text": "proportional to it."}, {"start": 554.32, "end": 558.72, "text": "I say loosely speaking only because I have yet to get quantitatively precise about those"}, {"start": 558.72, "end": 559.72, "text": "nudges."}, {"start": 559.72, "end": 563.44, "text": "But if you understood every change that I just referenced, why some are proportionally"}, {"start": 563.44, "end": 568.6800000000001, "text": "bigger than others, and how they all need to be added together, you understand the mechanics"}, {"start": 568.6800000000001, "end": 574.08, "text": "for what back propagation is actually doing."}, {"start": 574.08, "end": 578.48, "text": "By the way, in practice, it takes computers an extremely long time to add up the influence"}, {"start": 578.48, "end": 583.08, "text": "of every single training example, every single gradient descent step."}, {"start": 583.08, "end": 585.48, "text": "So here's what's commonly done instead."}, {"start": 585.48, "end": 590.36, "text": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches."}, {"start": 590.36, "end": 593.32, "text": "Let's say each one having 100 training examples."}, {"start": 593.32, "end": 596.96, "text": "Then you compute a step according to the mini-batch."}, {"start": 596.96, "end": 600.44, "text": "It's not going to be the actual gradient to the cost function, which depends on all of"}, {"start": 600.44, "end": 603.16, "text": "the training data, not this tiny subset."}, {"start": 603.16, "end": 606.12, "text": "So it's not the most efficient step downhill."}, {"start": 606.12, "end": 609.76, "text": "But each mini-batch does give you a pretty good approximation, and more importantly, it"}, {"start": 609.76, "end": 612.88, "text": "gives you a significant computational speed up."}, {"start": 612.88, "end": 616.88, "text": "If you would applaud the trajectory of your network under the relevant cost surface, it"}, {"start": 616.88, "end": 621.16, "text": "would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick"}, {"start": 621.16, "end": 626.28, "text": "steps, rather than a carefully calculating man determining the exact downhill direction"}, {"start": 626.28, "end": 631.6, "text": "of each step before taking a very slow and careful step in that direction."}, {"start": 631.6, "end": 636.08, "text": "This technique is referred to as stochastic gradient descent."}, {"start": 636.08, "end": 640.36, "text": "There's kind of a lot going on here, so let's just sum it up for ourselves, shall we?"}, {"start": 640.36, "end": 645.44, "text": "Backpropagation is the algorithm for determining how a single training example would like to"}, {"start": 645.44, "end": 647.5200000000001, "text": "nudge the weights and biases."}, {"start": 647.5200000000001, "end": 651.32, "text": "Not just in terms of whether they should go up or down, but in terms of what relative"}, {"start": 651.32, "end": 656.2, "text": "proportions to those changes cause the most rapid decrease to the cost."}, {"start": 656.2, "end": 660.6400000000001, "text": "A true gradient descent step would involve doing this for all your tens and thousands"}, {"start": 660.6400000000001, "end": 664.88, "text": "of training examples and averaging the desired changes that you get."}, {"start": 664.88, "end": 669.12, "text": "But that's computationally slow, so instead you randomly subdivide the data into these"}, {"start": 669.12, "end": 674.08, "text": "mini batches and compute each step with respect to a mini batch."}, {"start": 674.08, "end": 678.56, "text": "Repeatedly going through all of the mini batches and making these adjustments, you will converge"}, {"start": 678.56, "end": 683.04, "text": "towards a local minimum of the cost function, which is to say your network is going to end"}, {"start": 683.04, "end": 687.48, "text": "up doing a really good job on the training examples."}, {"start": 687.48, "end": 692.24, "text": "So with all of that said, every line of code that would go into implementing Backprop"}, {"start": 692.24, "end": 697.6, "text": "actually corresponds with something that you have now seen, at least in informal terms."}, {"start": 697.6, "end": 701.84, "text": "But sometimes knowing what the math does is only half the battle, and just representing"}, {"start": 701.84, "end": 704.96, "text": "the damn thing is where it gets all muddled and confusing."}, {"start": 704.96, "end": 709.32, "text": "So for those of you who do want to go deeper, the next video goes through the same ideas"}, {"start": 709.32, "end": 713.32, "text": "that were just presented here, but in terms of the underlying calculus, which should hopefully"}, {"start": 713.32, "end": 717.4, "text": "make it a little more familiar as you see the topic in other resources."}, {"start": 717.4, "end": 721.16, "text": "Before that, one thing worth emphasizing is that for this algorithm to work, and this"}, {"start": 721.16, "end": 725.1999999999999, "text": "goes for all sorts of machine learning beyond just neural networks, you need a lot of"}, {"start": 725.1999999999999, "end": 726.48, "text": "training data."}, {"start": 726.48, "end": 730.1999999999999, "text": "In our case, one thing that makes handwritten digits such a nice example is that there"}, {"start": 730.1999999999999, "end": 735.3199999999999, "text": "exists the M-NIST database, with so many examples that have been labeled by humans."}, {"start": 735.3199999999999, "end": 738.9599999999999, "text": "So a common challenge that those of you working in machine learning will be familiar with"}, {"start": 738.9599999999999, "end": 742.8399999999999, "text": "is just getting the labeled training data that you actually need, whether that's having"}, {"start": 742.8399999999999, "end": 747.0799999999999, "text": "people label tens of thousands of images or whatever other data type you might be dealing"}, {"start": 747.0799999999999, "end": 748.0799999999999, "text": "with."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0002.png", "text": "hy\n\\ dh, “sy,", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0003.png", "text": "oo pee. +S\nand bras [OLAS *\nLI Sy J\nChey wy -tyy001° 7 34", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0004.png", "text": "ve je SLRS PES, Oe\nO04 La wt\nAll we.ghts ~ we 3\n\" 164 3 ° °\nand branes 3\nt Chey wy -tr3 001° — 285\nDirection in\n13,002 dimensions???", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0005.png", "text": "3.20 re ee :\n\na a x\n~“ i © 2\nAll welts ; € :\n\nandl braars +\nNudge Ce weight\nPete reer\n_o~ Y\nf Wye. ane a", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0006.png", "text": "Training in 4 —-9\nprogress. . . a\non re pe Se Renan\nee 5 fale Oy Ys\nqe", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0007.png", "text": "beeh step 2 Average over all training examples\nuses every ;\nexampke 5 Cost of\nESN. \" ee o ——\nFon ERE. D- ra\nOo ae RO , G\n(eee PS\nYeu .", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0008.png", "text": "f | a ino\neR 6y . Ot\n: an ? aT 9 og", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0009.png", "text": "| | 0 iO\nhe og Ml eal\nBae NP\nPE PO ONG 7 y?\nv . ? 19 og", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0010.png", "text": "2 ae Oeypay tans te ay yds\ne\ns 8\ne\ne\ne\ne\ne\ns\nbd", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0011.png", "text": "2 ah a(Frar}te ay + tae ty pode\n\n~\n\nTuerease + $ apy\n\nTuciease «\ne\n\nChange a, yy\ns\n°", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0012.png", "text": "a SB ata ta aye bey yon de\nCo\n\nTuctease + | ie Ter!\n\nluiciease #\n\nin proportion tu a, e\n\nChange a, e\ne\n°", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0013.png", "text": "“Nenrons that fire together wire together”\n\nNo claims here\ni, : Dae Definitely not\nI — Le a ~~ lentist", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0014.png", "text": "“1 GR aieyay tan, + ta a, pode\nI ! $\nuucrease 4 . .\n1° G2\nI ‘.\ncl ease\n‘LP a: le\nin proportion tu a, ry\n1°\nChange a, e\ne\n1°", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0015.png", "text": "a @11 aL\naa) RR es 5 4\nAEROS LN fe", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0016.png", "text": "sie tte re\nIncrease BDULDng:\nIncrease i, pirrt f° ee\nin proportion toa, + + 8 98 tO Ss 7\nProt HOTRADT MBS\nChange a, perereree) yr =\nie proportion tow, DDN Ge OB:\nHedy gee?\nsiete ed", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0017.png", "text": "e\ne er\n| 4\ntw.) All weigitts aud Iieses\nWs uar", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0018.png", "text": "(2S (BIA fe ON\nny : eee we a - ue > ~", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0020.png", "text": "SBEOt’PFIAF EIPNOGS\nA2Z2y¥Y3B27%F G7 O08\n6032619797398 5", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0021.png", "text": "> >\n:\nwe, ' Se,\nq ww —_— ‘ Ce _.\nfi een Sy a eo Ww\nere, Eo eer", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0022.png", "text": "Average over\nfa (x (o (cy (7 ( all traunmg data\nto eS] uu? tue mt foie. ee eas\nwy CT os ee ce ee\nwe v ete sab} sa) ee a\nMisoul | Le) sus | eu fonie petin", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0023.png", "text": "fet ne anny aot come mek tH care, ate.\n= TALS. Te\nTeme:\neR fnchertoinepens\nPoe eria\nSitka enor ares ror en\nea ay ee tee\n. SR\nfo cette oe tet nts ns aid, teh\nmeh Bw eel vem 1 et em The cade you'd find\nSl STAD cg re *\nChesca atta ye mente in Nielsen's book\nsel ett, IEEE senso", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0024.png", "text": "Training in 3 3\nprogress. . . ,\nONS FRR Sage\naI uae, De\n‘i H", "timestamp": "0024"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_03_frame_0025.png", "text": "Clicky stuffs", "timestamp": "0025"}], "processing_info": {"audio_duration": 748.0799999999999, "total_segments": 184, "total_ocr_extractions": 23}}
{"video_id": "talk_04", "url": "https://www.youtube.com/watch?v=tIeHLnjs5U8  # 4. Backpropagation calculus | Chapter 4", "timestamp": "2025-07-31T17:00:40.791940", "asr_transcript": {"video_id": "talk_04", "language": "en", "text": " The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we're just going to focus on the connection between the last two neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the activation of the previous neuron is a L minus 1. These are not exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different indices later on. Now let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We'll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neurons activation plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It's actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear what they all are. Because now we're going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL? In this case comes out just to be AL minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it's just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is. Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is. So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix that I talked about in the Part 1 video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It's just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression describing how sensitive the cost is to a specific weight looks essentially the same. I'll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that...well, that's pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don't worry if it takes time for your mind to digest it all.", "segments": [{"start": 0.0, "end": 8.36, "text": "The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough"}, {"start": 8.36, "end": 11.120000000000001, "text": "of the back propagation algorithm."}, {"start": 11.120000000000001, "end": 14.88, "text": "Here we get a little bit more formal and dive into the relevant calculus."}, {"start": 14.88, "end": 18.52, "text": "It's normal for this to be at least a little confusing so the mantra to regularly pause"}, {"start": 18.52, "end": 21.96, "text": "and ponder certainly applies as much here as anywhere else."}, {"start": 21.96, "end": 26.560000000000002, "text": "Our main goal is to show how people in machine learning commonly think about the chain rule"}, {"start": 26.56, "end": 30.96, "text": "from calculus in the context of networks, which has kind of a different feel from how"}, {"start": 30.96, "end": 34.519999999999996, "text": "most introductory calculus courses approach the subject."}, {"start": 34.519999999999996, "end": 40.4, "text": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic."}, {"start": 40.4, "end": 44.8, "text": "Let's just start off with an extremely simple network, one where each layer has a single"}, {"start": 44.8, "end": 46.32, "text": "neuron in it."}, {"start": 46.32, "end": 51.0, "text": "So this particular network is determined by three weights and three biases, and our goal"}, {"start": 51.0, "end": 55.599999999999994, "text": "is to understand how sensitive the cost function is to these variables."}, {"start": 55.6, "end": 59.24, "text": "That way we know which adjustments to those terms is going to cause the most efficient"}, {"start": 59.24, "end": 61.96, "text": "decrease to the cost function."}, {"start": 61.96, "end": 66.0, "text": "And we're just going to focus on the connection between the last two neurons."}, {"start": 66.0, "end": 70.64, "text": "Let's label the activation of that last neuron with a superscript L indicating which layer"}, {"start": 70.64, "end": 71.72, "text": "it's in."}, {"start": 71.72, "end": 76.52000000000001, "text": "So the activation of the previous neuron is a L minus 1."}, {"start": 76.52000000000001, "end": 80.16, "text": "These are not exponents, they're just a way of indexing what we're talking about since"}, {"start": 80.16, "end": 83.28, "text": "I want to save subscripts for different indices later on."}, {"start": 83.68, "end": 87.96000000000001, "text": "Now let's say that the value we want this last activation to be for a given training"}, {"start": 87.96000000000001, "end": 92.96000000000001, "text": "example is y, for example, y might be 0 or 1."}, {"start": 92.96000000000001, "end": 100.28, "text": "So the cost of this simple network for a single training example is AL minus y squared."}, {"start": 100.28, "end": 106.08, "text": "We'll denote the cost of that one training example as C0."}, {"start": 106.08, "end": 112.0, "text": "As a reminder, this last activation is determined by a weight, which I'm going to call WL,"}, {"start": 112.0, "end": 117.52, "text": "times the previous neurons activation plus some bias, which I'll call BL."}, {"start": 117.52, "end": 121.32, "text": "And then you pump that through some special nonlinear function like the sigmoid or a ray"}, {"start": 121.32, "end": 122.32, "text": "loop."}, {"start": 122.32, "end": 125.4, "text": "It's actually going to make things easier for us if we give a special name to this weighted"}, {"start": 125.4, "end": 130.4, "text": "sum, like Z, with the same superscript as the relevant activations."}, {"start": 130.4, "end": 134.88, "text": "So this is a lot of terms and a way that you might conceptualize it is that the weight,"}, {"start": 134.88, "end": 140.16, "text": "the previous action and the bias altogether are used to compute Z, which in turn lets"}, {"start": 140.16, "end": 147.28, "text": "us compute A, which finally, along with a constant Y, lets us compute the cost."}, {"start": 147.28, "end": 152.92, "text": "And of course, AL minus 1 is influenced by its own weight and bias and such."}, {"start": 152.92, "end": 155.72, "text": "But we're not going to focus on that right now."}, {"start": 155.72, "end": 158.07999999999998, "text": "Now all of these are just numbers, right?"}, {"start": 158.07999999999998, "end": 161.92, "text": "And it can be nice to think of each one as having its own little number line."}, {"start": 161.92, "end": 167.32, "text": "Our first goal is to understand how sensitive the cost function is to small changes in"}, {"start": 167.32, "end": 169.8, "text": "our weight, WL."}, {"start": 169.8, "end": 171.16000000000003, "text": "Never phrase differently."}, {"start": 171.16000000000003, "end": 175.68, "text": "What is the derivative of C with respect to WL?"}, {"start": 175.68, "end": 181.16000000000003, "text": "When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change"}, {"start": 181.16000000000003, "end": 183.20000000000002, "text": "by 0.01."}, {"start": 183.20000000000002, "end": 188.76000000000002, "text": "And think of this Dell C term as meaning whatever the resulting nudge to the cost is."}, {"start": 188.76000000000002, "end": 191.4, "text": "What we want is their ratio."}, {"start": 191.4, "end": 197.68, "text": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some"}, {"start": 197.68, "end": 203.12, "text": "nudge to AL, which directly influences the cost."}, {"start": 203.12, "end": 207.92000000000002, "text": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny"}, {"start": 207.92000000000002, "end": 213.8, "text": "change W, that is the derivative of ZL with respect to WL."}, {"start": 213.8, "end": 218.76000000000002, "text": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that"}, {"start": 218.76000000000002, "end": 223.96, "text": "caused it, as well as the ratio between the final nudge to C and this intermediate nudge"}, {"start": 223.96, "end": 225.84, "text": "to AL."}, {"start": 225.84, "end": 231.84, "text": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity"}, {"start": 231.84, "end": 237.28, "text": "of C to small changes in WL."}, {"start": 237.28, "end": 240.92000000000002, "text": "So on screen right now, there's kind of a lot of symbols, and take a moment to just"}, {"start": 240.92000000000002, "end": 243.72, "text": "make sure it's clear what they all are."}, {"start": 243.72, "end": 247.4, "text": "Because now we're going to compute the relevant derivatives."}, {"start": 247.4, "end": 254.4, "text": "The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this"}, {"start": 254.4, "end": 259.44, "text": "means that its size is proportional to the difference between the networks output and"}, {"start": 259.44, "end": 261.36, "text": "the thing that we want it to be."}, {"start": 261.36, "end": 265.96, "text": "So if that output was very different, even slight changes stand to have a big impact on"}, {"start": 265.96, "end": 268.28000000000003, "text": "the final cost function."}, {"start": 268.28000000000003, "end": 273.92, "text": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function,"}, {"start": 273.92, "end": 277.32, "text": "or whatever non-linearity you choose to use."}, {"start": 277.32, "end": 281.48, "text": "And the derivative of ZL with respect to WL?"}, {"start": 281.48, "end": 286.08000000000004, "text": "In this case comes out just to be AL minus 1."}, {"start": 286.08000000000004, "end": 289.44, "text": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas"}, {"start": 289.44, "end": 294.16, "text": "without taking a moment to sit back and remind yourself of what they all actually mean."}, {"start": 294.16, "end": 298.84000000000003, "text": "In the case of this last derivative, the amount that that small nudge to the weight influenced"}, {"start": 298.84000000000003, "end": 303.28000000000003, "text": "the last layer depends on how strong the previous neuron is."}, {"start": 303.28000000000003, "end": 309.28000000000003, "text": "Remember, this is where that neurons that fire together wire together idea comes in."}, {"start": 309.28, "end": 314.79999999999995, "text": "And all of this is the derivative with respect to WL only of the cost for a specific single"}, {"start": 314.79999999999995, "end": 316.52, "text": "training example."}, {"start": 316.52, "end": 320.88, "text": "Since the full cost function involves averaging together all those costs across many different"}, {"start": 320.88, "end": 326.23999999999995, "text": "training examples, its derivative requires averaging this expression that we found over"}, {"start": 326.23999999999995, "end": 328.47999999999996, "text": "all training examples."}, {"start": 328.47999999999996, "end": 332.96, "text": "And of course that is just one component of the gradient vector, which itself is built"}, {"start": 332.96, "end": 337.55999999999995, "text": "up from the partial derivatives of the cost function with respect to all those weights"}, {"start": 337.56, "end": 340.76, "text": "and biases."}, {"start": 340.76, "end": 344.2, "text": "But even though that's just one of the many partial derivatives we need, it's more than"}, {"start": 344.2, "end": 346.44, "text": "50% of the work."}, {"start": 346.44, "end": 350.28000000000003, "text": "The sensitivity to the bias, for example, is almost identical."}, {"start": 350.28000000000003, "end": 358.88, "text": "We just need to change out this del Z del W term for a del Z del B."}, {"start": 358.88, "end": 364.4, "text": "And if you look at the relevant formula, that derivative comes out to be 1."}, {"start": 365.4, "end": 371.67999999999995, "text": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive"}, {"start": 371.67999999999995, "end": 376.2, "text": "this cost function is to the activation of the previous layer."}, {"start": 376.2, "end": 381.4, "text": "Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the"}, {"start": 381.4, "end": 386.59999999999997, "text": "previous activation, comes out to be the weight WL."}, {"start": 386.59999999999997, "end": 390.08, "text": "And again, even though we're not going to be able to directly influence that previous"}, {"start": 390.08, "end": 393.2, "text": "layer activation, it's helpful to keep track of."}, {"start": 393.2, "end": 398.64, "text": "Because now, we can just keep iterating this same chain rule idea backwards to see how"}, {"start": 398.64, "end": 403.64, "text": "sensitive the cost function is to previous weights and previous biases."}, {"start": 403.64, "end": 407.03999999999996, "text": "And you might think that this is an overly simple example, since all layers just have"}, {"start": 407.03999999999996, "end": 410.52, "text": "one neuron, and that things are going to get exponentially more complicated for a real"}, {"start": 410.52, "end": 411.52, "text": "network."}, {"start": 411.52, "end": 416.36, "text": "But honestly, not that much changes when we give the layers multiple neurons."}, {"start": 416.36, "end": 419.24, "text": "Really it's just a few more indices to keep track of."}, {"start": 419.32, "end": 423.72, "text": "Rather than the activation of a given layer simply being AL, it's also going to have a"}, {"start": 423.72, "end": 427.8, "text": "subscript indicating which neuron of that layer it is."}, {"start": 427.8, "end": 434.0, "text": "Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer"}, {"start": 434.0, "end": 439.76, "text": "L. For the cost, again, we look at what the desired output is, but this time we add"}, {"start": 439.76, "end": 446.08, "text": "up the squares of the differences between these last layer activations and the desired output."}, {"start": 446.15999999999997, "end": 453.24, "text": "That is, you take a sum over ALJ minus YJ squared."}, {"start": 453.24, "end": 456.47999999999996, "text": "Since there's a lot more weights, each one has to have a couple more indices to keep"}, {"start": 456.47999999999996, "end": 458.08, "text": "track of where it is."}, {"start": 458.08, "end": 465.76, "text": "So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK."}, {"start": 465.76, "end": 469.79999999999995, "text": "Those indices might feel a little backwards at first, but it lines up with how you'd index"}, {"start": 469.79999999999995, "end": 473.79999999999995, "text": "the weight matrix that I talked about in the Part 1 video."}, {"start": 473.8, "end": 478.40000000000003, "text": "Just as before, it's still nice to give a name to the relevant weighted sum, like Z,"}, {"start": 478.40000000000003, "end": 483.28000000000003, "text": "so that the activation of the last layer is just your special function, like the sigmoid,"}, {"start": 483.28000000000003, "end": 485.08, "text": "applied to Z."}, {"start": 485.08, "end": 488.64, "text": "You can kind of see what I mean, right, where all of these are essentially the same equations"}, {"start": 488.64, "end": 491.76, "text": "that we had before in the one neuron per layer case."}, {"start": 491.76, "end": 495.40000000000003, "text": "It's just that it looks a little more complicated."}, {"start": 495.40000000000003, "end": 500.08000000000004, "text": "And indeed, the chain ruled the rivetive expression describing how sensitive the cost"}, {"start": 500.08, "end": 503.84, "text": "is to a specific weight looks essentially the same."}, {"start": 503.84, "end": 509.4, "text": "I'll leave it to you to pause and think about each of those terms if you want."}, {"start": 509.4, "end": 515.16, "text": "What does change here, though, is the derivative of the cost with respect to one of the activations"}, {"start": 515.16, "end": 517.8, "text": "in the layer L minus 1."}, {"start": 517.8, "end": 521.96, "text": "In this case, the difference is that the neuron influences the cost function through multiple"}, {"start": 521.96, "end": 524.72, "text": "different paths."}, {"start": 524.72, "end": 531.2, "text": "It is on the one hand, it influences AL0, which plays a role in the cost function, but"}, {"start": 531.2, "end": 536.48, "text": "it also has an influence on AL1, which also plays a role in the cost function, and you"}, {"start": 536.48, "end": 540.28, "text": "have to add those up."}, {"start": 540.28, "end": 543.72, "text": "And that...well, that's pretty much it."}, {"start": 543.72, "end": 547.96, "text": "Once you know how sensitive the cost function is to the activations in this second to last"}, {"start": 547.96, "end": 552.38, "text": "layer, you can just repeat the process for all the weights and biases feeding into that"}, {"start": 552.38, "end": 553.9, "text": "layer."}, {"start": 553.9, "end": 555.42, "text": "So pat yourself on the back."}, {"start": 555.42, "end": 560.4599999999999, "text": "If all of this makes sense, you have now looked deep into the heart of back propagation,"}, {"start": 560.4599999999999, "end": 563.6999999999999, "text": "the workhorse behind how neural networks learn."}, {"start": 563.6999999999999, "end": 568.06, "text": "These chain rule expressions give you the derivatives that determine each component in"}, {"start": 568.06, "end": 575.02, "text": "the gradient that helps minimize the cost of the network by repeatedly stepping downhill."}, {"start": 575.02, "end": 578.98, "text": "If you sit back and think about all that, this is a lot of layers of complexity to wrap"}, {"start": 578.98, "end": 580.14, "text": "your mind around."}, {"start": 580.14, "end": 582.9, "text": "So don't worry if it takes time for your mind to digest it all."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0001.png", "text": "Plan\n: ey ee\nb .\nDerivatives in | coy i i :\ncomputatiozal prephe [Sor °", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0002.png", "text": "CU byt baseiby)\ne eo —_e e", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0003.png", "text": "Desired\noupy\nafcl I. 7]", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0004.png", "text": "Corer tia ht?\nDesired\na Lb gthe- :\nwhe te Oo ayes\nain a? y", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0005.png", "text": "ee ne cE\nCoos bath Vane\nre Desired\nee bth a a\nNS ops\na\n; oe .\n/ I,\n. y\nx att ° ° .", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0006.png", "text": "Ea . yeahh 2\n= ey pela loa:\nwy ep bgth lye\n-* ; . Desired\na whl ont a at ; ouipa\nLe\n| “™ / f. y\n| a a\nyo gibt neeeetees", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0007.png", "text": "—- . eT: a\nTet ant ey fala Moy:\n= eo pb gth Uae\nae fh ewha =h\neo Gholi gt rls cay ts Desired\n. NS ‘ “oe ‘ outa\n|Z\nLee\n4\nOM... e eh}\n|_~ i, f\noS Ph-ay uns\nily alo a y\na.\na", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0008.png", "text": "ve Fey Pc a eo eat\nnoo) a\nwf whigih Volant\n4. Desired\nal laps ourpn\nfesee ees fi git? y\nLA,\nNL.", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0009.png", "text": "a Bo Dat ee ce ial? = yl?\nrrr) as\noi th Wye\nSe ta hte:\nlat ;\nant e e 66\nwoo Fy qi a y", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0010.png", "text": "wa a ae ce iails = yl?\nne? a\noe wb gth Uy\n\nSO ota Bas a\nDate\ni / i\n\n“heady one\nat : . a : a y\n— - ti Aol\n7", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0011.png", "text": "we Oe HL ttn jaja a yh\nFe rs a 2\nOy?\nRpt git Wy!\nwh coi t\na a ol ,\nata # /\nr e © °\nail all ¥y", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0012.png", "text": "” at Ooh ots Jota’! ys\nah bE a Fae i\noy tah yl?\nLe OO\na at 4 Rope gh yt\ni\nNL ope :\n; ah aie! 3\n| ;\nnog ke e e @\nC abot all ¥", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0013.png", "text": "—-_\" oe tf os * jaialh = yn\nth gt VD Be a\na pe 52\nC0 fa als\naft o4 ' .\n: a Bo pi gt yt\n; ah oie! ;\nra: e e @\nran a hadi all y", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0014.png", "text": "whol bey\n® eal\n~ @\nSs", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0015.png", "text": "npol\nik ea\nfo 5 ta, 4)\n10\ntrey\n[eee Oe en\n-be-l to,\noa @ 4\no Desired output", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0016.png", "text": "! t poy. Foufeaa ioieayy L\n2 ote ag be) ay tw ty wh\nthi\na,\nuM vat 2\nmea -5 (ay - 9,3\nwe @O-.\" ~\nspel.\nto @", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0017.png", "text": ": t i he they\naf _\na, =o: )\nay-l\npy Nt tk 42\nAS 1 con > fa, ait\nhe\nve\na+ @", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0018.png", "text": "eo ty roo pe tglin!\nohh = ae’ Ma at\n_ f !\n———— aso.)\nSunt over aver 1\naol\niy sO .\n2 a) ta’ oy?\ntha 7\na, 70\nve\nal @\nha\nO-«", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0019.png", "text": "ae * ihea,*\n_— .\nan a dat\"!\nor\nKa yi", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_04_frame_0020.png", "text": "Clicky stuffs", "timestamp": "0020"}], "processing_info": {"audio_duration": 582.9, "total_segments": 137, "total_ocr_extractions": 20}}
{"video_id": "talk_05", "url": "https://www.youtube.com/watch?v=LPZh9BOjkQs  # 5. Large Language Models explained briefly", "timestamp": "2025-07-31T17:01:02.642380", "asr_transcript": {"video_id": "talk_05", "language": "en", "text": " Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You can then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant. You add on whatever the user types in as the first part of that interaction. Then you have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more natural if you allow it to select less likely words along the way at random. So what this means is even though the model itself is deterministic, a given prompt typically gives a different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years, larger models since then, train on much, much more. You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead they begin at random, meaning the model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be thousands, but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example. An algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and multiplications every single second. How long do you think that it would take for you to do all of the operations involved in training the largest language models? Do you think it would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story though. This whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Use flag-unhelpful or problematic predictions and their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many, many operations in parallel, known as GPUs. However, not all language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time. But then, a team of researchers at Google introduced a new model known as the transformer. Transformers don't read text from the start to the finish. They soak it all in at once in parallel. The very first step inside a transformer, and most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous values, so you have to somehow encode language using numbers, and each of these list of numbers may somehow encode the meaning of the corresponding word. What makes Transformers unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another, and refine the meanings that they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations. And as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Then the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. But also, on my second channel I just posted a talk that I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content that I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow on.", "segments": [{"start": 0.0, "end": 5.84, "text": "Imagine you happen across a short movie script that describes a scene between a person"}, {"start": 5.84, "end": 7.5600000000000005, "text": "and their AI assistant."}, {"start": 7.5600000000000005, "end": 13.68, "text": "The script has what the person asks the AI, but the AI's response has been torn off."}, {"start": 13.68, "end": 18.28, "text": "Suppose you also have this powerful magical machine that can take any text and provide"}, {"start": 18.28, "end": 21.46, "text": "a sensible prediction of what word comes next."}, {"start": 21.46, "end": 25.32, "text": "You can then finish the script by feeding in what you have to the machine, seeing what"}, {"start": 25.32, "end": 30.64, "text": "it would predict to start the AI's answer, and then repeating this over and over with"}, {"start": 30.64, "end": 33.480000000000004, "text": "a growing script completing the dialogue."}, {"start": 33.480000000000004, "end": 37.04, "text": "When you interact with a chatbot, this is exactly what's happening."}, {"start": 37.04, "end": 41.6, "text": "A large language model is a sophisticated mathematical function that predicts what word"}, {"start": 41.6, "end": 44.6, "text": "comes next for any piece of text."}, {"start": 44.6, "end": 49.2, "text": "Instead of predicting one word with certainty, though, what it does is assign a probability"}, {"start": 49.2, "end": 51.64, "text": "to all possible next words."}, {"start": 51.64, "end": 56.160000000000004, "text": "To build a chatbot, what you do is lay out some text that describes an interaction between"}, {"start": 56.160000000000004, "end": 59.28, "text": "a user and a hypothetical AI assistant."}, {"start": 59.28, "end": 63.92, "text": "You add on whatever the user types in as the first part of that interaction."}, {"start": 63.92, "end": 68.96000000000001, "text": "Then you have the model repeatedly predict the next word that such a hypothetical AI assistant"}, {"start": 68.96000000000001, "end": 73.12, "text": "would say in response, and that's what's presented to the user."}, {"start": 73.12, "end": 76.88, "text": "In doing this, the output tends to look a lot more natural if you allow it to select"}, {"start": 76.88, "end": 80.12, "text": "less likely words along the way at random."}, {"start": 80.12, "end": 84.88000000000001, "text": "So what this means is even though the model itself is deterministic, a given prompt typically"}, {"start": 84.88000000000001, "end": 88.2, "text": "gives a different answer each time it's run."}, {"start": 88.2, "end": 93.08000000000001, "text": "Models learn how to make these predictions by processing an enormous amount of text, typically"}, {"start": 93.08000000000001, "end": 94.56, "text": "pulled from the internet."}, {"start": 94.56, "end": 99.64, "text": "For a standard human to read the amount of text that was used to train GPT-3, for example,"}, {"start": 99.64, "end": 106.08000000000001, "text": "if they read non-stop 24-7, it would take over 2600 years, larger models since then,"}, {"start": 106.08000000000001, "end": 108.24000000000001, "text": "train on much, much more."}, {"start": 108.24, "end": 112.24, "text": "You can think of training a little bit like tuning the dials on a big machine."}, {"start": 112.24, "end": 117.16, "text": "The way that a language model behaves is entirely determined by these many different continuous"}, {"start": 117.16, "end": 121.47999999999999, "text": "values, usually called parameters or weights."}, {"start": 121.47999999999999, "end": 125.47999999999999, "text": "Changing those parameters will change the probabilities that the model gives for the"}, {"start": 125.47999999999999, "end": 127.88, "text": "next word on a given input."}, {"start": 127.88, "end": 132.95999999999998, "text": "What puts the large in large language model is how they can have hundreds of billions of"}, {"start": 132.95999999999998, "end": 135.32, "text": "these parameters."}, {"start": 135.32, "end": 138.48, "text": "No human ever deliberately sets those parameters."}, {"start": 138.48, "end": 143.48, "text": "Instead they begin at random, meaning the model just outputs gibberish, but they're repeatedly"}, {"start": 143.48, "end": 147.35999999999999, "text": "refined based on many example pieces of text."}, {"start": 147.35999999999999, "end": 151.88, "text": "One of these training examples could be just a handful of words, or it could be thousands,"}, {"start": 151.88, "end": 156.92, "text": "but in either case the way this works is to pass in all but the last word from that example"}, {"start": 156.92, "end": 161.68, "text": "into the model and compare the prediction that it makes with the true last word from"}, {"start": 161.68, "end": 163.28, "text": "the example."}, {"start": 163.28, "end": 168.36, "text": "An algorithm called back propagation is used to tweak all of the parameters in such a"}, {"start": 168.36, "end": 173.2, "text": "way that it makes the model a little more likely to choose the true last word and a little"}, {"start": 173.2, "end": 176.24, "text": "less likely to choose all the others."}, {"start": 176.24, "end": 181.08, "text": "When you do this for many, many trillions of examples, not only does the model start to"}, {"start": 181.08, "end": 185.6, "text": "give more accurate predictions on the training data, but it also starts to make more reasonable"}, {"start": 185.6, "end": 189.56, "text": "predictions on text that it's never seen before."}, {"start": 189.56, "end": 194.36, "text": "Given the huge number of parameters and the enormous amount of training data, the scale"}, {"start": 194.36, "end": 199.6, "text": "of computation involved in training a large language model is mind-boggling."}, {"start": 199.6, "end": 204.32, "text": "To illustrate, imagine that you could perform one billion additions and multiplications"}, {"start": 204.32, "end": 206.0, "text": "every single second."}, {"start": 206.0, "end": 210.6, "text": "How long do you think that it would take for you to do all of the operations involved"}, {"start": 210.6, "end": 213.44, "text": "in training the largest language models?"}, {"start": 213.44, "end": 216.12, "text": "Do you think it would take a year?"}, {"start": 216.12, "end": 219.04, "text": "Maybe something like 10,000 years?"}, {"start": 219.04, "end": 221.23999999999998, "text": "The answer is actually much more than that."}, {"start": 221.23999999999998, "end": 225.72, "text": "It's well over 100 million years."}, {"start": 225.72, "end": 227.64, "text": "This is only part of the story though."}, {"start": 227.64, "end": 229.72, "text": "This whole process is called pre-training."}, {"start": 229.72, "end": 234.07999999999998, "text": "The goal of auto-completing a random passage of text from the internet is very different"}, {"start": 234.07999999999998, "end": 236.82, "text": "from the goal of being a good AI assistant."}, {"start": 236.82, "end": 242.28, "text": "To address this, chatbots undergo another type of training, just as important, called reinforcement"}, {"start": 242.28, "end": 244.72, "text": "learning with human feedback."}, {"start": 244.72, "end": 249.68, "text": "Use flag-unhelpful or problematic predictions and their corrections further change the"}, {"start": 249.68, "end": 255.6, "text": "model's parameters, making them more likely to give predictions that users prefer."}, {"start": 255.6, "end": 260.04, "text": "Looking back at the pre-training though, this staggering amount of computation is only"}, {"start": 260.04, "end": 264.64, "text": "made possible by using special computer chips that are optimized for running many, many"}, {"start": 264.64, "end": 267.8, "text": "operations in parallel, known as GPUs."}, {"start": 267.8, "end": 272.2, "text": "However, not all language models can be easily parallelized."}, {"start": 272.2, "end": 277.36, "text": "Prior to 2017, most language models would process text one word at a time."}, {"start": 277.36, "end": 283.8, "text": "But then, a team of researchers at Google introduced a new model known as the transformer."}, {"start": 283.8, "end": 286.64, "text": "Transformers don't read text from the start to the finish."}, {"start": 286.64, "end": 289.84, "text": "They soak it all in at once in parallel."}, {"start": 289.84, "end": 294.52, "text": "The very first step inside a transformer, and most other language models for that matter,"}, {"start": 294.52, "end": 297.84, "text": "is to associate each word with a long list of numbers."}, {"start": 297.84, "end": 302.59999999999997, "text": "The reason for this is that the training process only works with continuous values, so"}, {"start": 302.59999999999997, "end": 307.28, "text": "you have to somehow encode language using numbers, and each of these list of numbers may"}, {"start": 307.28, "end": 311.0, "text": "somehow encode the meaning of the corresponding word."}, {"start": 311.0, "end": 316.96, "text": "What makes Transformers unique is their reliance on a special operation known as attention."}, {"start": 316.96, "end": 321.35999999999996, "text": "This operation gives all of these lists of numbers a chance to talk to one another,"}, {"start": 321.35999999999996, "end": 327.35999999999996, "text": "and refine the meanings that they encode based on the context around, all done in parallel."}, {"start": 327.36, "end": 332.48, "text": "For example, the numbers encoding the word bank might be changed based on the context surrounding"}, {"start": 332.48, "end": 337.88, "text": "it to somehow encode the more specific notion of a riverbank."}, {"start": 337.88, "end": 342.36, "text": "Transformers typically also include a second type of operation known as a feed-forward"}, {"start": 342.36, "end": 347.32, "text": "neural network, and this gives the model extra capacity to store more patterns about language"}, {"start": 347.32, "end": 349.36, "text": "learned during training."}, {"start": 349.36, "end": 353.88, "text": "All of this data repeatedly flows through many different iterations of these two fundamental"}, {"start": 353.88, "end": 355.16, "text": "operations."}, {"start": 355.16, "end": 360.24, "text": "And as it does so, the hope is that each list of numbers is enriched to encode whatever"}, {"start": 360.24, "end": 366.96000000000004, "text": "information might be needed to make an accurate prediction of what word follows in the passage."}, {"start": 366.96000000000004, "end": 372.0, "text": "At the end, one final function is performed on the last vector in this sequence, which"}, {"start": 372.0, "end": 376.84000000000003, "text": "now has had a chance to be influenced by all the other context from the input text, as"}, {"start": 376.84000000000003, "end": 381.64000000000004, "text": "well as everything the model learned during training, to produce a prediction of the next"}, {"start": 381.64000000000004, "end": 382.64000000000004, "text": "word."}, {"start": 382.64, "end": 388.68, "text": "Then the model's prediction looks like a probability for every possible next word."}, {"start": 388.68, "end": 393.12, "text": "Although researchers design the framework for how each of these steps work, it's important"}, {"start": 393.12, "end": 398.12, "text": "to understand that the specific behavior is an emergent phenomenon based on how those"}, {"start": 398.12, "end": 402.56, "text": "hundreds of billions of parameters are tuned during training."}, {"start": 402.56, "end": 407.15999999999997, "text": "This makes it incredibly challenging to determine why the model makes the exact predictions"}, {"start": 407.15999999999997, "end": 408.47999999999996, "text": "that it does."}, {"start": 408.48, "end": 412.88, "text": "What you can see is that when you use large language model predictions to autocomplete"}, {"start": 412.88, "end": 425.76, "text": "a prompt, the words that it generates are uncannily fluent, fascinating, and even useful."}, {"start": 425.76, "end": 429.24, "text": "If you're a new viewer and you're curious about more details on how transformers and"}, {"start": 429.24, "end": 432.52000000000004, "text": "attention work, boy do I have some material for you."}, {"start": 432.52000000000004, "end": 437.24, "text": "One option is to jump into a series I made about deep learning, where we visualize and"}, {"start": 437.24, "end": 442.2, "text": "motivate the details of attention and all the other steps in a transformer."}, {"start": 442.2, "end": 446.04, "text": "But also, on my second channel I just posted a talk that I gave a couple months ago about"}, {"start": 446.04, "end": 449.12, "text": "this topic for the company TNG in Munich."}, {"start": 449.12, "end": 453.04, "text": "Sometimes I actually prefer the content that I make as a casual talk rather than a produced"}, {"start": 453.04, "end": 456.88, "text": "video, but I leave it up to you which one of these feels like the better follow on."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0000.png", "text": "Large Language Models\nfor the curious beginner", "timestamp": "0000"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0001.png", "text": "(0 pe moti te tego —™\nUeammetcy aod tow ther te cebu\n| P\nperortseopensns™\nMawes\nA\ntransistor", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0002.png", "text": "Avtar tattnes 1s a cervefnatlon 7\n\nbetween atser and ¢ aelphcl very!\n\nTkrowkedgeatie AL messiah! :\n\nee\n\nUser Give me some ideas for what\n\nto da when vonting Santiagc\n\nAL Awbtnat >", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0003.png", "text": "Sees Lee Nes Aemureg A Sane bam me ane ae Hew ase rst» tian\nseeane Ce Ce pare fed cetmaey fae) 9 permet Somme” Tha\nwa ve the eae sea 1 Ae Owe wy eaten ltem Le\n\nhence te Bae tae\nhens Vyneenat Dor.\nRimpint as = ethers\n. Se Wie erat tor pet\nsaetoate the Ratace Whe\nteawe ke anette et Lea ey retin the peewee\nSo A Bee sutton Ube pat\nNao ecg erties\nathe Cae Tee Raided Cartce te The Sma pecs mote\nSopronanate, SER yan tem De Bee", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0004.png", "text": "Parameter\n<)\nwot os\nIt was the best we Bs\nY ' er\nof times it was — bet\nthe —> mals\nod [ne\nser ps\nther [oa", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0005.png", "text": "It was the best of times it was the worst", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0006.png", "text": "a | r\nae saa: — ==\n= rir", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0007.png", "text": "Repreterttirrertioperrcertiepresrerierptierrrdtterrteri\n°\nSecond Minute\n\nHour", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0008.png", "text": "Training\nStep 1: Pretraining Step 2: RLHF", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0009.png", "text": "It was the best offtimes it was the worst! of times", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0011.png", "text": "ol oe\nAytention\naa ip ey eye) (9) fey fd fee] fey fey", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0013.png", "text": "Down by the river[bank]\n——\nchee Eee bed 2 a", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0014.png", "text": "Compute==—\ntn History 23ee2-\nReanenaebe i\nmama\nNoein eewacetnia weceentic\nTSS *\noe tanned octane +\neatgea, cthe pyr oF 5 3 J\n\nMeanaer™ t\nn-type,\n3Enel Brown vI", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_05_frame_0015.png", "text": "Deep Learning Series As a talk\nxO ve\n= - £9 _.2¢ a", "timestamp": "0015"}], "processing_info": {"audio_duration": 456.88, "total_segments": 110, "total_ocr_extractions": 14}}
{"video_id": "talk_06", "url": "https://www.youtube.com/watch?v=wjZofJX0v4M  # 6. Transformers, the tech behind LLMs | Chapter 5", "timestamp": "2025-07-31T17:02:05.408017", "asr_transcript": {"video_id": "talk_06", "language": "en", "text": " The initials GPT stand for Generative Pre-Trained Transformer. So that first word is straightforward enough, these are bots that generate new text. Pre-trained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine tune it on specific tasks with additional training. But the last word, that's the real key piece. A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI. What I want to do with this video and the following chapters is go through a visually driven explanation for what actually happens inside a transformer. We're going to follow the data that flows through it and go step by step. There are many different kinds of models that you can build using transformers. Some models take in audio and produce a transcript. This sentence comes from a model going the other way around, producing synthetic speech just from text. All those tools that took the world by storm in 2022, like Dolly and Mid-Journey that take in a text description and produce an image are based on transformers. And even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible. And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another. But the variant that you and I will focus on, which is the type that underlies tools like chat GPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage. That prediction takes the form of a probability distribution over many different chunks of text that might follow. At first glance, you might think that predicting the next word feels like a very different goal from generating new text. But once you have a prediction model like this, a simple thing you could try to make it generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added. I don't know about you, but it really doesn't feel like this should actually work. In this animation, for example, I'm running GPT2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text. And the story just doesn't actually really make that much sense. But if I swap it out for API calls to GPT3 instead, which is the same basic model just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pie creature would live in a land of math and computation. This process here of repeated prediction and sampling is essentially what's happening when you interact with chat GPT or any of these other large language models, and you see them producing one word at a time. In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses. Let's kick things off with a very high level preview of how data flows through a transformer. We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood. First, the input is broken up into a bunch of little pieces. These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations. If images or sound are involved, then tokens could be little patches of that image, or little chunks of that sound. Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece. If you think of these vectors as giving coordinates in some very high-dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space. This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values. For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model. The attention block is what's responsible for figuring out which words in the context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated. And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors. After that, these vectors pass through a different kind of operation, and depending on the source that you're reading, this will be referred to as a multi-layer perceptron, or maybe a feed-forward layer, and here the vectors don't talk to each other. They all go through the same operation in parallel, and while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions. All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices. I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview. After that, the process essentially repeats. You go back and forth between attention blocks and multi-layer perceptron blocks. Until at the very end, the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence. We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next. And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over. Some of you in the know may remember how long before chat GPT came into the scene, this is what early demos of GPT3 looked like. You would have it auto-complete stories and essays based on an initial snippet. To make a tool like this into a chat bot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response. There is more to say about an added step of training that's required to make this work well, but at a high level this is the general idea. In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around. If you're comfortable with that background knowledge and a little impatient, you could probably feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer. After that, I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point. For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformer specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning. At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves. What I mean by that is let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition. We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior. For example, maybe the simplest form of machine learning is linear regression, where your inputs and your outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices. That line is described by two continuous parameters, say the slope and the y intercept, and the goal of linear regression, is to determine those parameters to closely match the data. Needless to say, deep learning models get much more complicated, GPT-3 for example, has not two, but 175 billion parameters, but here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data, or being completely intractable to train. Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well. What unifies them is that they all use the same training algorithm, it's called back propagation, we talked about it in previous chapters, and the context that I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format, and if you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling kind of arbitrary. First, whatever kind of model you're making, the input has to be formatted as an array of real numbers. This could simply mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor. You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output. For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens. In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums. You also sprinkle some nonlinear functions throughout, but they won't depend on parameters. Typically, though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product. It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum. It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters that transform vectors that are drawn from the data being processed. For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices. Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does. As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from. Even if nowadays there are bigger and better models, this one has a certain charm as the first large language model to really capture the world's attention outside of ML communities. Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks. I just want to set the scene going in that as you peek under the hood to see what happens inside a tool like JGPT, almost all of the actual computation looks like matrix vector multiplication. There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray. The weights are the actual brains. They are the things learned during training, and they determine how it behaves. The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text. With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors. I mentioned how those chunks are called tokens, which might be pieces of words or punctuation. But every now and then in this chapter, and especially in the next one, I'd like to just pretend that it's broken more cleanly into words, because we humans think in words, this'll just make it much easier to reference little examples and clarify each step. The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words. These columns are what determines what vector each word turns into in that first step. We label it w-e, and like all the matrices we see, its values begin random, but they're going to be learned based on data. Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sits the foundation for everything that follows, so let's take a moment to get familiar with it. We often call this embedding a word, which invites you to think of these vectors very geometrically, as points in some high dimensional space. Visualizing a list of three numbers, as coordinates for points in 3D space, would be no problem, but word embeddings tend to be much, much higher dimensional. In GPT-3, they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions. In the same way that you could take a two-dimensional slice through a 3D space, and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results. The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning. For the simple word-to-vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes. And if you want to pull up some Python and play along at home, this is the specific model that I'm using to make the animations. It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning. A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector in the space connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen. So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point. At least kind of, despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way that queen is used in training data is not merely a feminine version of king. When I played around family relations seem to illustrate the idea much better. The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information. Another example is that if you take the embedding of Italy and you subtract the embedding of Germany and then you add that to the embedding of Hitler, you get something very close to the embedding of Mussolini. It's as if the model learned to associate some directions with Italianness and others with World War II axis leaders. Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan and you add it to sushi, you end up very close to broadwurst. Also in playing this game of finding nearest neighbors, I was very pleased to see how close cat was to both beast and monster. One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align. Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good since so much of our computation has to look like weighted sums. Geometrically, the dot product is positive when vectors point in similar directions. It's zero if they're perpendicular and it's negative whenever they point in opposite directions. For example, let's say you were playing with this model and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space. To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns and compare it to the dot products with the corresponding plural nouns. If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction. It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3 and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word. Again, the specifics for how words get embedded is learned using data. This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model, and using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se but of tokens, and the embedding dimension is 12,288. Multiplying those tells us this consists of about 617 million weights. Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion. In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words. For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context. A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language. Think about your own understanding of a given word. The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently. To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings. But you should think of the primary goal of this network that it flows through, as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent. The network can only process a fixed number of vectors at a time, known as its context size. For GPT-3, it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions. This context size limits how much text the transformer can incorporate when it's making a prediction of the next word. This is why long conversations with certain chatbots, like the early versions of chat GPT, often gave the feeling of the bot kind of losing the threat of conversation as you continued too long. We'll go into the details of attention in due time, but skipping ahead, I want to talk for a minute about what happens at the very end. Remember, the desired output is a probability distribution over all tokens that might come next. For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately proceeding, we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape. This involves two different steps. The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary. Then there's a function that normalizes this into a probability distribution. It's called Softmax, and we'll talk more about it in just a second, but before that, it might seem a little bit weird to only use this last embedding to make a prediction. When after all, in that last step, there are thousands of other vectors in the layer just sitting there with their own context-rich meanings. This has to do with the fact that in the training process, it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it. There's a lot more to be said about training later on, but I just want to call that out right now. This matrix is called the Unimbedding Matrix, and we give it the label WU. Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process. Keeping score on our total parameter count, this unimbedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension. It's very similar to the embedding matrix just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far, is a little over a billion. A small, but not wholly insignificant fraction of the 175 billion that we'll end up with in total. As the very last mini-lesson for this chapter, I want to talk more about the softmax function, since it makes another appearance for us once we dive into the attention blocks. The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1. However, if you're playing the deep learning game, where everything you do looks like matrix vector multiplication, the outputs that you get by defaults don't abide by this at all. The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1. Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution, in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0. That's all you really need to know. But if you're curious, the way that it works, is to first raise E to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values, and divide each term by that sum, which normalizes it into a list that adds up to 1. You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output, the corresponding term dominates the distribution, so if you were sampling from it, you'd almost certainly just be picking the maximizing input. But it's softer than just picking the max, in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs. In some situations, like when ChatchyPt is using this distribution to create a next word, there's room for a little bit of extra fun, by adding a little extra spice into this function, with a constant T thrown into the denominator of those exponents. We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when T is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if T is smaller, then the bigger values will dominate more aggressively. We're in the extreme, setting T equal to zero means all of the weight goes to that maximum value. For example, I'll have GPT3 generate a story with the seed text once upon a time there was A, but I'm going to use different temperatures in each case. Temperature zero means that it always goes with the most predictable word, and what you get ends up being kind of a trite derivative of goldilocks. A higher temperature gives it a chance to choose less likely words, but it comes with a risk. In this case, the story starts out a bit more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense. Technically speaking, the API doesn't actually let you pick a temperature bigger than two. There is no mathematical reason for this, it's just an arbitrary constraint imposed, I suppose, to keep their tool from being seen generating things that are too nonsensical. So if you're curious the way this animation is actually working, is I'm taking the 20 most probable next tokens that GPT3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of one fifth. As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm going to say logits. So for instance, when you feed in some text, do you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output, as the logits for the next word prediction. A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid Waxon Wax Off-Style. You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication, with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI, should be relatively smooth. For that, come join me in the next chapter. As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters. A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review. In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.", "segments": [{"start": 0.0, "end": 4.96, "text": "The initials GPT stand for Generative Pre-Trained Transformer."}, {"start": 4.96, "end": 9.52, "text": "So that first word is straightforward enough, these are bots that generate new text."}, {"start": 9.52, "end": 14.8, "text": "Pre-trained refers to how the model went through a process of learning from a massive amount of data,"}, {"start": 14.8, "end": 20.56, "text": "and the prefix insinuates that there's more room to fine tune it on specific tasks with additional training."}, {"start": 20.56, "end": 23.28, "text": "But the last word, that's the real key piece."}, {"start": 23.28, "end": 27.68, "text": "A transformer is a specific kind of neural network, a machine learning model,"}, {"start": 27.68, "end": 31.52, "text": "and it's the core invention underlying the current boom in AI."}, {"start": 31.52, "end": 35.92, "text": "What I want to do with this video and the following chapters is go through a visually driven"}, {"start": 35.92, "end": 40.8, "text": "explanation for what actually happens inside a transformer. We're going to follow the data that"}, {"start": 40.8, "end": 46.239999999999995, "text": "flows through it and go step by step. There are many different kinds of models that you can build"}, {"start": 46.239999999999995, "end": 51.120000000000005, "text": "using transformers. Some models take in audio and produce a transcript."}, {"start": 51.120000000000005, "end": 56.480000000000004, "text": "This sentence comes from a model going the other way around, producing synthetic speech just from text."}, {"start": 56.559999999999995, "end": 61.919999999999995, "text": "All those tools that took the world by storm in 2022, like Dolly and Mid-Journey that take in a text"}, {"start": 61.919999999999995, "end": 67.2, "text": "description and produce an image are based on transformers. And even if I can't quite get it"}, {"start": 67.2, "end": 71.92, "text": "to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is"}, {"start": 71.92, "end": 78.32, "text": "even remotely possible. And the original transformer introduced in 2017 by Google was invented for the"}, {"start": 78.32, "end": 84.0, "text": "specific use case of translating text from one language into another. But the variant that you and I"}, {"start": 84.0, "end": 89.36, "text": "will focus on, which is the type that underlies tools like chat GPT, will be a model that's trained"}, {"start": 89.36, "end": 94.88, "text": "to take in a piece of text, maybe even with some surrounding images or sound accompanying it,"}, {"start": 94.88, "end": 99.84, "text": "and produce a prediction for what comes next in the passage. That prediction takes the form of a"}, {"start": 99.84, "end": 105.52, "text": "probability distribution over many different chunks of text that might follow. At first glance,"}, {"start": 105.52, "end": 109.2, "text": "you might think that predicting the next word feels like a very different goal from generating"}, {"start": 110.16, "end": 114.16, "text": "new text. But once you have a prediction model like this, a simple thing you could try to make"}, {"start": 114.16, "end": 119.2, "text": "it generate a longer piece of text is to give it an initial snippet to work with, have it take a"}, {"start": 119.2, "end": 124.48, "text": "random sample from the distribution it just generated, append that sample to the text, and then run"}, {"start": 124.48, "end": 129.04, "text": "the whole process again to make a new prediction based on all the new text, including what it just"}, {"start": 129.04, "end": 133.2, "text": "added. I don't know about you, but it really doesn't feel like this should actually work."}, {"start": 133.2, "end": 138.16, "text": "In this animation, for example, I'm running GPT2 on my laptop and having it repeatedly"}, {"start": 138.16, "end": 142.72, "text": "predict and sample the next chunk of text to generate a story based on the seed text."}, {"start": 142.72, "end": 147.76, "text": "And the story just doesn't actually really make that much sense. But if I swap it out for"}, {"start": 147.76, "end": 154.72, "text": "API calls to GPT3 instead, which is the same basic model just much bigger, suddenly almost magically"}, {"start": 154.72, "end": 159.51999999999998, "text": "we do get a sensible story, one that even seems to infer that a pie creature would live in a land"}, {"start": 159.51999999999998, "end": 164.96, "text": "of math and computation. This process here of repeated prediction and sampling is essentially what's"}, {"start": 164.96, "end": 169.76000000000002, "text": "happening when you interact with chat GPT or any of these other large language models,"}, {"start": 169.76000000000002, "end": 174.64000000000001, "text": "and you see them producing one word at a time. In fact, one feature that I would very much enjoy"}, {"start": 174.64000000000001, "end": 179.12, "text": "is the ability to see the underlying distribution for each new word that it chooses."}, {"start": 183.92000000000002, "end": 188.4, "text": "Let's kick things off with a very high level preview of how data flows through a transformer."}, {"start": 188.4, "end": 192.8, "text": "We will spend much more time motivating and interpreting and expanding on the details of each"}, {"start": 192.8, "end": 197.76000000000002, "text": "step, but in broad strokes, when one of these chatbots generates a given word, here's what's going"}, {"start": 197.76000000000002, "end": 203.04000000000002, "text": "on under the hood. First, the input is broken up into a bunch of little pieces. These pieces are"}, {"start": 203.04000000000002, "end": 208.32000000000002, "text": "called tokens, and in the case of text these tend to be words or little pieces of words or other"}, {"start": 208.32000000000002, "end": 214.64000000000001, "text": "common character combinations. If images or sound are involved, then tokens could be little patches"}, {"start": 214.64000000000001, "end": 220.0, "text": "of that image, or little chunks of that sound. Each one of these tokens is then associated with a"}, {"start": 220.0, "end": 225.76, "text": "vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece."}, {"start": 225.76, "end": 229.92, "text": "If you think of these vectors as giving coordinates in some very high-dimensional space,"}, {"start": 229.92, "end": 235.12, "text": "words with similar meanings tend to land on vectors that are close to each other in that space."}, {"start": 235.12, "end": 239.6, "text": "This sequence of vectors then passes through an operation that's known as an attention block,"}, {"start": 239.6, "end": 243.76, "text": "and this allows the vectors to talk to each other and pass information back and forth to update"}, {"start": 243.76, "end": 249.04, "text": "their values. For example, the meaning of the word model in the phrase a machine learning model"}, {"start": 249.04, "end": 253.2, "text": "is different from its meaning in the phrase a fashion model. The attention block is what's"}, {"start": 253.2, "end": 258.0, "text": "responsible for figuring out which words in the context are relevant to updating the meanings of"}, {"start": 258.0, "end": 263.44, "text": "which other words, and how exactly those meanings should be updated. And again, whenever I use the"}, {"start": 263.44, "end": 269.52, "text": "word meaning, this is somehow entirely encoded in the entries of those vectors. After that,"}, {"start": 269.52, "end": 273.6, "text": "these vectors pass through a different kind of operation, and depending on the source that you're"}, {"start": 273.6, "end": 278.4, "text": "reading, this will be referred to as a multi-layer perceptron, or maybe a feed-forward layer,"}, {"start": 278.4, "end": 282.88, "text": "and here the vectors don't talk to each other. They all go through the same operation in parallel,"}, {"start": 282.88, "end": 287.2, "text": "and while this block is a little bit harder to interpret, later on we'll talk about how the step"}, {"start": 287.2, "end": 292.23999999999995, "text": "is a little bit like asking a long list of questions about each vector, and then updating them"}, {"start": 292.23999999999995, "end": 297.44, "text": "based on the answers to those questions. All of the operations in both of these blocks"}, {"start": 297.44, "end": 303.28, "text": "look like a giant pile of matrix multiplications, and our primary job is going to be to understand"}, {"start": 303.91999999999996, "end": 309.59999999999997, "text": "how to read the underlying matrices. I'm glossing over some details about some normalization steps"}, {"start": 309.59999999999997, "end": 314.4, "text": "that happen in between, but this is after all a high-level preview. After that, the process"}, {"start": 314.4, "end": 319.35999999999996, "text": "essentially repeats. You go back and forth between attention blocks and multi-layer perceptron blocks."}, {"start": 320.32, "end": 324.88, "text": "Until at the very end, the hope is that all of the essential meaning of the passage"}, {"start": 324.88, "end": 329.84, "text": "has somehow been baked into the very last vector in the sequence. We then perform a certain"}, {"start": 329.84, "end": 335.44, "text": "operation on that last vector that produces a probability distribution over all possible tokens,"}, {"start": 335.44, "end": 340.79999999999995, "text": "all possible little chunks of text that might come next. And like I said, once you have a tool"}, {"start": 340.79999999999995, "end": 345.76, "text": "that predicts what comes next given a snippet of text, you can feed it a little bit of seed text"}, {"start": 345.76, "end": 350.64, "text": "and have it repeatedly play this game of predicting what comes next, sampling from the distribution,"}, {"start": 350.64, "end": 355.67999999999995, "text": "appending it, and then repeating over and over. Some of you in the know may remember how long"}, {"start": 355.68, "end": 361.28000000000003, "text": "before chat GPT came into the scene, this is what early demos of GPT3 looked like. You would have"}, {"start": 361.28000000000003, "end": 367.44, "text": "it auto-complete stories and essays based on an initial snippet. To make a tool like this into a chat"}, {"start": 367.44, "end": 372.8, "text": "bot, the easiest starting point is to have a little bit of text that establishes the setting of a user"}, {"start": 372.8, "end": 377.68, "text": "interacting with a helpful AI assistant, what you would call the system prompt, and then you would"}, {"start": 377.68, "end": 382.64, "text": "use the user's initial question or prompt as the first bit of dialogue, and then you have it start"}, {"start": 382.71999999999997, "end": 388.88, "text": "predicting what such a helpful AI assistant would say in response. There is more to say about an"}, {"start": 388.88, "end": 393.52, "text": "added step of training that's required to make this work well, but at a high level this is the general"}, {"start": 393.52, "end": 399.68, "text": "idea. In this chapter, you and I are going to expand on the details of what happens at the very"}, {"start": 399.68, "end": 404.32, "text": "beginning of the network, at the very end of the network, and I also want to spend a lot of time"}, {"start": 404.32, "end": 408.8, "text": "reviewing some important bits of background knowledge, things that would have been second nature"}, {"start": 408.88, "end": 413.76, "text": "to any machine learning engineer by the time transformers came around. If you're comfortable"}, {"start": 413.76, "end": 417.2, "text": "with that background knowledge and a little impatient, you could probably feel free to skip to the"}, {"start": 417.2, "end": 421.92, "text": "next chapter, which is going to focus on the attention blocks, generally considered the heart of"}, {"start": 421.92, "end": 426.88, "text": "the transformer. After that, I want to talk more about these multi-layer perceptron blocks,"}, {"start": 426.88, "end": 431.44, "text": "how training works, and a number of other details that will have been skipped up to that point."}, {"start": 432.08000000000004, "end": 436.64, "text": "For broader context, these videos are additions to a mini-series about deep learning,"}, {"start": 436.64, "end": 440.56, "text": "and it's okay if you haven't watched the previous ones, I think you can do it out of order,"}, {"start": 440.56, "end": 445.2, "text": "but before diving into transformer specifically, I do think it's worth making sure that we're on the"}, {"start": 445.2, "end": 450.71999999999997, "text": "same page about the basic premise and structure of deep learning. At the risk of stating the obvious,"}, {"start": 450.71999999999997, "end": 456.24, "text": "this is one approach to machine learning, which describes any model where you're using data to"}, {"start": 456.24, "end": 461.36, "text": "somehow determine how a model behaves. What I mean by that is let's say you want a function that"}, {"start": 461.36, "end": 466.08, "text": "takes in an image and it produces a label describing it, or our example of predicting the next"}, {"start": 466.15999999999997, "end": 471.84, "text": "word given a passage of text, or any other task that seems to require some element of intuition and"}, {"start": 471.84, "end": 476.24, "text": "pattern recognition. We almost take this for granted these days, but the idea with machine learning"}, {"start": 476.24, "end": 481.2, "text": "is that rather than trying to explicitly define a procedure for how to do that task in code,"}, {"start": 481.2, "end": 486.24, "text": "which is what people would have done in the earliest days of AI, instead you set up a very flexible"}, {"start": 486.24, "end": 492.15999999999997, "text": "structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many"}, {"start": 492.24, "end": 497.76000000000005, "text": "examples of what the output should look like for a given input to tweak and tune the values of those"}, {"start": 497.76000000000005, "end": 503.76000000000005, "text": "parameters to mimic this behavior. For example, maybe the simplest form of machine learning is linear"}, {"start": 503.76000000000005, "end": 508.8, "text": "regression, where your inputs and your outputs are each single numbers, something like the square"}, {"start": 508.8, "end": 514.8000000000001, "text": "footage of a house and its price, and what you want is to find a line of best fit through this data,"}, {"start": 514.8000000000001, "end": 520.4, "text": "you know, to predict future house prices. That line is described by two continuous parameters,"}, {"start": 520.4, "end": 525.76, "text": "say the slope and the y intercept, and the goal of linear regression, is to determine those"}, {"start": 525.76, "end": 532.4, "text": "parameters to closely match the data. Needless to say, deep learning models get much more complicated,"}, {"start": 532.4, "end": 540.16, "text": "GPT-3 for example, has not two, but 175 billion parameters, but here's the thing, it's not a given"}, {"start": 540.16, "end": 545.28, "text": "that you can create some giant model with a huge number of parameters without it either grossly"}, {"start": 545.36, "end": 551.6, "text": "overfitting the training data, or being completely intractable to train. Deep learning describes a"}, {"start": 551.6, "end": 557.04, "text": "class of models that in the last couple decades have proven to scale remarkably well. What unifies"}, {"start": 557.04, "end": 561.4399999999999, "text": "them is that they all use the same training algorithm, it's called back propagation, we talked about"}, {"start": 561.4399999999999, "end": 566.24, "text": "it in previous chapters, and the context that I want you to have as we go in is that in order for"}, {"start": 566.24, "end": 571.6, "text": "this training algorithm to work well at scale, these models have to follow a certain specific format,"}, {"start": 571.6, "end": 575.84, "text": "and if you know this format going in, it helps to explain many of the choices for how a"}, {"start": 575.84, "end": 580.5600000000001, "text": "transformer processes language, which otherwise run the risk of feeling kind of arbitrary."}, {"start": 581.28, "end": 586.16, "text": "First, whatever kind of model you're making, the input has to be formatted as an array of real"}, {"start": 586.16, "end": 591.44, "text": "numbers. This could simply mean a list of numbers, it could be a two-dimensional array, or very"}, {"start": 591.44, "end": 596.72, "text": "often you deal with higher dimensional arrays, where the general term used is tensor. You often"}, {"start": 596.72, "end": 602.8000000000001, "text": "think of that input data as being progressively transformed into many distinct layers, where again,"}, {"start": 602.8000000000001, "end": 607.36, "text": "each layer is always structured as some kind of array of real numbers, until you get to a final layer"}, {"start": 607.36, "end": 612.72, "text": "which you consider the output. For example, the final layer in our text processing model is a list of"}, {"start": 612.72, "end": 618.4, "text": "numbers representing the probability distribution for all possible next tokens. In deep learning,"}, {"start": 618.4, "end": 623.44, "text": "these model parameters are almost always referred to as weights, and this is because a key feature of"}, {"start": 623.44, "end": 628.8800000000001, "text": "these models is that the only way these parameters interact with the data being processed is through"}, {"start": 628.8800000000001, "end": 633.7600000000001, "text": "weighted sums. You also sprinkle some nonlinear functions throughout, but they won't depend on"}, {"start": 633.7600000000001, "end": 639.0400000000001, "text": "parameters. Typically, though, instead of seeing the weighted sums all naked and written out"}, {"start": 639.0400000000001, "end": 644.6400000000001, "text": "explicitly like this, you'll instead find them packaged together as various components in a matrix"}, {"start": 644.6400000000001, "end": 650.6400000000001, "text": "vector product. It amounts to saying the same thing, if you think back to how matrix vector multiplication"}, {"start": 650.64, "end": 656.24, "text": "works, each component in the output looks like a weighted sum. It's just often conceptually"}, {"start": 656.24, "end": 661.28, "text": "cleaner for you and me to think about matrices that are filled with tunable parameters that"}, {"start": 661.28, "end": 669.04, "text": "transform vectors that are drawn from the data being processed. For example, those 175 billion weights"}, {"start": 669.04, "end": 676.16, "text": "in GPT-3 are organized into just under 28,000 distinct matrices. Those matrices in turn fall into"}, {"start": 676.16, "end": 680.0799999999999, "text": "eight different categories, and what you and I are going to do is step through each one of those"}, {"start": 680.0799999999999, "end": 684.8, "text": "categories to understand what that type does. As we go through, I think it's kind of fun to"}, {"start": 684.8, "end": 691.04, "text": "reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from."}, {"start": 691.68, "end": 696.24, "text": "Even if nowadays there are bigger and better models, this one has a certain charm as the"}, {"start": 696.24, "end": 701.04, "text": "first large language model to really capture the world's attention outside of ML communities."}, {"start": 701.04, "end": 705.12, "text": "Also, practically speaking, companies tend to keep much tighter lips around the specific"}, {"start": 705.12, "end": 710.0, "text": "numbers for more modern networks. I just want to set the scene going in that as you peek under the"}, {"start": 710.0, "end": 715.92, "text": "hood to see what happens inside a tool like JGPT, almost all of the actual computation looks like"}, {"start": 715.92, "end": 720.48, "text": "matrix vector multiplication. There's a little bit of a risk getting lost in the sea of billions"}, {"start": 720.48, "end": 725.68, "text": "of numbers, but you should draw a very sharp distinction in your mind between the weights of the"}, {"start": 725.68, "end": 730.88, "text": "model, which I'll always color in blue or red, and the data being processed, which I'll always"}, {"start": 730.96, "end": 736.24, "text": "color in gray. The weights are the actual brains. They are the things learned during training,"}, {"start": 736.24, "end": 742.16, "text": "and they determine how it behaves. The data being processed simply encodes whatever specific input"}, {"start": 742.16, "end": 748.24, "text": "is fed into the model for a given run, like an example snippet of text. With all of that as"}, {"start": 748.24, "end": 753.28, "text": "foundation, let's dig into the first step of this text processing example, which is to break up"}, {"start": 753.28, "end": 757.92, "text": "the input into little chunks and turn those chunks into vectors. I mentioned how those chunks are"}, {"start": 757.92, "end": 762.7199999999999, "text": "called tokens, which might be pieces of words or punctuation. But every now and then in this"}, {"start": 762.7199999999999, "end": 766.8, "text": "chapter, and especially in the next one, I'd like to just pretend that it's broken more"}, {"start": 766.8, "end": 771.76, "text": "cleanly into words, because we humans think in words, this'll just make it much easier to reference"}, {"start": 771.76, "end": 778.3199999999999, "text": "little examples and clarify each step. The model has a predefined vocabulary, some list of all"}, {"start": 778.3199999999999, "end": 784.0799999999999, "text": "possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding"}, {"start": 784.1600000000001, "end": 790.72, "text": "matrix, has a single column for each one of these words. These columns are what determines what"}, {"start": 790.72, "end": 798.5600000000001, "text": "vector each word turns into in that first step. We label it w-e, and like all the matrices we see,"}, {"start": 798.5600000000001, "end": 804.96, "text": "its values begin random, but they're going to be learned based on data. Turning words into vectors"}, {"start": 804.96, "end": 809.6800000000001, "text": "was common practice in machine learning long before transformers, but it's a little weird if"}, {"start": 809.68, "end": 814.0, "text": "you've never seen it before, and it sits the foundation for everything that follows, so let's"}, {"start": 814.0, "end": 818.88, "text": "take a moment to get familiar with it. We often call this embedding a word, which invites you to"}, {"start": 818.88, "end": 824.7199999999999, "text": "think of these vectors very geometrically, as points in some high dimensional space. Visualizing a"}, {"start": 824.7199999999999, "end": 829.92, "text": "list of three numbers, as coordinates for points in 3D space, would be no problem, but word embeddings"}, {"start": 829.92, "end": 836.88, "text": "tend to be much, much higher dimensional. In GPT-3, they have 12,288 dimensions, and as you'll see,"}, {"start": 836.88, "end": 842.08, "text": "it matters to work in a space that has a lot of distinct directions. In the same way that you could"}, {"start": 842.08, "end": 847.52, "text": "take a two-dimensional slice through a 3D space, and project all the points onto that slice,"}, {"start": 847.52, "end": 851.84, "text": "for the sake of animating word embeddings that a simple model is giving me, I'm going to do an"}, {"start": 851.84, "end": 856.72, "text": "analogous thing by choosing a three-dimensional slice through this very high dimensional space,"}, {"start": 856.72, "end": 862.4, "text": "and projecting the word vectors down onto that and displaying the results. The big idea here is"}, {"start": 862.4, "end": 867.4399999999999, "text": "that as a model tweaks and tunes its weights to determine how exactly words get embedded as"}, {"start": 867.4399999999999, "end": 872.88, "text": "vectors during training, it tends to settle on a set of embeddings where directions in the space"}, {"start": 872.88, "end": 877.52, "text": "have a kind of semantic meaning. For the simple word-to-vector model I'm running here,"}, {"start": 877.52, "end": 882.0, "text": "if I run a search for all the words whose embeddings are closest to that of tower,"}, {"start": 882.0, "end": 886.96, "text": "you'll notice how they all seem to give very similar tower-ish vibes. And if you want to pull up"}, {"start": 886.96, "end": 891.6, "text": "some Python and play along at home, this is the specific model that I'm using to make the animations."}, {"start": 891.6, "end": 896.24, "text": "It's not a transformer, but it's enough to illustrate the idea that directions in the space"}, {"start": 896.24, "end": 902.0, "text": "can carry semantic meaning. A very classic example of this is how if you take the difference"}, {"start": 902.0, "end": 906.88, "text": "between the vectors for woman and man, something you would visualize as a little vector in the space"}, {"start": 906.88, "end": 912.0, "text": "connecting the tip of one to the tip of the other, it's very similar to the difference between"}, {"start": 912.0, "end": 918.72, "text": "king and queen. So let's say you didn't know the word for a female monarch, you could find it"}, {"start": 918.72, "end": 925.2, "text": "by taking king, adding this woman-man direction, and searching for the embeddings closest to that point."}, {"start": 926.72, "end": 931.12, "text": "At least kind of, despite this being a classic example for the model I'm playing with,"}, {"start": 931.12, "end": 935.0400000000001, "text": "the true embedding of queen is actually a little farther off than this would suggest,"}, {"start": 935.0400000000001, "end": 939.84, "text": "presumably because the way that queen is used in training data is not merely a feminine"}, {"start": 939.84, "end": 945.12, "text": "version of king. When I played around family relations seem to illustrate the idea much better."}, {"start": 946.0, "end": 950.5600000000001, "text": "The point is, it looks like during training the model found it advantageous to choose embeddings"}, {"start": 950.5600000000001, "end": 958.0, "text": "such that one direction in this space encodes gender information. Another example is that if you take"}, {"start": 958.0, "end": 963.2, "text": "the embedding of Italy and you subtract the embedding of Germany and then you add that to the"}, {"start": 963.2, "end": 969.2, "text": "embedding of Hitler, you get something very close to the embedding of Mussolini. It's as if the model"}, {"start": 969.2800000000001, "end": 975.6, "text": "learned to associate some directions with Italianness and others with World War II axis leaders."}, {"start": 976.32, "end": 981.2, "text": "Maybe my favorite example in this vein is how in some models, if you take the difference between"}, {"start": 981.2, "end": 986.08, "text": "Germany and Japan and you add it to sushi, you end up very close to broadwurst."}, {"start": 987.2800000000001, "end": 991.76, "text": "Also in playing this game of finding nearest neighbors, I was very pleased to see how close cat"}, {"start": 991.76, "end": 997.2800000000001, "text": "was to both beast and monster. One bit of mathematical intuition that's helpful to have in mind,"}, {"start": 997.28, "end": 1002.0, "text": "especially for the next chapter, is how the dot product of two vectors can be thought of as a way"}, {"start": 1002.0, "end": 1008.48, "text": "to measure how well they align. Computationally, dot products involve multiplying all the corresponding"}, {"start": 1008.48, "end": 1013.28, "text": "components and then adding the results, which is good since so much of our computation has to look"}, {"start": 1013.28, "end": 1020.24, "text": "like weighted sums. Geometrically, the dot product is positive when vectors point in similar directions."}, {"start": 1020.24, "end": 1025.52, "text": "It's zero if they're perpendicular and it's negative whenever they point in opposite directions."}, {"start": 1026.4, "end": 1031.68, "text": "For example, let's say you were playing with this model and you hypothesize that the embedding of"}, {"start": 1031.68, "end": 1038.32, "text": "cats minus cat might represent a sort of plurality direction in this space. To test this, I'm going to"}, {"start": 1038.32, "end": 1043.36, "text": "take this vector and compute its dot product against the embeddings of certain singular nouns"}, {"start": 1043.36, "end": 1047.84, "text": "and compare it to the dot products with the corresponding plural nouns. If you play around with"}, {"start": 1047.84, "end": 1052.32, "text": "this, you'll notice that the plural ones do indeed seem to consistently give higher values than"}, {"start": 1052.3999999999999, "end": 1058.0, "text": "the singular ones, indicating that they align more with this direction. It's also fun how if you"}, {"start": 1058.0, "end": 1064.24, "text": "take this dot product with the embeddings of the words 1, 2, 3 and so on, they give increasing values,"}, {"start": 1064.24, "end": 1068.8799999999999, "text": "so it's as if we can quantitatively measure how plural the model finds a given word."}, {"start": 1070.08, "end": 1074.8799999999999, "text": "Again, the specifics for how words get embedded is learned using data. This embedding matrix,"}, {"start": 1074.8799999999999, "end": 1079.6799999999998, "text": "whose columns tell us what happens to each word, is the first pile of weights in our model,"}, {"start": 1079.76, "end": 1086.24, "text": "and using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again,"}, {"start": 1086.24, "end": 1093.6000000000001, "text": "technically this consists not of words per se but of tokens, and the embedding dimension is 12,288."}, {"start": 1093.6000000000001, "end": 1098.88, "text": "Multiplying those tells us this consists of about 617 million weights. Let's go ahead and add"}, {"start": 1098.88, "end": 1103.8400000000001, "text": "this to a running tally, remembering that by the end we should count up to 175 billion."}, {"start": 1105.3600000000001, "end": 1109.2, "text": "In the case of transformers, you really want to think of the vectors in this embedding space"}, {"start": 1109.2, "end": 1114.56, "text": "as not merely representing individual words. For one thing, they also encode information about"}, {"start": 1114.56, "end": 1119.3600000000001, "text": "the position of that word, which we'll talk about later, but more importantly, you should think"}, {"start": 1119.3600000000001, "end": 1125.3600000000001, "text": "of them as having the capacity to soak in context. A vector that started its life as the embedding"}, {"start": 1125.3600000000001, "end": 1130.64, "text": "of the word king, for example, might progressively get tugged and pulled by various blocks in this"}, {"start": 1130.64, "end": 1136.24, "text": "network so that by the end it points in a much more specific and nuanced direction that somehow"}, {"start": 1136.24, "end": 1141.2, "text": "encodes that it was a king who lived in Scotland, and who had achieved his post after murdering"}, {"start": 1141.2, "end": 1146.16, "text": "the previous king, and who's being described in Shakespearean language. Think about your own"}, {"start": 1146.16, "end": 1151.76, "text": "understanding of a given word. The meaning of that word is clearly informed by the surroundings,"}, {"start": 1151.76, "end": 1156.72, "text": "and sometimes this includes context from a long distance away, so in putting together a model"}, {"start": 1156.72, "end": 1161.68, "text": "that has the ability to predict what word comes next, the goal is to somehow empower it to"}, {"start": 1161.68, "end": 1166.72, "text": "incorporate context efficiently. To be clear, in that very first step, when you create the array of"}, {"start": 1166.72, "end": 1171.6000000000001, "text": "vectors based on the input text, each one of those is simply plucked out of the embedding matrix,"}, {"start": 1171.6000000000001, "end": 1176.16, "text": "so initially each one can only encode the meaning of a single word without any input from its"}, {"start": 1176.16, "end": 1181.3600000000001, "text": "surroundings. But you should think of the primary goal of this network that it flows through,"}, {"start": 1181.3600000000001, "end": 1185.76, "text": "as being to enable each one of those vectors to soak up a meaning that's much more rich and"}, {"start": 1185.76, "end": 1191.2, "text": "specific than what mere individual words could represent. The network can only process a fixed"}, {"start": 1191.2, "end": 1196.64, "text": "number of vectors at a time, known as its context size. For GPT-3, it was trained with a context"}, {"start": 1196.64, "end": 1203.1200000000001, "text": "size of 2048, so the data flowing through the network always looks like this array of 2048 columns,"}, {"start": 1203.1200000000001, "end": 1208.8, "text": "each of which has 12,000 dimensions. This context size limits how much text the transformer can"}, {"start": 1208.8, "end": 1214.0, "text": "incorporate when it's making a prediction of the next word. This is why long conversations with"}, {"start": 1214.0, "end": 1219.2, "text": "certain chatbots, like the early versions of chat GPT, often gave the feeling of the bot kind of"}, {"start": 1219.2, "end": 1223.8400000000001, "text": "losing the threat of conversation as you continued too long. We'll go into the details of"}, {"start": 1223.8400000000001, "end": 1228.32, "text": "attention in due time, but skipping ahead, I want to talk for a minute about what happens at the very"}, {"start": 1228.32, "end": 1234.96, "text": "end. Remember, the desired output is a probability distribution over all tokens that might come next."}, {"start": 1234.96, "end": 1241.6000000000001, "text": "For example, if the very last word is Professor, and the context includes words like Harry Potter,"}, {"start": 1241.6000000000001, "end": 1246.32, "text": "and immediately proceeding, we see least favorite teacher, and also if you give me some leeway by"}, {"start": 1246.32, "end": 1250.72, "text": "letting me pretend that tokens simply look like full words, then a well-trained network that"}, {"start": 1250.72, "end": 1256.32, "text": "had built up knowledge of Harry Potter would presumably assign a high number to the word Snape."}, {"start": 1256.32, "end": 1261.84, "text": "This involves two different steps. The first one is to use another matrix that maps the very last"}, {"start": 1261.84, "end": 1267.6, "text": "vector in that context to a list of 50,000 values, one for each token in the vocabulary."}, {"start": 1268.24, "end": 1272.8, "text": "Then there's a function that normalizes this into a probability distribution. It's called"}, {"start": 1272.8, "end": 1277.04, "text": "Softmax, and we'll talk more about it in just a second, but before that, it might seem a little"}, {"start": 1277.04, "end": 1282.6399999999999, "text": "bit weird to only use this last embedding to make a prediction. When after all, in that last step,"}, {"start": 1282.6399999999999, "end": 1287.76, "text": "there are thousands of other vectors in the layer just sitting there with their own context-rich"}, {"start": 1287.76, "end": 1292.48, "text": "meanings. This has to do with the fact that in the training process, it turns out to be much more"}, {"start": 1292.48, "end": 1298.1599999999999, "text": "efficient if you use each one of those vectors in the final layer to simultaneously make a prediction"}, {"start": 1298.16, "end": 1302.88, "text": "for what would come immediately after it. There's a lot more to be said about training later on,"}, {"start": 1302.88, "end": 1307.8400000000001, "text": "but I just want to call that out right now. This matrix is called the Unimbedding Matrix,"}, {"start": 1307.8400000000001, "end": 1313.68, "text": "and we give it the label WU. Again, like all the weight matrices we see, its entries begin at random,"}, {"start": 1313.68, "end": 1318.3200000000002, "text": "but they are learned during the training process. Keeping score on our total parameter count,"}, {"start": 1318.3200000000002, "end": 1323.52, "text": "this unimbedding matrix has one row for each word in the vocabulary, and each row has the same"}, {"start": 1323.52, "end": 1328.24, "text": "number of elements as the embedding dimension. It's very similar to the embedding matrix just"}, {"start": 1328.24, "end": 1333.44, "text": "with the order swapped, so it adds another 617 million parameters to the network, meaning our"}, {"start": 1333.44, "end": 1339.92, "text": "count so far, is a little over a billion. A small, but not wholly insignificant fraction of the 175"}, {"start": 1339.92, "end": 1344.72, "text": "billion that we'll end up with in total. As the very last mini-lesson for this chapter,"}, {"start": 1344.72, "end": 1349.36, "text": "I want to talk more about the softmax function, since it makes another appearance for us once we dive"}, {"start": 1349.52, "end": 1354.6399999999999, "text": "into the attention blocks. The idea is that if you want a sequence of numbers to act as a"}, {"start": 1354.6399999999999, "end": 1360.32, "text": "probability distribution, say a distribution over all possible next words, then each value has to"}, {"start": 1360.32, "end": 1366.4799999999998, "text": "be between 0 and 1, and you also need all of them to add up to 1. However, if you're playing the"}, {"start": 1366.4799999999998, "end": 1371.9199999999998, "text": "deep learning game, where everything you do looks like matrix vector multiplication, the outputs"}, {"start": 1371.9199999999998, "end": 1377.04, "text": "that you get by defaults don't abide by this at all. The values are often negative, or much bigger"}, {"start": 1377.12, "end": 1382.32, "text": "than 1, and they almost certainly don't add up to 1. Softmax is the standard way to turn an"}, {"start": 1382.32, "end": 1388.08, "text": "arbitrary list of numbers into a valid distribution, in such a way that the largest values end up closest"}, {"start": 1388.08, "end": 1393.6, "text": "to 1, and the smaller values end up very close to 0. That's all you really need to know. But if you're"}, {"start": 1393.6, "end": 1398.96, "text": "curious, the way that it works, is to first raise E to the power of each of the numbers, which means"}, {"start": 1398.96, "end": 1404.24, "text": "you now have a list of positive values, and then you can take the sum of all those positive values,"}, {"start": 1404.24, "end": 1410.48, "text": "and divide each term by that sum, which normalizes it into a list that adds up to 1. You'll notice"}, {"start": 1410.48, "end": 1415.44, "text": "that if one of the numbers in the input is meaningfully bigger than the rest, then in the output,"}, {"start": 1415.44, "end": 1420.0, "text": "the corresponding term dominates the distribution, so if you were sampling from it, you'd almost"}, {"start": 1420.0, "end": 1425.36, "text": "certainly just be picking the maximizing input. But it's softer than just picking the max,"}, {"start": 1425.36, "end": 1429.84, "text": "in the sense that when other values are similarly large, they also get meaningful weight in the"}, {"start": 1429.84, "end": 1435.28, "text": "distribution, and everything changes continuously as you continuously vary the inputs. In some"}, {"start": 1435.28, "end": 1440.8799999999999, "text": "situations, like when ChatchyPt is using this distribution to create a next word, there's room for"}, {"start": 1440.8799999999999, "end": 1446.8, "text": "a little bit of extra fun, by adding a little extra spice into this function, with a constant T thrown"}, {"start": 1446.8, "end": 1452.1599999999999, "text": "into the denominator of those exponents. We call it the temperature, since it vaguely resembles the"}, {"start": 1452.1599999999999, "end": 1457.6799999999998, "text": "role of temperature in certain thermodynamics equations, and the effect is that when T is larger,"}, {"start": 1457.68, "end": 1462.64, "text": "you give more weight to the lower values, meaning the distribution is a little bit more uniform,"}, {"start": 1463.3600000000001, "end": 1466.96, "text": "and if T is smaller, then the bigger values will dominate more aggressively."}, {"start": 1467.6000000000001, "end": 1472.64, "text": "We're in the extreme, setting T equal to zero means all of the weight goes to that maximum value."}, {"start": 1473.28, "end": 1479.6000000000001, "text": "For example, I'll have GPT3 generate a story with the seed text once upon a time there was A,"}, {"start": 1480.3200000000002, "end": 1485.76, "text": "but I'm going to use different temperatures in each case. Temperature zero means that it always"}, {"start": 1485.76, "end": 1491.52, "text": "goes with the most predictable word, and what you get ends up being kind of a trite derivative of"}, {"start": 1491.52, "end": 1497.28, "text": "goldilocks. A higher temperature gives it a chance to choose less likely words, but it comes with"}, {"start": 1497.28, "end": 1502.24, "text": "a risk. In this case, the story starts out a bit more originally, about a young web artist from"}, {"start": 1502.24, "end": 1508.96, "text": "South Korea, but it quickly degenerates into nonsense. Technically speaking, the API doesn't actually"}, {"start": 1508.96, "end": 1513.2, "text": "let you pick a temperature bigger than two. There is no mathematical reason for this, it's just"}, {"start": 1513.2, "end": 1517.8400000000001, "text": "an arbitrary constraint imposed, I suppose, to keep their tool from being seen generating things"}, {"start": 1517.8400000000001, "end": 1522.56, "text": "that are too nonsensical. So if you're curious the way this animation is actually working,"}, {"start": 1522.56, "end": 1528.32, "text": "is I'm taking the 20 most probable next tokens that GPT3 generates, which seems to be the maximum"}, {"start": 1528.32, "end": 1532.4, "text": "they'll give me, and then I tweak the probabilities based on an exponent of one fifth."}, {"start": 1533.04, "end": 1537.2, "text": "As another bit of jargon, in the same way that you might call the components of the output of"}, {"start": 1537.2, "end": 1543.68, "text": "this function probabilities, people often refer to the inputs as logits, or some people say logits,"}, {"start": 1543.68, "end": 1548.16, "text": "some people say logits, I'm going to say logits. So for instance, when you feed in some text,"}, {"start": 1548.16, "end": 1552.16, "text": "do you have all these word embeddings flow through the network, and you do this final multiplication"}, {"start": 1552.16, "end": 1557.44, "text": "with the unembedding matrix, machine learning people would refer to the components in that raw,"}, {"start": 1557.44, "end": 1561.28, "text": "unnormalized output, as the logits for the next word prediction."}, {"start": 1562.24, "end": 1567.28, "text": "A lot of the goal with this chapter was to lay the foundations for understanding the attention"}, {"start": 1567.28, "end": 1573.92, "text": "mechanism, Karate Kid Waxon Wax Off-Style. You see, if you have a strong intuition for word embeddings,"}, {"start": 1573.92, "end": 1579.36, "text": "for softmax, for how dot products measure similarity, and also the underlying premise that most"}, {"start": 1579.36, "end": 1584.48, "text": "of the calculations have to look like matrix multiplication, with matrices full of tunable parameters,"}, {"start": 1585.2, "end": 1590.8799999999999, "text": "then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI,"}, {"start": 1590.88, "end": 1594.4, "text": "should be relatively smooth. For that, come join me in the next chapter."}, {"start": 1596.48, "end": 1601.5200000000002, "text": "As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters."}, {"start": 1601.5200000000002, "end": 1606.0, "text": "A final version should be up in public in a week or two, it usually depends on how much I end up"}, {"start": 1606.0, "end": 1610.48, "text": "changing based on that review. In the meantime, if you want to dive into attention, and if you want"}, {"start": 1610.48, "end": 1612.88, "text": "to help the channel out a little bit, it's there waiting."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0000.png", "text": "a", "timestamp": "0000"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0001.png", "text": "Generative Pre-trained Transformer", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0002.png", "text": "text-to-image .\n\netn paaeqrng sf a cate y\n\nstcerce te aaine ad Transformer |? be oY aS\nBes ~", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0003.png", "text": "Behold, a wild pi creature,\nforeging in tts native\n“\nBS.", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0004.png", "text": "Rehold, a wild pi creature,\nforaging in ite cael\n-.", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0005.png", "text": "Tetad, a wld pr creature,\nferagnnag st ris tiNtsve habitat of\nmnatnenmatica, kamal nid\ncompater code! With, sts infinite en\ncagits werd irtatsonal a\ntendeacen, tis strange telex [se\ncreatuce sm Ts\net I\nTransformer ® 27'S\n= A\nwewces 8\nGPT3 ba\nmens ow\nmaw", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0006.png", "text": "weirg the “thungat paces of te\n\nrnddel These: prubad.tity str butoes\n\nrepenent the inodels urcertanding af\n\nlangage ane ts abbity i predict he\n\nPrev! wort oaned on previo wuts\n“en", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0007.png", "text": "‘Tokens\nTa date, the cldverest thinker of all time was\n7", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0008.png", "text": "Rial Bh mee 5", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0009.png", "text": "al we Ee\n\nlit aoe\n\n|\ni\na\ni\n7", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0011.png", "text": "Tu date, the cleverest thucker of all tue wad ne\n{\nI\nI\nUP ey | : H\neee me! o ° :\npiety Eo, i\ntet tee te Mell | 1\nPU Yn rt \"He", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0012.png", "text": "To date, the cleverest thiake: of al\ntime wae undoubtedly Binmte:e. for\nhit theonven of Relativity. wae\ncompletely revolutionized our\nunderstanding of the phymcal wo-ki\na) teat the g:ouadwork for modern\noe He physics, H.s ideas aout the\na ie relat onship geteren space —___\nata: fill\neoxr a\n~ a\nwon\nroy\naatce ot\na\n«", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0013.png", "text": "What Zollows 6.8 conversation\nbetwen a iver and a helpful, very\nknowledgeable Al os.stant\na\nUser: Give me some ideas for what el\nto do when youting Saatiago ts a\nwale,\nAI Amistant: Sure, there are ty heen? weer: on\nplenty of things to do un JSUT CITAGT a ann\nSantiago! One option could be to Cal\nwe\ntake a coed\n“\nom", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0014.png", "text": "fo Attention\ni itrvyrity\n(J | bide dada\ni ee eae gea\nlace\n‘\\ ote\nCc} ae\nNL", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0015.png", "text": "; |", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0016.png", "text": "Input Model Output\n. Tt? —?> — coffee mug\nae\nInput Model Output\nbor wo, a0 Laat te Mandar\nDelano ace agua\nUr oe pet. kealting, aed — daughter\nsme coaseag, The pre bat\nfe eto a cow at be", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0017.png", "text": "Linear Regression\n37 : . .\n: Square fontage", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0018.png", "text": "Tnput 175B Parameters Output\nwes Dot & reckies ioaanes, the\nfoanaer te which be mud them\nwordy aloud under tbe — —— negligence\nGeot-wmuling chro, Dox wer\nWwe expremmey of", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0019.png", "text": "77 Pile of matrices\n; Deep Learning", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0020.png", "text": "Input\npp\ni =\nOutput", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0021.png", "text": "Input\nwy ry teat, tears t+ wala\nN\nN Output\nheseunaa fo Me ne ies\nvp ee te an é\nwae wenn bs ~", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0022.png", "text": "Weights\n4 fh Oy Coppa UE RU be Beare\ncn tay gy Cy [Dae Pon On ee Oe eee\nMy ey Fare eres ee ee ne, a\nCA my hy fA dla, bets ete Reo bees bnads\nInput Output\nSavas avd — 2", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0023.png", "text": "We will break this down", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0025.png", "text": "The goal! of our model is to predict the nex\n27?", "timestamp": "0025"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0026.png", "text": ". | words, ~ 50k", "timestamp": "0026"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0027.png", "text": "\" nraedding inattix\naardvark — if TK", "timestamp": "0027"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0028.png", "text": "3d_ vectors Word vectors\nT 30 ve\nI, .\nL/ z\n,\n+ .", "timestamp": "0028"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0030.png", "text": "\\ E(woman) - E(man)\nI Finan}\n|\ni Foaci!\nt\n4\nL .\n1", "timestamp": "0030"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0031.png", "text": "wir oT", "timestamp": "0031"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0032.png", "text": "B(Hitla) + EQ.) - B(Germany) ©\n(Germany =", "timestamp": "0032"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0033.png", "text": "Embeddings closest to Kécati\nE dog!\nE rasbiti\nBonackes +\nFE oeatar\nKicat)", "timestamp": "0033"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0034.png", "text": "vary\n. my ~\nv wyiey\noo Wy\n2 +\nWw se feds] crate\n. Wy °\n1; '\nnee rw,\nDot product ll\n3.73", "timestamp": "0034"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0035.png", "text": "e Oem BS Bed\n“hee PRP) 8) — 162\ncre", "timestamp": "0035"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0036.png", "text": "We -\nEmbedding matrix", "timestamp": "0036"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0037.png", "text": "The sine\n:\ni", "timestamp": "0037"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0038.png", "text": "The King doth wake tonight and takes his rouse ..\nfoe +\n. Beat Sern\nna \"OSSop", "timestamp": "0038"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0040.png", "text": "|\nContext size = 2.048\n“te acter. tw\nSee bee na x\nen oe", "timestamp": "0040"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0044.png", "text": "Wiis PES", "timestamp": "0044"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0045.png", "text": "Mini-lesson on Softmax\nPPpee\nEpa 2: i]\nones\nete |\neae -\nme\nae a\nww“", "timestamp": "0045"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0046.png", "text": "x]\n| i 0.20\n. SARA", "timestamp": "0046"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0047.png", "text": "softmax\n\nux 0 fie uo\n\n51 ett fie Ou)\n\n“05 efor aos | 0\n\n+1 | —+ eefhe =] 012 | =\n\n“34 eft foe 12 |) =!\n22 efi 000\n\n“24 of fuer 022 |", "timestamp": "0047"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0048.png", "text": "Temp — 0 Temp — 5\nOster crete ates tt waa chet gil Once emia tere rae 0 young and\nnamed Gartuocks Sie lived ee cagaring web artet frum Sut Koren by tle\n= ser\n; oP\n7 rs\n<3 wary\nit “E\nit on\n” Ce lal", "timestamp": "0048"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0049.png", "text": "softinax with Lempers tise\n3 m1 oN\n\n+60 ae hia oo7]) a\n\n3 oihe van\n\n“40 aye ou2 | 4\n\n“15 | — ewfyer |} ago]\n\n“99 nln iY |\n23 mpte: 0.00\n\n40 ae be 002 Jt", "timestamp": "0049"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0050.png", "text": "Temp — 0 Temp — 5\nOst ce tose thes nas venue and\nroperng art artnt from Souls Kurea lo,\noat\ncell", "timestamp": "0050"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0051.png", "text": "*\nTemp — 0 Temp — 5\nfone he ug Chaioceahip mates os ame\nJontabe thal thine stele on sae\nplay the only amy ne col do that beck\nea th: cpdedteal The\na\ni\"\nHe\ni\nwe\nced\neae\nwh", "timestamp": "0051"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0053.png", "text": "Next up: Attention\n2. oe oe et ret om\n4obogobue soa\npert eee\nbhp bbe oes\nSasa aa aa\na a ee ae ee ee a eS\nCr a oe Oe oe ae ee ed\n[et ee oe er ee ee ee eS\nfr ie ce Se ae ce ee Y\nparen Warr err arr arr wrrary re\nCr ne 2c? ore ae Se ae ore oY\nimi ol Tone cea ea ce ee\nfemme To ce ee ee ee ea we en FS", "timestamp": "0053"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_06_frame_0054.png", "text": "MW Clicky Stuffs 7,\nLinton of suecisor cesses these lessons are upg tes\nthreeth by ese. sueleas thine below |S bes snpsert\na\nteaguea tery nave tne te ee es Venton\nbentin rae tne Come How Soy dt Sewer\nCee foe onan Rerwee p waa porenves\nvee tee on tee Srtecae merce\nbow bance Deen yas bee bas Se nee", "timestamp": "0054"}], "processing_info": {"audio_duration": 1612.88, "total_segments": 306, "total_ocr_extractions": 47}}
{"video_id": "talk_07", "url": "https://www.youtube.com/watch?v=eMlx5fFNoYc  # 7. Attention in transformers, step-by-step | Chapter 6", "timestamp": "2025-07-31T17:03:02.246014", "asr_transcript": {"video_id": "talk_07", "language": "en", "text": " In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models and a lot of other tools in the modern wave of AI. It first hit the scene in a now famous 2017 paper called Attention is All You Need. And in this chapter, you and I will dig into what this attention mechanism is, visualizing how it processes data. As a quick recap, here's the important context I want you to have in mind. The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next. The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words. But just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words. The first step in a transformer is to associate each token with a high dimensional vector, what we call its embedding. Now the most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning. In the last chapter, we saw an example for how direction can correspond to gender. In the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun. Just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning. The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning. I should say at front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in. I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable. Consider the phrase's American Shrew Moll, one mole of carbon dioxide, and take a biopsy of the mole. You and I know that the word mole has different meanings in each one of these, based on the context. But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all three of these cases, because this initial token embedding is effectively a lookup table with no reference to the context. It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one. The picture you might have in mind is that there are multiple distinct directions in this embedding space, encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these more specific directions as a function of the context. To take another example, consider the embedding of the word tower. This is presumably some very generic, non-specific direction in the space associated with lots of other large, tall nouns. If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel Tower, maybe correlated with vectors associated with Paris and France and things made of steel. If it was also preceded by the word miniature, then the vector should be updated even further so that it no longer correlates with large tall things. More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially one's that are quite far away, and potentially with information that's much richer than just a single word. What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation that you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence. So imagine for example that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was, if the model's going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word. To step through the computations though, let's take a much simpler example, imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest, and for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns. What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel. Again, the initial embedding for each word is some high-dimensional vector that only encodes the meaning of that particular word with no context. Actually, that's not quite true. They also encode the position of the word. There's a lot more to say about the specific way the positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context. Let's go ahead and denote these embeddings with the letter E. The goal is to have a series of computations produce a new refined set of embeddings, where for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives. And playing the deep learning game, we want most of the computations involved to look like matrix vector products, where the matrices are full of tunable weights, things that the model will learn based on data. To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an intention head doing. As with so much deep learning, the true behavior is much harder to parse, because it's based on tweaking and tuning a huge number of parameters to minimize some cost function. It's just that as we step through all of the different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete. For the first step of this process, you might imagine each noun, like a creature, asking the question, hey, are there any adjectives sitting in front of me? And for the words fluffy and blue to each be able to answer, yeah, I'm an adjective and I'm in that position. That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word. This query vector though has a much smaller dimension than the embedding vector, say 128. Computing this query looks like taking a certain matrix, which I'll label WQ, and multiplying it by the embedding. Compressing things a bit, let's write that query vector as Q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrows start gives you the vector at the arrows end. In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token. The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice what this matrix does in a particular attention head is challenging to parse. But for R-sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions. As to what it does to other embeddings, who knows? Maybe it simultaneously tries to accomplish some other goal with those, right now where laser focused on the nouns. At the same time associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings. This produces a second sequence of vectors that we call the keys. Conceptually you want to think of the keys as potentially answering the queries. This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space. You think of the keys as matching the queries whenever they closely align with each other. In our example, you would imagine that the key matrix maps the adjectives, like fluffy and blue, to vectors that are closely aligned with the query produced by the word creature. To measure how well each key matches each query, you compute a dot product between each possible key query pair. I like to visualize a grid full of a bunch of dots where the bigger dots correspond to the larger dot products, the places where the keys and queries align. For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers. In the lingo machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot product between the key for some other word, like the and the query for creature, would be some small or negative value that reflects that these are unrelated to each other. So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word. The way we're about to use these scores is to take a certain weighted sum along each column weighted by the relevance. So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between zero and one, and for each column to add up to one as if they were a probability distribution. If you're coming in from the last chapter, you know what we need to do then. We compute a softmax along each one of these columns to normalize the values. In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values. At this point, you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top. We call this grid an attention pattern. Now if you look at the original transformer paper, there's a really compact way that they write this all down. Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices. This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries. A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space. Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column. As to that V term, we'll talk about it in just a second. Before that, there's one other technical detail that so far I've skipped. During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient. If you simultaneously have it predict every possible next token following each initial sub sequence of tokens in this passage. For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the. This is really nice because it means what would otherwise be a single training example effectively acts as many. For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next. What this means is that we want all of these spots here, the ones representing later tokens, influencing earlier ones, to somehow be forced to be zero. The simplest thing you might think to do is to set them equal to zero, but if you did that, the columns wouldn't add up to one anymore, they wouldn't be normalized. So instead, a common way to do this is that before applying softbacks, you set all of those entries to be negative infinity. If you do that, then after applying softbacks, all of those get turned into zero, but the columns stay normalized. This process is called masking. There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones. Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size. So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial. As you might imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics. Okay, great. Computing this pattern lets the model deduce which words are relevant to which other words. You need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to. For example, you want the embedding of fluffy to somehow cause a change to creature that moves it to a different part of this 12,000 dimensional embedding space that more specifically encodes a fluffy creature. What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention. This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example, fluffy. The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of creature. So this value vector lives in the same very high dimensional space as the embeddings. When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this? Looking back in our diagram, let's set aside all of the keys and the queries since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors. You might think of these value vectors as being kind of associated with the corresponding keys. For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column. For example, here, under the embedding of creature, you would be adding large proportions of the value vectors for fluffy and blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out. And then finally, the way to actually update the embedding associated with this column, obviously encoding some context-free meaning of creature, you add together all of these rescaled values in the column, producing a change that you want to add that I'll label Delta E, and then you add that to the original embedding. Hopefully, what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature. And of course, you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings produces a full sequence of more refined embeddings popping out of the attention block. Zooming out, this whole process is what you would describe as a single head of attention. As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value. I want to take a moment to continue what we started in the last chapter with the score keeping where we count up the total number of model parameters using the numbers from GPT-3. These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space. This gives us an additional 1.5 million or so parameters for each one. If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and its outputs live in this very large embedding space. If true, that would mean about 150 million added parameters. And to be clear, you could do that. You could devote orders of magnitude more parameters to the value map than to the key and query. But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query. This is especially relevant in the setting of running multiple attention heads in parallel. The way this looks is that the value map is factored as a product of two smaller matrices. Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs both in this larger embedding space, for example, taking the embedding of blue to this blueness direction that you would add to nouns. It's just that it's broken up into two separate steps. The first matrix on the right here has a smaller number of rows, typically the same size as the key query space. What this means, as you can think of it as mapping the large embedding vectors down to a much smaller space. This is not the conventional naming, but I'm going to call this the value down matrix. The second matrix, maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates. I'm going to call this one the value up matrix, which again is not conventional. The way that you would see this written in most papers looks a little different. I'll talk about it in a minute, in my opinion, it tends to make things a little more conceptually confusing. To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low-rank transformation. Turning back to the parameter count, all four of these matrices have the same size, and adding them all up, we get about 6.3 million parameters for one attention head. As a quick side note, to be a little more accurate, everything describes so far is what people would call a self attention head, to distinguish it from a variation that comes up in other models that's called cross attention. This isn't relevant to our GPT example, but if you're curious, cross attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription. Across attention head looks almost identical, the only difference is that the key and query maps act on different data sets. In a model-loving translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another. In this setting, there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones. Staying focused on self attention, though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is. All that's really left to us is to lay out the sense in which you do this many, many different times. In our central example, we focused on adjectives updating nouns, but of course, there are lots of different ways that context can influence the meaning of a word. If the words they crashed the, preceded the word car, it has implications for the shape and the structure of that car. And a lot of associations might be less grammatical. If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter. Whereas if instead the words queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince. For every different type of contextual updating that you might imagine, the parameters of these key inquiry matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings. And again, in practice, the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token. As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps. GPT-3, for example, uses 96 attention heads inside each block. Considering that each one is already a bit confusing, it's certainly a lot to hold in your head. Just to spell it all out very explicitly, this means you have 96 distinct key inquiry matrices, producing 96 distinct attention patterns, then each head has its own distinct value matrices used to produce 96 sequences of value vectors. These are all added together using the corresponding attention patterns as weights. What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position. So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position. This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it. Again, this is a lot to think about, so don't worry at all if it takes some time to sink in. The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning. Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention, ends up with around 600 million parameters. There's one added, slightly annoying thing that I should really mention for any of you who go on to read more about transformers. You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices. The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way, that would be a valid design. But the way that you see this written in papers and the way that it's implemented in practice looks a little different. All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block. And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down, projection into the smaller space. For the curious among you, I've left a non-screen note about it. It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources. Starting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block. For one thing, it also goes through these other operations called multi-layer perceptrons, we'll talk more about those in the next chapter, and then it repeatedly goes through many, many copies of both of these operations. What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings. The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure, things like sentiment and tone and whether it's a poem, and what underlying scientific truths are relevant to the piece and things like that. Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads. That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total. So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps. In the next chapter, you and I will talk more about those other blocks, and also a lot more about the training process. A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs. Even that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance. There's a huge advantage to parallelizable architectures that let you do this. If you want to learn more about this stuff, I've left lots of links in the description. In particular, anything produced by Andre Carpathia or Chris Ola tend to be pure gold. In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation. Also, Brit Cruise from the channel The Art of the Problem has a really nice video about the history of large language models.", "segments": [{"start": 0.0, "end": 4.36, "text": "In the last chapter, you and I started to step through the internal workings of a transformer."}, {"start": 4.36, "end": 7.88, "text": "This is one of the key pieces of technology inside large language models"}, {"start": 7.88, "end": 10.8, "text": "and a lot of other tools in the modern wave of AI."}, {"start": 10.8, "end": 15.56, "text": "It first hit the scene in a now famous 2017 paper called Attention is All You Need."}, {"start": 15.56, "end": 19.64, "text": "And in this chapter, you and I will dig into what this attention mechanism is,"}, {"start": 19.64, "end": 22.0, "text": "visualizing how it processes data."}, {"start": 22.0, "end": 30.12, "text": "As a quick recap, here's the important context I want you to have in mind."}, {"start": 30.12, "end": 34.76, "text": "The goal of the model that you and I are studying is to take in a piece of text and predict"}, {"start": 34.76, "end": 36.8, "text": "what word comes next."}, {"start": 36.8, "end": 41.56, "text": "The input text is broken up into little pieces that we call tokens, and these are very often"}, {"start": 41.56, "end": 43.56, "text": "words or pieces of words."}, {"start": 43.56, "end": 47.36, "text": "But just to make the examples in this video easier for you and me to think about, let's"}, {"start": 47.36, "end": 51.44, "text": "simplify by pretending that tokens are always just words."}, {"start": 51.44, "end": 56.519999999999996, "text": "The first step in a transformer is to associate each token with a high dimensional vector,"}, {"start": 56.519999999999996, "end": 58.199999999999996, "text": "what we call its embedding."}, {"start": 58.199999999999996, "end": 62.64, "text": "Now the most important idea I want you to have in mind is how directions in this high-dimensional"}, {"start": 62.64, "end": 67.64, "text": "space of all possible embeddings can correspond with semantic meaning."}, {"start": 67.64, "end": 71.68, "text": "In the last chapter, we saw an example for how direction can correspond to gender."}, {"start": 71.68, "end": 75.8, "text": "In the sense that adding a certain step in this space can take you from the embedding"}, {"start": 75.8, "end": 80.2, "text": "of a masculine noun to the embedding of the corresponding feminine noun."}, {"start": 80.2, "end": 83.88000000000001, "text": "Just one example you could imagine how many other directions in this high-dimensional"}, {"start": 83.88000000000001, "end": 88.8, "text": "space could correspond to numerous other aspects of a word's meaning."}, {"start": 88.8, "end": 93.28, "text": "The aim of a transformer is to progressively adjust these embeddings so that they don't"}, {"start": 93.28, "end": 98.76, "text": "merely encode an individual word, but instead they bake in some much, much richer contextual"}, {"start": 98.76, "end": 99.76, "text": "meaning."}, {"start": 99.76, "end": 103.96000000000001, "text": "I should say at front that a lot of people find the attention mechanism, this key piece"}, {"start": 103.96000000000001, "end": 108.24000000000001, "text": "in a transformer, very confusing, so don't worry if it takes some time for things to"}, {"start": 108.24000000000001, "end": 109.48, "text": "sink in."}, {"start": 109.48, "end": 114.08, "text": "I think that before we dive into the computational details and all the matrix multiplications,"}, {"start": 114.08, "end": 118.24000000000001, "text": "it's worth thinking about a couple examples for the kind of behavior that we want attention"}, {"start": 118.24000000000001, "end": 120.28, "text": "to enable."}, {"start": 120.28, "end": 125.48, "text": "Consider the phrase's American Shrew Moll, one mole of carbon dioxide, and take a biopsy"}, {"start": 125.48, "end": 126.72, "text": "of the mole."}, {"start": 126.72, "end": 130.28, "text": "You and I know that the word mole has different meanings in each one of these, based on the"}, {"start": 130.28, "end": 131.48000000000002, "text": "context."}, {"start": 131.48000000000002, "end": 135.08, "text": "But after the first step of a transformer, the one that breaks up the text and associates"}, {"start": 135.08, "end": 139.96, "text": "each token with a vector, the vector that's associated with mole would be the same in all"}, {"start": 139.96, "end": 144.56, "text": "three of these cases, because this initial token embedding is effectively a lookup table"}, {"start": 144.56, "end": 147.0, "text": "with no reference to the context."}, {"start": 147.0, "end": 150.88000000000002, "text": "It's only in the next step of the transformer that the surrounding embeddings have the"}, {"start": 150.88000000000002, "end": 153.88000000000002, "text": "chance to pass information into this one."}, {"start": 153.88000000000002, "end": 157.24, "text": "The picture you might have in mind is that there are multiple distinct directions in"}, {"start": 157.24, "end": 161.88000000000002, "text": "this embedding space, encoding the multiple distinct meanings of the word mole, and that"}, {"start": 161.88, "end": 166.96, "text": "a well-trained attention block calculates what you need to add to the generic embedding"}, {"start": 166.96, "end": 173.28, "text": "to move it to one of these more specific directions as a function of the context."}, {"start": 173.28, "end": 177.04, "text": "To take another example, consider the embedding of the word tower."}, {"start": 177.04, "end": 181.72, "text": "This is presumably some very generic, non-specific direction in the space associated with lots"}, {"start": 181.72, "end": 184.04, "text": "of other large, tall nouns."}, {"start": 184.04, "end": 188.68, "text": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism"}, {"start": 188.68, "end": 193.4, "text": "to update this vector so that it points in a direction that more specifically encodes"}, {"start": 193.4, "end": 198.28, "text": "the Eiffel Tower, maybe correlated with vectors associated with Paris and France and things"}, {"start": 198.28, "end": 199.92000000000002, "text": "made of steel."}, {"start": 199.92000000000002, "end": 204.72, "text": "If it was also preceded by the word miniature, then the vector should be updated even further"}, {"start": 204.72, "end": 209.64000000000001, "text": "so that it no longer correlates with large tall things."}, {"start": 209.64000000000001, "end": 213.48000000000002, "text": "More generally than just refining the meaning of a word, the attention block allows the"}, {"start": 213.48000000000002, "end": 218.60000000000002, "text": "model to move information encoded in one embedding to that of another, potentially one's"}, {"start": 218.6, "end": 222.23999999999998, "text": "that are quite far away, and potentially with information that's much richer than"}, {"start": 222.23999999999998, "end": 224.04, "text": "just a single word."}, {"start": 224.04, "end": 228.0, "text": "What we saw in the last chapter was how after all of the vectors flow through the network,"}, {"start": 228.0, "end": 232.2, "text": "including many different attention blocks, the computation that you perform to produce"}, {"start": 232.2, "end": 239.07999999999998, "text": "a prediction of the next token is entirely a function of the last vector in the sequence."}, {"start": 239.07999999999998, "end": 243.88, "text": "So imagine for example that the text you input is most of an entire mystery novel, all the"}, {"start": 243.88, "end": 248.96, "text": "way up to a point near the end, which reads, therefore the murderer was, if the model's"}, {"start": 248.96, "end": 253.64, "text": "going to accurately predict the next word, that final vector in the sequence, which began"}, {"start": 253.64, "end": 258.08, "text": "its life simply embedding the word was, will have to have been updated by all of the"}, {"start": 258.08, "end": 263.32, "text": "attention blocks to represent much much more than any individual word, somehow encoding"}, {"start": 263.32, "end": 267.44, "text": "all of the information from the full context window that's relevant to predicting the"}, {"start": 267.44, "end": 269.52, "text": "next word."}, {"start": 269.52, "end": 273.24, "text": "To step through the computations though, let's take a much simpler example, imagine"}, {"start": 273.24, "end": 278.6, "text": "that the input includes the phrase, a fluffy blue creature roamed the verdant forest, and"}, {"start": 278.6, "end": 282.84000000000003, "text": "for the moment, suppose that the only type of update that we care about is having the"}, {"start": 282.84000000000003, "end": 286.96000000000004, "text": "adjectives adjust the meanings of their corresponding nouns."}, {"start": 286.96000000000004, "end": 291.56, "text": "What I'm about to describe is what we would call a single head of attention, and later"}, {"start": 291.56, "end": 295.84000000000003, "text": "we will see how the attention block consists of many different heads run in parallel."}, {"start": 295.84000000000003, "end": 300.6, "text": "Again, the initial embedding for each word is some high-dimensional vector that only encodes"}, {"start": 300.6, "end": 304.0, "text": "the meaning of that particular word with no context."}, {"start": 304.0, "end": 305.48, "text": "Actually, that's not quite true."}, {"start": 305.48, "end": 307.76000000000005, "text": "They also encode the position of the word."}, {"start": 307.76000000000005, "end": 312.36, "text": "There's a lot more to say about the specific way the positions are encoded, but right now,"}, {"start": 312.36, "end": 316.04, "text": "all you need to know is that the entries of this vector are enough to tell you both what"}, {"start": 316.04, "end": 319.48, "text": "the word is and where it exists in the context."}, {"start": 319.48, "end": 322.36, "text": "Let's go ahead and denote these embeddings with the letter E."}, {"start": 322.36, "end": 327.28000000000003, "text": "The goal is to have a series of computations produce a new refined set of embeddings, where"}, {"start": 327.28, "end": 332.55999999999995, "text": "for example, those corresponding to the nouns have ingested the meaning from their corresponding"}, {"start": 332.55999999999995, "end": 333.88, "text": "adjectives."}, {"start": 333.88, "end": 337.71999999999997, "text": "And playing the deep learning game, we want most of the computations involved to look"}, {"start": 337.71999999999997, "end": 342.44, "text": "like matrix vector products, where the matrices are full of tunable weights, things that the"}, {"start": 342.44, "end": 344.71999999999997, "text": "model will learn based on data."}, {"start": 344.71999999999997, "end": 348.91999999999996, "text": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate"}, {"start": 348.91999999999996, "end": 352.96, "text": "the type of behavior that you could imagine an intention head doing."}, {"start": 352.96, "end": 356.84, "text": "As with so much deep learning, the true behavior is much harder to parse, because it's based"}, {"start": 356.84, "end": 361.96, "text": "on tweaking and tuning a huge number of parameters to minimize some cost function."}, {"start": 361.96, "end": 365.47999999999996, "text": "It's just that as we step through all of the different matrices filled with parameters"}, {"start": 365.47999999999996, "end": 370.15999999999997, "text": "that are involved in this process, I think it's really helpful to have an imagined example"}, {"start": 370.15999999999997, "end": 374.2, "text": "of something that it could be doing to help keep it all more concrete."}, {"start": 374.2, "end": 378.35999999999996, "text": "For the first step of this process, you might imagine each noun, like a creature, asking"}, {"start": 378.35999999999996, "end": 382.28, "text": "the question, hey, are there any adjectives sitting in front of me?"}, {"start": 382.28, "end": 386.79999999999995, "text": "And for the words fluffy and blue to each be able to answer, yeah, I'm an adjective"}, {"start": 386.8, "end": 389.44, "text": "and I'm in that position."}, {"start": 389.44, "end": 394.32, "text": "That question is somehow encoded as yet another vector, another list of numbers, which we"}, {"start": 394.32, "end": 396.96000000000004, "text": "call the query for this word."}, {"start": 396.96000000000004, "end": 403.24, "text": "This query vector though has a much smaller dimension than the embedding vector, say 128."}, {"start": 403.24, "end": 408.04, "text": "Computing this query looks like taking a certain matrix, which I'll label WQ, and multiplying"}, {"start": 408.04, "end": 411.36, "text": "it by the embedding."}, {"start": 411.36, "end": 415.36, "text": "Compressing things a bit, let's write that query vector as Q, and then anytime you see"}, {"start": 415.36, "end": 420.32, "text": "me put a matrix next to an arrow like this one, it's meant to represent that multiplying"}, {"start": 420.32, "end": 426.24, "text": "this matrix by the vector at the arrows start gives you the vector at the arrows end."}, {"start": 426.24, "end": 430.48, "text": "In this case, you multiply this matrix by all of the embeddings in the context, producing"}, {"start": 430.48, "end": 433.68, "text": "one query vector for each token."}, {"start": 433.68, "end": 437.72, "text": "The entries of this matrix are parameters of the model, which means the true behavior"}, {"start": 437.72, "end": 441.72, "text": "is learned from data, and in practice what this matrix does in a particular attention"}, {"start": 441.72, "end": 443.96000000000004, "text": "head is challenging to parse."}, {"start": 443.96000000000004, "end": 447.6, "text": "But for R-sake, imagining an example that we might hope that it would learn, we'll"}, {"start": 447.6, "end": 452.64000000000004, "text": "suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller"}, {"start": 452.64000000000004, "end": 458.96000000000004, "text": "query space that somehow encodes the notion of looking for adjectives in preceding positions."}, {"start": 458.96000000000004, "end": 461.84000000000003, "text": "As to what it does to other embeddings, who knows?"}, {"start": 461.84000000000003, "end": 465.16, "text": "Maybe it simultaneously tries to accomplish some other goal with those, right now where"}, {"start": 465.16, "end": 467.32000000000005, "text": "laser focused on the nouns."}, {"start": 467.32, "end": 472.2, "text": "At the same time associated with this is a second matrix called the key matrix, which"}, {"start": 472.2, "end": 475.32, "text": "you also multiply by every one of the embeddings."}, {"start": 475.32, "end": 479.32, "text": "This produces a second sequence of vectors that we call the keys."}, {"start": 479.32, "end": 483.96, "text": "Conceptually you want to think of the keys as potentially answering the queries."}, {"start": 483.96, "end": 488.32, "text": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps"}, {"start": 488.32, "end": 492.24, "text": "the embedding vectors to that same smaller dimensional space."}, {"start": 492.24, "end": 497.56, "text": "You think of the keys as matching the queries whenever they closely align with each other."}, {"start": 497.56, "end": 501.52, "text": "In our example, you would imagine that the key matrix maps the adjectives, like fluffy"}, {"start": 501.52, "end": 507.64, "text": "and blue, to vectors that are closely aligned with the query produced by the word creature."}, {"start": 507.64, "end": 512.44, "text": "To measure how well each key matches each query, you compute a dot product between each"}, {"start": 512.44, "end": 514.84, "text": "possible key query pair."}, {"start": 514.84, "end": 519.2, "text": "I like to visualize a grid full of a bunch of dots where the bigger dots correspond to"}, {"start": 519.2, "end": 523.44, "text": "the larger dot products, the places where the keys and queries align."}, {"start": 523.44, "end": 528.12, "text": "For our adjective noun example, that would look a little more like this, where if the"}, {"start": 528.12, "end": 532.8000000000001, "text": "keys produced by fluffy and blue really do align closely with the query produced by"}, {"start": 532.8000000000001, "end": 539.2, "text": "creature, then the dot products in these two spots would be some large positive numbers."}, {"start": 539.2, "end": 542.5600000000001, "text": "In the lingo machine learning people would say that this means the embeddings of fluffy"}, {"start": 542.5600000000001, "end": 546.0, "text": "and blue attend to the embedding of creature."}, {"start": 546.0, "end": 550.52, "text": "By contrast to the dot product between the key for some other word, like the and the"}, {"start": 550.52, "end": 555.88, "text": "query for creature, would be some small or negative value that reflects that these are unrelated"}, {"start": 555.88, "end": 557.84, "text": "to each other."}, {"start": 557.84, "end": 563.44, "text": "So we have this grid of values that can be any real number from negative infinity to infinity,"}, {"start": 563.44, "end": 568.04, "text": "giving us a score for how relevant each word is to updating the meaning of every other"}, {"start": 568.04, "end": 569.2, "text": "word."}, {"start": 569.2, "end": 573.44, "text": "The way we're about to use these scores is to take a certain weighted sum along each"}, {"start": 573.44, "end": 576.48, "text": "column weighted by the relevance."}, {"start": 576.48, "end": 581.08, "text": "So instead of having values range from negative infinity to infinity, what we want is for the"}, {"start": 581.08, "end": 586.32, "text": "numbers in these columns to be between zero and one, and for each column to add up to one"}, {"start": 586.32, "end": 589.2800000000001, "text": "as if they were a probability distribution."}, {"start": 589.2800000000001, "end": 592.6, "text": "If you're coming in from the last chapter, you know what we need to do then."}, {"start": 592.6, "end": 600.36, "text": "We compute a softmax along each one of these columns to normalize the values."}, {"start": 600.36, "end": 604.32, "text": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid"}, {"start": 604.32, "end": 606.96, "text": "with these normalized values."}, {"start": 606.96, "end": 610.24, "text": "At this point, you're safe to think about each column as giving weights according to"}, {"start": 610.24, "end": 615.04, "text": "how relevant the word on the left is to the corresponding value at the top."}, {"start": 615.04, "end": 618.0, "text": "We call this grid an attention pattern."}, {"start": 618.0, "end": 621.6800000000001, "text": "Now if you look at the original transformer paper, there's a really compact way that"}, {"start": 621.6800000000001, "end": 623.88, "text": "they write this all down."}, {"start": 623.88, "end": 630.84, "text": "Here the variables q and k represent the full arrays of query and key vectors respectively,"}, {"start": 630.84, "end": 635.28, "text": "those little vectors you get by multiplying the embeddings by the query and the key matrices."}, {"start": 635.28, "end": 639.72, "text": "This expression up in the numerator is a really compact way to represent the grid of all"}, {"start": 639.72, "end": 643.84, "text": "possible dot products between pairs of keys and queries."}, {"start": 643.84, "end": 648.28, "text": "A small technical detail that I didn't mention is that for numerical stability, it happens"}, {"start": 648.28, "end": 652.88, "text": "to be helpful to divide all of these values by the square root of the dimension in that"}, {"start": 652.88, "end": 654.88, "text": "key query space."}, {"start": 654.88, "end": 659.04, "text": "Then this softmax that's wrapped around the full expression is meant to be understood"}, {"start": 659.04, "end": 661.8, "text": "to apply column by column."}, {"start": 661.8, "end": 665.2, "text": "As to that V term, we'll talk about it in just a second."}, {"start": 665.2, "end": 669.04, "text": "Before that, there's one other technical detail that so far I've skipped."}, {"start": 669.04, "end": 673.12, "text": "During the training process, when you run this model on a given text example, and all"}, {"start": 673.12, "end": 677.48, "text": "of the weights are slightly adjusted and tuned to either reward or punish it based on how"}, {"start": 677.48, "end": 681.64, "text": "high a probability it assigns to the true next word in the passage, it turns out to make"}, {"start": 681.64, "end": 683.96, "text": "the whole training process a lot more efficient."}, {"start": 683.96, "end": 689.08, "text": "If you simultaneously have it predict every possible next token following each initial"}, {"start": 689.08, "end": 691.96, "text": "sub sequence of tokens in this passage."}, {"start": 691.96, "end": 695.68, "text": "For example, with the phrase that we've been focusing on, it might also be predicting"}, {"start": 695.68, "end": 700.28, "text": "what words follow creature and what words follow the."}, {"start": 700.28, "end": 703.92, "text": "This is really nice because it means what would otherwise be a single training example"}, {"start": 703.92, "end": 706.12, "text": "effectively acts as many."}, {"start": 706.12, "end": 710.16, "text": "For the purposes of our attention pattern, it means that you never want to allow later"}, {"start": 710.16, "end": 714.88, "text": "words to influence earlier words, since otherwise they could kind of give away the answer"}, {"start": 714.88, "end": 716.52, "text": "for what comes next."}, {"start": 716.52, "end": 721.28, "text": "What this means is that we want all of these spots here, the ones representing later tokens,"}, {"start": 721.28, "end": 725.88, "text": "influencing earlier ones, to somehow be forced to be zero."}, {"start": 725.88, "end": 729.24, "text": "The simplest thing you might think to do is to set them equal to zero, but if you did"}, {"start": 729.24, "end": 733.04, "text": "that, the columns wouldn't add up to one anymore, they wouldn't be normalized."}, {"start": 733.04, "end": 737.3199999999999, "text": "So instead, a common way to do this is that before applying softbacks, you set all of those"}, {"start": 737.3199999999999, "end": 739.68, "text": "entries to be negative infinity."}, {"start": 739.68, "end": 743.9599999999999, "text": "If you do that, then after applying softbacks, all of those get turned into zero, but the"}, {"start": 743.9599999999999, "end": 746.0799999999999, "text": "columns stay normalized."}, {"start": 746.0799999999999, "end": 747.8399999999999, "text": "This process is called masking."}, {"start": 747.8399999999999, "end": 751.92, "text": "There are versions of attention where you don't apply it, but in our GPT example, even"}, {"start": 751.92, "end": 755.52, "text": "though this is more relevant during the training phase than it would be, say, running it as"}, {"start": 755.52, "end": 759.76, "text": "a chatbot or something like that, you do always apply this masking to prevent later tokens"}, {"start": 759.76, "end": 762.56, "text": "from influencing earlier ones."}, {"start": 762.56, "end": 767.0, "text": "Another fact that's worth reflecting on about this attention pattern is how its size is"}, {"start": 767.0, "end": 769.84, "text": "equal to the square of the context size."}, {"start": 769.84, "end": 774.04, "text": "So this is why context size can be a really huge bottleneck for large language models,"}, {"start": 774.04, "end": 776.4, "text": "and scaling it up is non-trivial."}, {"start": 776.4, "end": 780.72, "text": "As you might imagine, motivated by a desire for bigger and bigger context windows, recent"}, {"start": 780.72, "end": 785.88, "text": "years have seen some variations to the attention mechanism aimed at making context more scalable,"}, {"start": 785.88, "end": 789.28, "text": "but right here, you and I are staying focused on the basics."}, {"start": 789.28, "end": 791.64, "text": "Okay, great."}, {"start": 791.64, "end": 795.24, "text": "Computing this pattern lets the model deduce which words are relevant to which other"}, {"start": 795.24, "end": 796.24, "text": "words."}, {"start": 796.24, "end": 800.96, "text": "You need to actually update the embeddings, allowing words to pass information to whichever"}, {"start": 800.96, "end": 803.08, "text": "other words they're relevant to."}, {"start": 803.08, "end": 808.16, "text": "For example, you want the embedding of fluffy to somehow cause a change to creature that"}, {"start": 808.16, "end": 813.08, "text": "moves it to a different part of this 12,000 dimensional embedding space that more specifically"}, {"start": 813.08, "end": 815.44, "text": "encodes a fluffy creature."}, {"start": 815.44, "end": 818.72, "text": "What I'm going to do here is first show you the most straightforward way that you could"}, {"start": 818.72, "end": 822.88, "text": "do this, though there's a slight way that this gets modified in the context of multi-headed"}, {"start": 822.88, "end": 824.16, "text": "attention."}, {"start": 824.16, "end": 828.0, "text": "This most straightforward way would be to use a third matrix, what we call the value"}, {"start": 828.0, "end": 833.4, "text": "matrix, which you multiply by the embedding of that first word, for example, fluffy."}, {"start": 833.4, "end": 837.24, "text": "The result of this is what you would call a value vector, and this is something that"}, {"start": 837.24, "end": 841.36, "text": "you add to the embedding of the second word, in this case something you add to the embedding"}, {"start": 841.36, "end": 842.6, "text": "of creature."}, {"start": 842.6, "end": 847.48, "text": "So this value vector lives in the same very high dimensional space as the embeddings."}, {"start": 847.48, "end": 852.24, "text": "When you multiply this value matrix by the embedding of a word, you might think of it as saying,"}, {"start": 852.24, "end": 856.88, "text": "if this word is relevant to adjusting the meaning of something else, what exactly should"}, {"start": 856.88, "end": 862.16, "text": "be added to the embedding of that something else in order to reflect this?"}, {"start": 862.16, "end": 866.96, "text": "Looking back in our diagram, let's set aside all of the keys and the queries since after"}, {"start": 866.96, "end": 870.32, "text": "you compute the attention pattern you're done with those, then you're going to take this"}, {"start": 870.32, "end": 875.08, "text": "value matrix and multiply it by every one of those embeddings to produce a sequence of"}, {"start": 875.08, "end": 877.08, "text": "value vectors."}, {"start": 877.08, "end": 880.48, "text": "You might think of these value vectors as being kind of associated with the corresponding"}, {"start": 880.48, "end": 882.4, "text": "keys."}, {"start": 882.4, "end": 887.8000000000001, "text": "For each column in this diagram, you multiply each of the value vectors by the corresponding"}, {"start": 887.8000000000001, "end": 890.04, "text": "weight in that column."}, {"start": 890.04, "end": 894.44, "text": "For example, here, under the embedding of creature, you would be adding large proportions of the"}, {"start": 894.44, "end": 900.08, "text": "value vectors for fluffy and blue, while all of the other value vectors get zeroed out,"}, {"start": 900.08, "end": 902.12, "text": "or at least nearly zeroed out."}, {"start": 902.12, "end": 906.52, "text": "And then finally, the way to actually update the embedding associated with this column,"}, {"start": 906.52, "end": 910.6, "text": "obviously encoding some context-free meaning of creature, you add together all of these"}, {"start": 910.6, "end": 915.72, "text": "rescaled values in the column, producing a change that you want to add that I'll label"}, {"start": 915.72, "end": 919.64, "text": "Delta E, and then you add that to the original embedding."}, {"start": 919.64, "end": 925.0, "text": "Hopefully, what results is a more refined vector encoding the more contextually rich meaning,"}, {"start": 925.0, "end": 927.4399999999999, "text": "like that of a fluffy blue creature."}, {"start": 927.4399999999999, "end": 931.52, "text": "And of course, you don't just do this to one embedding, you apply the same weighted sum"}, {"start": 931.52, "end": 936.48, "text": "across all of the columns in this picture, producing a sequence of changes, adding"}, {"start": 936.48, "end": 941.0, "text": "all of those changes to the corresponding embeddings produces a full sequence of more"}, {"start": 941.0, "end": 944.6800000000001, "text": "refined embeddings popping out of the attention block."}, {"start": 944.6800000000001, "end": 949.6800000000001, "text": "Zooming out, this whole process is what you would describe as a single head of attention."}, {"start": 949.6800000000001, "end": 954.84, "text": "As I've described things so far, this process is parameterized by three distinct matrices,"}, {"start": 954.84, "end": 959.48, "text": "all filled with tunable parameters, the key, the query, and the value."}, {"start": 959.48, "end": 962.76, "text": "I want to take a moment to continue what we started in the last chapter with the score"}, {"start": 962.76, "end": 967.0, "text": "keeping where we count up the total number of model parameters using the numbers from"}, {"start": 967.0, "end": 969.28, "text": "GPT-3."}, {"start": 969.28, "end": 975.36, "text": "These key and query matrices each have 12,288 columns, matching the embedding dimension,"}, {"start": 975.36, "end": 980.2, "text": "and 128 rows, matching the dimension of that smaller key query space."}, {"start": 980.2, "end": 984.8, "text": "This gives us an additional 1.5 million or so parameters for each one."}, {"start": 984.8, "end": 989.24, "text": "If you look at that value matrix by contrast, the way I've described things so far would"}, {"start": 989.24, "end": 996.64, "text": "suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both"}, {"start": 996.64, "end": 1001.48, "text": "its inputs and its outputs live in this very large embedding space."}, {"start": 1001.48, "end": 1005.64, "text": "If true, that would mean about 150 million added parameters."}, {"start": 1005.64, "end": 1007.4, "text": "And to be clear, you could do that."}, {"start": 1007.4, "end": 1012.12, "text": "You could devote orders of magnitude more parameters to the value map than to the key and query."}, {"start": 1012.12, "end": 1016.0, "text": "But in practice, it is much more efficient if instead you make it so that the number of"}, {"start": 1016.0, "end": 1020.28, "text": "parameters devoted to this value map is the same as the number devoted to the key and"}, {"start": 1020.28, "end": 1021.36, "text": "the query."}, {"start": 1021.36, "end": 1026.12, "text": "This is especially relevant in the setting of running multiple attention heads in parallel."}, {"start": 1026.12, "end": 1030.96, "text": "The way this looks is that the value map is factored as a product of two smaller matrices."}, {"start": 1030.96, "end": 1035.08, "text": "Conceptually, I would still encourage you to think about the overall linear map, one with"}, {"start": 1035.08, "end": 1039.88, "text": "inputs and outputs both in this larger embedding space, for example, taking the embedding of"}, {"start": 1039.88, "end": 1043.92, "text": "blue to this blueness direction that you would add to nouns."}, {"start": 1043.92, "end": 1047.1200000000001, "text": "It's just that it's broken up into two separate steps."}, {"start": 1047.1200000000001, "end": 1051.68, "text": "The first matrix on the right here has a smaller number of rows, typically the same size as"}, {"start": 1051.68, "end": 1053.0800000000002, "text": "the key query space."}, {"start": 1053.0800000000002, "end": 1057.1200000000001, "text": "What this means, as you can think of it as mapping the large embedding vectors down to"}, {"start": 1057.1200000000001, "end": 1059.24, "text": "a much smaller space."}, {"start": 1059.24, "end": 1063.4, "text": "This is not the conventional naming, but I'm going to call this the value down matrix."}, {"start": 1063.4, "end": 1068.2, "text": "The second matrix, maps from this smaller space back up to the embedding space, producing"}, {"start": 1068.2, "end": 1070.8400000000001, "text": "the vectors that you use to make the actual updates."}, {"start": 1070.84, "end": 1075.1599999999999, "text": "I'm going to call this one the value up matrix, which again is not conventional."}, {"start": 1075.1599999999999, "end": 1078.12, "text": "The way that you would see this written in most papers looks a little different."}, {"start": 1078.12, "end": 1081.9199999999998, "text": "I'll talk about it in a minute, in my opinion, it tends to make things a little more conceptually"}, {"start": 1081.9199999999998, "end": 1083.1999999999998, "text": "confusing."}, {"start": 1083.1999999999998, "end": 1087.12, "text": "To throw in linear algebra jargon here, what we're basically doing is constraining the"}, {"start": 1087.12, "end": 1091.48, "text": "overall value map to be a low-rank transformation."}, {"start": 1091.48, "end": 1095.8, "text": "Turning back to the parameter count, all four of these matrices have the same size, and"}, {"start": 1095.8, "end": 1102.08, "text": "adding them all up, we get about 6.3 million parameters for one attention head."}, {"start": 1102.08, "end": 1105.3999999999999, "text": "As a quick side note, to be a little more accurate, everything describes so far is what"}, {"start": 1105.3999999999999, "end": 1109.36, "text": "people would call a self attention head, to distinguish it from a variation that comes"}, {"start": 1109.36, "end": 1112.3999999999999, "text": "up in other models that's called cross attention."}, {"start": 1112.3999999999999, "end": 1116.72, "text": "This isn't relevant to our GPT example, but if you're curious, cross attention involves"}, {"start": 1116.72, "end": 1121.8799999999999, "text": "models that process two distinct types of data, like text in one language and text in"}, {"start": 1121.88, "end": 1126.3600000000001, "text": "another language that's part of an ongoing generation of a translation, or maybe audio"}, {"start": 1126.3600000000001, "end": 1130.5600000000002, "text": "input of speech and an ongoing transcription."}, {"start": 1130.5600000000002, "end": 1134.92, "text": "Across attention head looks almost identical, the only difference is that the key and query"}, {"start": 1134.92, "end": 1137.8000000000002, "text": "maps act on different data sets."}, {"start": 1137.8000000000002, "end": 1142.2, "text": "In a model-loving translation, for example, the keys might come from one language, while"}, {"start": 1142.2, "end": 1146.7600000000002, "text": "the queries come from another, and the attention pattern could describe which words from one"}, {"start": 1146.7600000000002, "end": 1150.3600000000001, "text": "language correspond to which words in another."}, {"start": 1150.36, "end": 1153.56, "text": "In this setting, there would typically be no masking, since there's not really any"}, {"start": 1153.56, "end": 1157.28, "text": "notion of later tokens affecting earlier ones."}, {"start": 1157.28, "end": 1161.12, "text": "Staying focused on self attention, though, if you understood everything so far, and if"}, {"start": 1161.12, "end": 1164.52, "text": "you were to stop here, you would come away with the essence of what attention really"}, {"start": 1164.52, "end": 1165.84, "text": "is."}, {"start": 1165.84, "end": 1170.36, "text": "All that's really left to us is to lay out the sense in which you do this many, many"}, {"start": 1170.36, "end": 1172.1599999999999, "text": "different times."}, {"start": 1172.1599999999999, "end": 1176.32, "text": "In our central example, we focused on adjectives updating nouns, but of course, there are"}, {"start": 1176.32, "end": 1180.3999999999999, "text": "lots of different ways that context can influence the meaning of a word."}, {"start": 1180.3999999999999, "end": 1185.1599999999999, "text": "If the words they crashed the, preceded the word car, it has implications for the shape"}, {"start": 1185.1599999999999, "end": 1187.1599999999999, "text": "and the structure of that car."}, {"start": 1187.1599999999999, "end": 1189.8, "text": "And a lot of associations might be less grammatical."}, {"start": 1189.8, "end": 1194.52, "text": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might"}, {"start": 1194.52, "end": 1196.48, "text": "be referring to Harry Potter."}, {"start": 1196.48, "end": 1200.9199999999998, "text": "Whereas if instead the words queen, Sussex, and William were in that passage, then perhaps"}, {"start": 1200.9199999999998, "end": 1205.08, "text": "the embedding of Harry should instead be updated to refer to the prince."}, {"start": 1205.08, "end": 1209.6399999999999, "text": "For every different type of contextual updating that you might imagine, the parameters of these"}, {"start": 1209.6399999999999, "end": 1214.0, "text": "key inquiry matrices would be different to capture the different attention patterns, and"}, {"start": 1214.0, "end": 1219.96, "text": "the parameters of our value map would be different based on what should be added to the embeddings."}, {"start": 1219.96, "end": 1224.6, "text": "And again, in practice, the true behavior of these maps is much more difficult to interpret,"}, {"start": 1224.6, "end": 1228.12, "text": "where the weights are set to do whatever the model needs them to do to best accomplish"}, {"start": 1228.12, "end": 1231.4399999999998, "text": "its goal of predicting the next token."}, {"start": 1231.44, "end": 1235.98, "text": "As I said before, everything we described is a single head of attention, and a full attention"}, {"start": 1235.98, "end": 1240.72, "text": "block inside a transformer consists of what's called multi-headed attention, where you run"}, {"start": 1240.72, "end": 1246.56, "text": "a lot of these operations in parallel, each with its own distinct key query and value maps."}, {"start": 1246.56, "end": 1252.3200000000002, "text": "GPT-3, for example, uses 96 attention heads inside each block."}, {"start": 1252.3200000000002, "end": 1255.8400000000001, "text": "Considering that each one is already a bit confusing, it's certainly a lot to hold in"}, {"start": 1255.8400000000001, "end": 1256.92, "text": "your head."}, {"start": 1256.92, "end": 1261.3600000000001, "text": "Just to spell it all out very explicitly, this means you have 96 distinct key inquiry"}, {"start": 1261.36, "end": 1267.08, "text": "matrices, producing 96 distinct attention patterns, then each head has its own distinct"}, {"start": 1267.08, "end": 1272.76, "text": "value matrices used to produce 96 sequences of value vectors."}, {"start": 1272.76, "end": 1277.4399999999998, "text": "These are all added together using the corresponding attention patterns as weights."}, {"start": 1277.4399999999998, "end": 1282.36, "text": "What this means is that for each position in the context, each token, every one of these"}, {"start": 1282.36, "end": 1287.56, "text": "heads produces a proposed change to be added to the embedding in that position."}, {"start": 1287.56, "end": 1292.52, "text": "So what you do is you sum together all of those proposed changes, one for each head, and"}, {"start": 1292.52, "end": 1296.6799999999998, "text": "you add the result to the original embedding of that position."}, {"start": 1296.6799999999998, "end": 1302.6399999999999, "text": "This entire sum here would be one slice of what's outputted from this multi-headed attention"}, {"start": 1302.6399999999999, "end": 1307.72, "text": "block, a single one of those refined embeddings that pops out the other end of it."}, {"start": 1307.72, "end": 1311.8799999999999, "text": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink"}, {"start": 1311.8799999999999, "end": 1312.8799999999999, "text": "in."}, {"start": 1312.8799999999999, "end": 1316.76, "text": "The overall idea is that by running many distinct heads in parallel, you're giving the"}, {"start": 1316.76, "end": 1323.8, "text": "model the capacity to learn many distinct ways that context changes meaning."}, {"start": 1323.8, "end": 1328.0, "text": "Pulling up our running tally for parameter count with 96 heads, each including its own"}, {"start": 1328.0, "end": 1333.36, "text": "variation of these four matrices, each block of multi-headed attention, ends up with around"}, {"start": 1333.36, "end": 1336.48, "text": "600 million parameters."}, {"start": 1336.48, "end": 1339.68, "text": "There's one added, slightly annoying thing that I should really mention for any of you"}, {"start": 1339.68, "end": 1342.2, "text": "who go on to read more about transformers."}, {"start": 1342.2, "end": 1346.56, "text": "You remember how I said that the value map is factored out into these two distinct matrices,"}, {"start": 1346.56, "end": 1349.8799999999999, "text": "which I labeled as the value down and the value up matrices."}, {"start": 1349.8799999999999, "end": 1354.76, "text": "The way that I framed things would suggest that you see this pair of matrices inside each"}, {"start": 1354.76, "end": 1360.32, "text": "attention head, and you could absolutely implement it this way, that would be a valid design."}, {"start": 1360.32, "end": 1363.9199999999998, "text": "But the way that you see this written in papers and the way that it's implemented in practice"}, {"start": 1363.9199999999998, "end": 1365.3999999999999, "text": "looks a little different."}, {"start": 1365.3999999999999, "end": 1371.44, "text": "All of these value up matrices for each head appear stapled together in one giant matrix"}, {"start": 1371.44, "end": 1376.8, "text": "that we call the output matrix, associated with the entire multi-headed attention block."}, {"start": 1376.8, "end": 1380.52, "text": "And when you see people refer to the value matrix for a given attention head, they're"}, {"start": 1380.52, "end": 1384.8600000000001, "text": "typically only referring to this first step, the one that I was labeling as the value"}, {"start": 1384.8600000000001, "end": 1388.28, "text": "down, projection into the smaller space."}, {"start": 1388.28, "end": 1391.28, "text": "For the curious among you, I've left a non-screen note about it."}, {"start": 1391.28, "end": 1394.4, "text": "It's one of those details that runs the risk of distracting from the main conceptual"}, {"start": 1394.4, "end": 1397.96, "text": "points, but I do want to call it out just so that you know if you read about this in other"}, {"start": 1397.96, "end": 1399.6000000000001, "text": "sources."}, {"start": 1399.6, "end": 1403.28, "text": "Starting aside all the technical nuances, in the preview from the last chapter we saw"}, {"start": 1403.28, "end": 1408.48, "text": "how data flowing through a transformer doesn't just flow through a single attention block."}, {"start": 1408.48, "end": 1412.6399999999999, "text": "For one thing, it also goes through these other operations called multi-layer perceptrons,"}, {"start": 1412.6399999999999, "end": 1416.9199999999998, "text": "we'll talk more about those in the next chapter, and then it repeatedly goes through many,"}, {"start": 1416.9199999999998, "end": 1419.8799999999999, "text": "many copies of both of these operations."}, {"start": 1419.8799999999999, "end": 1424.9599999999998, "text": "What this means is that after a given word imbibes some of its context, there are many more"}, {"start": 1424.96, "end": 1430.8400000000001, "text": "chances for this more nuanced embedding to be influenced by its more nuanced surroundings."}, {"start": 1430.8400000000001, "end": 1435.04, "text": "The further down the network you go, with each embedding taking in more and more meaning"}, {"start": 1435.04, "end": 1439.4, "text": "from all the other embeddings, which themselves are getting more and more nuanced, the hope"}, {"start": 1439.4, "end": 1443.96, "text": "is that there's the capacity to encode higher level and more abstract ideas about a given"}, {"start": 1443.96, "end": 1449.68, "text": "input beyond just descriptors and grammatical structure, things like sentiment and tone"}, {"start": 1449.68, "end": 1453.92, "text": "and whether it's a poem, and what underlying scientific truths are relevant to the piece"}, {"start": 1453.92, "end": 1456.68, "text": "and things like that."}, {"start": 1456.68, "end": 1462.8400000000001, "text": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the"}, {"start": 1462.8400000000001, "end": 1468.4, "text": "total number of key query and value parameters is multiplied by another 96, which brings"}, {"start": 1468.4, "end": 1474.0, "text": "the total sum to just under 58 billion distinct parameters devoted to all of the attention"}, {"start": 1474.0, "end": 1475.0, "text": "heads."}, {"start": 1475.0, "end": 1479.92, "text": "That is a lot to be sure, but it's only about a third of the 175 billion that are in"}, {"start": 1479.92, "end": 1481.52, "text": "the network in total."}, {"start": 1481.52, "end": 1485.96, "text": "So even though attention gets all of the attention, the majority of parameters come from the"}, {"start": 1485.96, "end": 1488.48, "text": "blocks sitting in between these steps."}, {"start": 1488.48, "end": 1492.0, "text": "In the next chapter, you and I will talk more about those other blocks, and also a lot"}, {"start": 1492.0, "end": 1494.04, "text": "more about the training process."}, {"start": 1494.04, "end": 1498.32, "text": "A big part of the story for the success of the attention mechanism is not so much any"}, {"start": 1498.32, "end": 1504.16, "text": "specific kind of behavior that it enables, but the fact that it's extremely parallelizable,"}, {"start": 1504.16, "end": 1509.48, "text": "meaning that you can run a huge number of computations in a short time using GPUs."}, {"start": 1509.48, "end": 1512.8, "text": "Even that one of the big lessons about deep learning in the last decade or two has been"}, {"start": 1512.8, "end": 1517.52, "text": "that scale alone seems to give huge qualitative improvements in model performance."}, {"start": 1517.52, "end": 1522.14, "text": "There's a huge advantage to parallelizable architectures that let you do this."}, {"start": 1522.14, "end": 1525.8600000000001, "text": "If you want to learn more about this stuff, I've left lots of links in the description."}, {"start": 1525.8600000000001, "end": 1530.56, "text": "In particular, anything produced by Andre Carpathia or Chris Ola tend to be pure gold."}, {"start": 1530.56, "end": 1534.28, "text": "In this video, I wanted to just jump into attention in its current form, but if you're"}, {"start": 1534.28, "end": 1537.96, "text": "curious about more of the history for how we got here and how you might reinvent this idea"}, {"start": 1537.96, "end": 1541.88, "text": "for yourself, my friend Vivek just put up a couple videos giving a lot more of that"}, {"start": 1541.88, "end": 1542.88, "text": "motivation."}, {"start": 1542.88, "end": 1546.76, "text": "Also, Brit Cruise from the channel The Art of the Problem has a really nice video about"}, {"start": 1546.76, "end": 1548.4, "text": "the history of large language models."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0000.png", "text": "Previously: Transformers\nee ed DB ee,\nyuty 1\n\nIt py\nL", "timestamp": "0000"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0001.png", "text": "~ Quick Recap", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0002.png", "text": "“Embedding” :\nWords =£————+ Vectors\nAll\ndata .\nin .\ndeep :\nlearning, a st\nmust ia\nbe presented a me\nrepresented. she\nas ‘\nvectors 2", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0003.png", "text": "‘The King doth wake tonight and takes his rouse...", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0004.png", "text": "Americ:", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0006.png", "text": "‘Tower\ni\nl\nfoo |\ni]\n‘\ni}\niy\nLZ lessee ect\ni\nf 4 :", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0007.png", "text": "Rial) Tet TMS", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0008.png", "text": "Tt was a dark and stormy night. © © ars ti\nts dew oties se Piet Lose\n\nHaden a anpete tee\nCr PT Te ay\n\nCe eo wba tes st nl\nTrmpess Da ee eb eee ab\nsete cag sts PE tee a see\npoet bee Deda oboe cad sta wn He", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0009.png", "text": "titi Tt", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0010.png", "text": "alfluffy \"blue|ereature|roamed]the|verdant forest\nll) ) 1d dd", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0011.png", "text": "alfluffy blue|ereature|roamed|the|verdant forest\nge edo dou du\nF, Fe BF, Fy E,, F, F; Fy", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0012.png", "text": "Input Tunable parameters Output\nweed Abe old cieah, “wy ob\nbare Ou. rare maya of\nfeipang. 6 veideog Some of > —> have\npe hawe dacnp wuye, ard gxte of\no", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0013.png", "text": "fluffy blue|creature,\n\na rr ee ee\nE,) BE E; £, EB EE\nSo Any ad.ectinen\naw front of te\"", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0014.png", "text": "sive", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0015.png", "text": "‘affluffy|blue creature|roamat the[verdant fort)\nbib doboy dos\nE EL FE E Ey EY E; B\nbebe fF bb tf\nEmbedding space Query Key space\n: iy", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0016.png", "text": "creatste’\nM\nFE,\na,\ndulfy oF. kK.\nblue E> Ke,", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0017.png", "text": "LTIPTTITT\n\nPERE REE E\n\n444 4 €@ &@ & &\nWei cee", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0018.png", "text": "Fra]\n\nr\n\nE,\n\na\nufyl+f,—>K, « «© e -\nbluel~z,—> K] ¢ © -@ .\n\n. . e . e e", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0019.png", "text": "In a minute...\nNiobe odin, ~~,\nae i\nBiya Ley ar", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0020.png", "text": "‘\nDP pee\namet ea\nGri A. -\npean ~\noer a\net owe\nee", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0021.png", "text": "Auenuion(Q. 8: ¥) softmax ( * if) v\nee | ee ee |\nQ1 O2 Q1 Qi Qs Qe Ky hy Ky Wy Ae BY\nIt} ttt tl Prtt tet", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0022.png", "text": ": KTQ\\y\nAttention(Q, KV) =lsoftinax ( —= ] V\nVay\nts\n\nHEAL a be\n\neee ”\n\nwe -\n\nat te * .", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0023.png", "text": "the L777 |\n\nthe Austy +777]\n\nthe Multy blue —#[—77_ ]\n\nthe fluffy blue creature —» 37]\n\nthe dutfy blue crenture roaned —» 777]\n\nthe Tuffy blue creature rasned th —e [77]\n\nthe fluffy blue creature: ronmed the} verdant —» [77]\n\nthe Duffy blue creature roamed the verdant forest [777]", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0024.png", "text": "ify Me omens, nummy le! nen) re\ncar >k @- - . .\ntae tk @ + @ : es .\nber SK oF oe @e@ - or\newes “ek, Te - .\nredo HK Ls ee! @ es .\ni oe i\nfrwrdest oe PK -r @@\nfet “ek de ee ee", "timestamp": "0024"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0025.png", "text": "titwt WW", "timestamp": "0025"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0026.png", "text": "A RRP Arran Arr errr erred\nSHEE BASES EE\nETERS ESSE EEE\nSHHHISHEEE EINE BIR HEHT\nREEHETESESLSESESeTEE Tea EESEESEESSESEEEESS EEEET}\nPREIS RE REL\nSPHTHPPPEPPEEESEE SEIN PEE PEE\nIEEE TEU EE HEE Sy Ha TEE\nSpSSSEESSESSSSRRSROSEOERSCSEOSESSCES USERS SSTOSROSS\nSTH PITS LT Lg PE ET\nsSretheetretestestettceni PlStiHtesitateste et tate", "timestamp": "0026"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0027.png", "text": "fluffy creature\nL |\n” : _", "timestamp": "0027"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0029.png", "text": "i? Ter te Tr\n\nFo@® FR FE F BF FF £\n\nkaon @--6 se: - 8\nBally + @-e-.F-.- -\nbee @e- . . .\n‘erweture +, + +e :\nacd +8 e- .\nBEE @- .\nTeel Ee .", "timestamp": "0029"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0030.png", "text": "Valve mats ix ine)\nuy i\nwon ey :\nthf +e eV, eas\nbie oe Mot, cue\ncmt +E, “sv. :\nraed +E SV .\ntek Soy .\nede oH Ss v. .\nTeeeet oH, Sev 5", "timestamp": "0030"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0031.png", "text": "a fam] we a faa]\napap cerry) apes\nFlefele fe dade fe\nnote es)\nBhar ey ]e 7 hen\nyee ae wm fe\nsonia? Ley .\nsma Ha | °\nseer, Se 9, [ee “T nl aw\nwees. 4 fe . a fre\nFear Seed .\n“", "timestamp": "0031"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0033.png", "text": "ham cee Value\n12.238,\n—_.", "timestamp": "0033"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0034.png", "text": "as\nll\n(2 Query params) #0 Nes pecan)", "timestamp": "0034"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0035.png", "text": "doomput ours\n\nLinen tay 228s 2am\n12282 4 i\nrs", "timestamp": "0035"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0038.png", "text": "I do not want to pet it\n\nzr er er Sr Se Se\n\nE. By by KE bes bey ky\n\nbe ye yn Ie Jer Jeo ee\n\nQ & & & & & @\n\nJO reno wR wR wR a we\nDE, PKR at A ae,\nfreux| reek Sw a a\nPASH PR AN a Ge aR aN a\n| ene eS a a OR\ncaresser] ob. eR A AN AN A a GH", "timestamp": "0038"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0039.png", "text": "Th t Now do that about\nSEP pleat 10,000 times", "timestamp": "0039"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0040.png", "text": "pert:\n. . ow\n.. wizard .,. Hogwarts ... Hermione ... Harry f 2\nbk\n- Queen ... Sussex ... William ... Harry", "timestamp": "0040"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0041.png", "text": "We Wi, any wth\nJohn hit the brakes sharply, they screeched loudly, and he jolted forward.", "timestamp": "0041"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0042.png", "text": "We nas 7 _\nis ann re\n\nee TT ie\n\n: Pee i i re eg ottte gS\ntee! |: : . oo id", "timestamp": "0042"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0043.png", "text": "Lt J tf ot\n\nT T T T\n\n3. af. af. a\nak 4 ak! pak pak +5:", "timestamp": "0043"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0044.png", "text": "We it, aT wu,\nJohn tnt the brakes sharply, they screeched loudly, and he jolted forward.", "timestamp": "0044"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0045.png", "text": "do inpat ¢_outy:t\n12.2788 1228S\n128 .\n—“ 12.2Rk 4 i\nVawnes", "timestamp": "0045"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0046.png", "text": "Heed 1 Head 2 Head 3\nValue map Value nap Value map", "timestamp": "0046"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0047.png", "text": "SR wat Multilayer\nlite Perceptron\noe\n\nf Big\nMi\n\ntu —", "timestamp": "0047"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0048.png", "text": "‘Two roads diverged in a wood, and I\nT took the one lews traveled by,\nBt + 1\none .\n7 ON\n.\n\\", "timestamp": "0048"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0049.png", "text": "Total wergate 175 S101 oN,\n\nOrganved .nty 2743s matr.ces cS) GPT-3\n\nKey A ore caked Sa orate devete tthe\n\nQuery Ao gety teeta totals s Nepens 1b\n\nOatput tocared §e uw beads te oe DOU a\nAT9S2.05S. 496", "timestamp": "0049"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0050.png", "text": "Wo i, “Wy wt,\na\nJoh. tit the brakes sharply, they screeched loudly, and he jolted forward.", "timestamp": "0050"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0051.png", "text": "Andrey Karpaths Anthropic\nLET'S BUILD GFT. A Mathematical Framework\nFROM SCRATCH. 2) for Transtormer Circuits\nIM CODE =\nSPELLEO OUT. /\nWAN  ¢", "timestamp": "0051"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_07_frame_0052.png", "text": "nm Clicky Stuffs 7\n\nRrctend of apecor cumsagss these lorons are up aerted\n\nthree by wewers. stcloas thine below [2b es supyert\nfone tae mete womans mene\n= x rien hae\nv daar ann me\nwesw Pane pares een\npees mom rece en\nNmne me Faeroe Naess ope Reanives\nett Des — om ee Lacan Cay Wethe", "timestamp": "0052"}], "processing_info": {"audio_duration": 1548.4, "total_segments": 391, "total_ocr_extractions": 48}}
{"video_id": "talk_08", "url": "https://www.youtube.com/watch?v=9-Jl0dxWQs8  # 8. How might LLMs store facts | Chapter 7", "timestamp": "2025-07-31T17:03:51.655743", "asr_transcript": {"video_id": "talk_08", "language": "en", "text": " If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport. And I think in general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow. You and I have been studying a model that's trained to take in a piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a certain normalization step in between. After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other distinct directions in this super-high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don't merely encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge. Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction. And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball. So let's say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let's now dive into the meat of the lesson. What happens inside a multi layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What's going to happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end we'll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel. In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what we'll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we've seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists. That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as M plus J. Then taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E, and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan. And otherwise, it would be one or something smaller than one. And that's just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from data. This other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan. And otherwise, it's zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT-3, whose numbers we've been following is just under 50,000. In fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice. You could make it more. You could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we're measuring is high for Michael Plus Jordan, it would also necessarily be somewhat triggered by Michael Plus Phelps, and also Alexis Plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo, for short. Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often, models will use a slightly modified function that's called the J-LU, which has the same basic shape, it's just a bit smoother. But for our purposes, it's a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here. Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function, like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it's inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call this the down projection matrix. And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns. The reason it's nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists. What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn't just have to be basketball, the model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder what's that doing, as with all parameter field objects here, it's kind of hard to say exactly. Maybe there's some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact again, I'll call this big matrix W down. And similarly, call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in, encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that example, it was recognized handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought provoking fact about higher dimensions that I actually didn't know until I dug into transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two thirds of the total parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days, known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you can fit is only N, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in two or three dimensions, this makes no difference that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows the distribution of angles between pairs of these vectors. So because they started at random those angles could be anything from 0 to 180 degrees, but you'll notice that already even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's what the distribution of angles looks like. We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren't going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're superimposed on all these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it's all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are a number of non-machine learning related videos that I'm excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.", "segments": [{"start": 0.0, "end": 2.32, "text": "If you feed a large language model the phrase,"}, {"start": 2.32, "end": 4.96, "text": "Michael Jordan plays the sport of blank,"}, {"start": 4.96, "end": 6.96, "text": "and you have it predict what comes next,"}, {"start": 6.96, "end": 9.52, "text": "and it correctly predicts basketball."}, {"start": 9.52, "end": 14.16, "text": "This would suggest that somewhere, inside its hundreds of billions of parameters,"}, {"start": 14.16, "end": 18.64, "text": "it's baked in knowledge about a specific person and his specific sport."}, {"start": 18.64, "end": 21.76, "text": "And I think in general anyone who's played around with one of these models"}, {"start": 21.76, "end": 25.52, "text": "has the clear sense that it's memorized tons and tons of facts."}, {"start": 25.52, "end": 27.76, "text": "So a reasonable question you could ask is,"}, {"start": 27.76, "end": 30.880000000000003, "text": "how exactly does that work, and where do those facts live?"}, {"start": 35.84, "end": 38.56, "text": "Last December a few researchers from Google Deep Mind"}, {"start": 38.56, "end": 40.800000000000004, "text": "posted about work on this question,"}, {"start": 40.800000000000004, "end": 44.72, "text": "and they were using this specific example of matching athletes to their sports."}, {"start": 44.72, "end": 49.44, "text": "And although a full mechanistic understanding of how facts are stored remains unsolved,"}, {"start": 49.44, "end": 51.36, "text": "they had some interesting partial results,"}, {"start": 51.36, "end": 54.0, "text": "including the very general high-level conclusion"}, {"start": 54.08, "end": 57.84, "text": "that the facts seem to live inside a specific part of these networks,"}, {"start": 57.84, "end": 62.96, "text": "known fancifully as the multi-layer perceptrons, or MLPs for short."}, {"start": 62.96, "end": 66.72, "text": "In the last couple of chapters, you and I have been digging into the details behind"}, {"start": 66.72, "end": 70.16, "text": "transformers, the architecture underlying large language models,"}, {"start": 70.16, "end": 72.88, "text": "and also underlying a lot of other modern AI."}, {"start": 72.88, "end": 76.64, "text": "In the most recent chapter we were focusing on a piece called Attention,"}, {"start": 76.64, "end": 81.6, "text": "and the next step for you and me is to dig into the details of what happens inside these"}, {"start": 81.6, "end": 84.96, "text": "multi-layer perceptrons, which make up the other big portion of the network."}, {"start": 85.52, "end": 88.16, "text": "The computation here is actually relatively simple,"}, {"start": 88.16, "end": 90.32, "text": "especially when you compare it to attention."}, {"start": 90.32, "end": 94.96, "text": "It boils down essentially to a pair of matrix multiplications with a simple something in between."}, {"start": 95.52, "end": 100.47999999999999, "text": "However, interpreting what these computations are doing is exceedingly challenging."}, {"start": 101.44, "end": 105.28, "text": "Our main goal here is to step through the computations and make them memorable,"}, {"start": 105.28, "end": 110.16, "text": "but I'd like to do it in the context of showing a specific example of how one of these blocks could,"}, {"start": 110.16, "end": 115.44, "text": "at least in principle, store a concrete fact. Specifically, it'll be storing the fact that"}, {"start": 115.44, "end": 120.56, "text": "Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I"}, {"start": 120.56, "end": 125.6, "text": "had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've"}, {"start": 125.6, "end": 130.48, "text": "either watched the last two chapters, or otherwise you have a basic sense for what a transformer is,"}, {"start": 130.48, "end": 134.56, "text": "but refreshers never hurt, so here's the quick reminder of the overall flow."}, {"start": 135.12, "end": 139.6, "text": "You and I have been studying a model that's trained to take in a piece of text"}, {"start": 139.6, "end": 144.96, "text": "and predict what comes next. That input text is first broken into a bunch of tokens,"}, {"start": 144.96, "end": 149.12, "text": "which means little chunks that are typically words or little pieces of words,"}, {"start": 149.12, "end": 155.68, "text": "and each token is associated with a high-dimensional vector, which is to say a long list of numbers."}, {"start": 155.68, "end": 161.12, "text": "This sequence of vectors then repeatedly passes through two kinds of operation."}, {"start": 161.12, "end": 165.44, "text": "Attention, which allows the vectors to pass information between one another,"}, {"start": 165.52, "end": 169.6, "text": "and then the multi-layer perceptrons, the thing that we're going to dig into today."}, {"start": 169.6, "end": 172.32, "text": "And also there's a certain normalization step in between."}, {"start": 173.28, "end": 177.76, "text": "After the sequence of vectors has flowed through many, many different iterations of both of these"}, {"start": 177.76, "end": 184.4, "text": "blocks, by the end, the hope is that each vector has soaked up enough information, both from the"}, {"start": 184.4, "end": 189.84, "text": "context, all of the other words and the input, and also from the general knowledge that was baked"}, {"start": 189.84, "end": 195.04, "text": "into the model weights through training, that it can be used to make a prediction of what token"}, {"start": 195.04, "end": 200.23999999999998, "text": "comes next. One of the key ideas that I want you to have in your mind is that all of these"}, {"start": 200.23999999999998, "end": 205.04, "text": "vectors live in a very, very high-dimensional space, and when you think about that space,"}, {"start": 205.04, "end": 208.72, "text": "different directions can encode different kinds of meaning."}, {"start": 209.84, "end": 214.56, "text": "So a very classic example that I like to refer back to is how if you look at the embedding of"}, {"start": 214.56, "end": 220.23999999999998, "text": "woman and subtract the embedding of man, and you take that little step and you add it to another"}, {"start": 220.24, "end": 225.36, "text": "masculine noun, something like uncle, you land somewhere very, very close to the corresponding"}, {"start": 225.36, "end": 230.8, "text": "feminine noun. In this sense, this particular direction encodes gender information."}, {"start": 231.52, "end": 237.04000000000002, "text": "The idea is that many other distinct directions in this super-high-dimensional space could correspond"}, {"start": 237.04000000000002, "end": 243.20000000000002, "text": "to other features that the model might want to represent. In a transformer, these vectors don't"}, {"start": 243.20000000000002, "end": 248.64000000000001, "text": "merely encode the meaning of a single word, though. As they flow through the network, they"}, {"start": 248.64, "end": 254.64, "text": "imbibe a much richer meaning based on all the context around them, and also based on the model's"}, {"start": 254.64, "end": 260.0, "text": "knowledge. Ultimately, each one needs to encode something far, far beyond the meaning of a single"}, {"start": 260.0, "end": 265.91999999999996, "text": "word, since it needs to be sufficient to predict what will come next. We've already seen how attention"}, {"start": 265.91999999999996, "end": 271.44, "text": "blocks let you incorporate context, but a majority of the model parameters actually live inside the"}, {"start": 271.44, "end": 277.59999999999997, "text": "MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store"}, {"start": 278.56, "end": 283.36, "text": "facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it"}, {"start": 283.36, "end": 288.16, "text": "could store the fact that Michael Jordan plays basketball. Now this toy example is going to"}, {"start": 288.16, "end": 292.64000000000004, "text": "require that you and I make a couple of assumptions about that high-dimensional space. First, we'll"}, {"start": 292.64000000000004, "end": 298.16, "text": "suppose that one of the directions represents the idea of a first name Michael, and then another"}, {"start": 298.16, "end": 304.08000000000004, "text": "nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third"}, {"start": 304.08, "end": 309.35999999999996, "text": "direction will represent the idea of basketball. So specifically what I mean by this is if you look"}, {"start": 309.35999999999996, "end": 314.32, "text": "in the network and you pluck out one of the vectors being processed, if its dot product with this"}, {"start": 314.32, "end": 319.91999999999996, "text": "first name Michael direction is one, that's what it would mean for the vector to be encoding the"}, {"start": 319.91999999999996, "end": 326.24, "text": "idea of a person with that first name. Otherwise, that dot product would be zero or negative,"}, {"start": 326.24, "end": 330.96, "text": "meaning the vector doesn't really align with that direction. And for simplicity, let's completely"}, {"start": 330.96, "end": 335.2, "text": "ignore the very reasonable question of what it might mean if that dot product was bigger than one."}, {"start": 336.0, "end": 340.96, "text": "Similarly, its dot product with these other directions would tell you whether it represents"}, {"start": 340.96, "end": 347.67999999999995, "text": "the last name Jordan, or basketball. So let's say a vector is meant to represent the full name"}, {"start": 347.67999999999995, "end": 352.56, "text": "Michael Jordan, then its dot product with both of these directions would have to be one."}, {"start": 353.28, "end": 358.0, "text": "Since the text Michael Jordan spans two different tokens, this would also mean we have to"}, {"start": 358.0, "end": 363.2, "text": "assume that an earlier attention block has successfully passed information to the second of"}, {"start": 363.2, "end": 369.2, "text": "these two vectors so as to ensure that it can encode both names. With all of those as the assumptions,"}, {"start": 369.2, "end": 374.8, "text": "let's now dive into the meat of the lesson. What happens inside a multi layer perceptron?"}, {"start": 377.28, "end": 382.08, "text": "You might think of this sequence of vectors flowing into the block, and remember each vector was"}, {"start": 382.08, "end": 387.04, "text": "originally associated with one of the tokens from the input text. What's going to happen is that"}, {"start": 387.04, "end": 391.6, "text": "each individual vector from that sequence goes through a short series of operations,"}, {"start": 391.6, "end": 396.64000000000004, "text": "we'll unpack them in just a moment, and at the end we'll get another vector with the same dimension."}, {"start": 396.64000000000004, "end": 401.84000000000003, "text": "That other vector is going to get added to the original one that flowed in, and that sum is the"}, {"start": 401.84000000000003, "end": 407.84000000000003, "text": "result flowing out. This sequence of operations is something you apply to every vector in the sequence,"}, {"start": 407.84000000000003, "end": 412.56, "text": "associated with every token in the input, and it all happens in parallel. In particular,"}, {"start": 412.56, "end": 416.48, "text": "the vectors don't talk to each other in this step, they're all kind of doing their own thing."}, {"start": 416.48, "end": 420.56, "text": "And for you and me, that actually makes it a lot simpler, because it means if we understand what"}, {"start": 420.56, "end": 425.52000000000004, "text": "happens to just one of the vectors through this block, we effectively understand what happens to all"}, {"start": 425.52000000000004, "end": 430.96000000000004, "text": "of them. When I say this block is going to encode the fact that Michael Jordan plays basketball,"}, {"start": 430.96000000000004, "end": 436.48, "text": "what I mean is that if a vector flows in that encodes first name Michael and last name Jordan,"}, {"start": 436.48, "end": 441.44, "text": "then this sequence of computations will produce something that includes that direction basketball,"}, {"start": 441.44, "end": 443.92, "text": "which is what we'll add on to the vector in that position."}, {"start": 444.40000000000003, "end": 449.84000000000003, "text": "The first step of this process looks like multiplying that vector by a very big matrix,"}, {"start": 449.84000000000003, "end": 454.40000000000003, "text": "no surprises there, this is deep learning, and this matrix like all of the other ones we've"}, {"start": 454.40000000000003, "end": 459.20000000000005, "text": "seen is filled with model parameters that are learned from data, which you might think of as a"}, {"start": 459.20000000000005, "end": 463.28000000000003, "text": "bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is."}, {"start": 464.16, "end": 469.04, "text": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being"}, {"start": 469.04, "end": 474.8, "text": "its own vector and taking a bunch of dot products between those rows and the vector being processed,"}, {"start": 474.8, "end": 480.88, "text": "which I'll label as E for embedding. For example, suppose that very first row happened to equal this"}, {"start": 480.88, "end": 486.32000000000005, "text": "first name Michael direction that we're presuming exists. That would mean that the first component"}, {"start": 486.32000000000005, "end": 492.16, "text": "in this output, this dot product right here, would be one if that vector encodes the first name"}, {"start": 492.16, "end": 498.08000000000004, "text": "Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would"}, {"start": 498.08, "end": 503.76, "text": "mean if that first row was this first name Michael plus last name Jordan direction. And for"}, {"start": 503.76, "end": 509.44, "text": "simplicity, let me go ahead and write that down as M plus J. Then taking a dot product with this"}, {"start": 509.44, "end": 515.68, "text": "embedding E, things distribute really nicely, so it looks like M dot E plus J dot E, and notice how"}, {"start": 515.68, "end": 521.52, "text": "that means the ultimate value would be two if the vector encodes the full name Michael Jordan."}, {"start": 521.52, "end": 526.8, "text": "And otherwise, it would be one or something smaller than one. And that's just one row in this"}, {"start": 526.8, "end": 532.16, "text": "matrix. You might think of all of the other rows as in parallel asking some other kinds of questions,"}, {"start": 532.16, "end": 537.76, "text": "probing at some other sorts of features of the vector being processed. Very often this step also"}, {"start": 537.76, "end": 542.4799999999999, "text": "involves adding another vector through the output, which is full of model parameters learned from data."}, {"start": 542.4799999999999, "end": 547.4399999999999, "text": "This other vector is known as the bias. For our example, I want you to imagine that the value of"}, {"start": 547.4399999999999, "end": 553.76, "text": "this bias in that very first component is negative one, meaning our final output looks like that relevant"}, {"start": 553.76, "end": 559.28, "text": "dot product, but minus one. You might very reasonably ask why I would want you to assume that"}, {"start": 559.28, "end": 564.3199999999999, "text": "the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a"}, {"start": 564.3199999999999, "end": 570.08, "text": "value here, which is positive, if and only if a vector encodes the full name Michael Jordan."}, {"start": 570.08, "end": 575.4399999999999, "text": "And otherwise, it's zero or negative. The total number of rows in this matrix, which is"}, {"start": 575.4399999999999, "end": 580.48, "text": "something like the number of questions being asked in the case of GPT-3, whose numbers we've been"}, {"start": 580.48, "end": 585.76, "text": "following is just under 50,000. In fact, it's exactly four times the number of dimensions in this"}, {"start": 585.76, "end": 590.0, "text": "embedding space. That's a design choice. You could make it more. You could make it less, but having"}, {"start": 590.0, "end": 595.28, "text": "a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a"}, {"start": 595.28, "end": 600.4, "text": "higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the"}, {"start": 600.4, "end": 606.4, "text": "vector we're processing as E, and let's label this bias vector as B up and put that all back down"}, {"start": 606.4, "end": 613.68, "text": "in the diagram. At this point, a problem is that this operation is purely linear, but language is"}, {"start": 613.68, "end": 619.6, "text": "a very non-linear process. If the entry that we're measuring is high for Michael Plus Jordan,"}, {"start": 619.6, "end": 625.68, "text": "it would also necessarily be somewhat triggered by Michael Plus Phelps, and also Alexis Plus Jordan,"}, {"start": 625.68, "end": 631.36, "text": "despite those being unrelated conceptually. What you really want is a simple yes or no for the"}, {"start": 631.36, "end": 637.36, "text": "full name. So the next step is to pass this large intermediate vector through a very simple non-linear"}, {"start": 637.36, "end": 642.5600000000001, "text": "function. A common choice is one that takes all of the negative values and maps them to zero,"}, {"start": 642.5600000000001, "end": 648.5600000000001, "text": "and leaves all of the positive values unchanged. And continuing with the deep learning tradition of"}, {"start": 648.5600000000001, "end": 654.88, "text": "overly fancy names, this very simple function is often called the rectified linear unit, or"}, {"start": 654.88, "end": 660.48, "text": "Rayloo, for short. Here's what the graph looks like. So taking our imagined example where this first"}, {"start": 660.48, "end": 666.5600000000001, "text": "entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero"}, {"start": 666.5600000000001, "end": 672.4, "text": "or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value"}, {"start": 672.4, "end": 677.52, "text": "where all of the zero and negative values just get clipped to zero. So this output would be one for"}, {"start": 677.52, "end": 683.04, "text": "the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior"}, {"start": 683.04, "end": 689.44, "text": "of an AND gate. Often, models will use a slightly modified function that's called the J-LU,"}, {"start": 689.44, "end": 693.6800000000001, "text": "which has the same basic shape, it's just a bit smoother. But for our purposes, it's a little"}, {"start": 693.6800000000001, "end": 699.36, "text": "bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a"}, {"start": 699.36, "end": 704.32, "text": "transformer, they're talking about these values right here. Whenever you see that common neural"}, {"start": 704.32, "end": 708.6400000000001, "text": "network picture with a layer of dots and a bunch of lines connecting to the previous layer,"}, {"start": 708.6400000000001, "end": 714.48, "text": "which we had earlier in this series, that's typically meant to convey this combination of a linear"}, {"start": 714.5600000000001, "end": 721.2, "text": "step, a matrix multiplication, followed by some simple, termwise non-linear function, like a Rayloo."}, {"start": 722.32, "end": 727.52, "text": "You would say that this neuron is active whenever this value is positive, and that it's inactive"}, {"start": 727.52, "end": 733.76, "text": "if that value is zero. The next step looks very similar to the first one. You multiply by a very"}, {"start": 733.76, "end": 739.36, "text": "large matrix and you add on a certain bias term. In this case, the number of dimensions in the output"}, {"start": 739.36, "end": 744.08, "text": "is back down to the size of that embedding space, so I'm going to go ahead and call this the"}, {"start": 744.08, "end": 749.2800000000001, "text": "down projection matrix. And this time, instead of thinking of things row by row, it's actually"}, {"start": 749.2800000000001, "end": 754.8000000000001, "text": "nicer to think of it column by column. You see, another way that you can hold matrix multiplication"}, {"start": 754.8000000000001, "end": 760.24, "text": "in your head is to imagine taking each column of the matrix and multiplying it by the corresponding"}, {"start": 760.24, "end": 765.5200000000001, "text": "term in the vector that it's processing and adding together all of those rescaled columns."}, {"start": 766.72, "end": 771.12, "text": "The reason it's nicer to think about this way is because here, the columns have the same"}, {"start": 771.12, "end": 776.64, "text": "dimension as the embedding space, so we can think of them as directions in that space. For instance,"}, {"start": 776.64, "end": 781.6, "text": "we will imagine that the model has learned to make that first column into this basketball direction"}, {"start": 781.6, "end": 787.04, "text": "that we suppose exists. What that would mean is that when the relevant neuron in that first"}, {"start": 787.04, "end": 792.88, "text": "position is active, we'll be adding this column to the final result. But if that neuron was inactive,"}, {"start": 792.88, "end": 798.16, "text": "if that number was zero, then this would have no effect. And it doesn't just have to be basketball,"}, {"start": 798.16, "end": 803.12, "text": "the model could also bake into this column many other features that it wants to associate with"}, {"start": 803.12, "end": 809.76, "text": "something that has the full name Michael Jordan. And at the same time, all of the other columns"}, {"start": 809.76, "end": 815.92, "text": "in this matrix are telling you what will be added to the final result if the corresponding neuron"}, {"start": 815.92, "end": 821.12, "text": "is active. And if you have a bias in this case, it's something that you're just adding every"}, {"start": 821.12, "end": 826.0799999999999, "text": "single time, regardless of the neuron values. You might wonder what's that doing, as with all"}, {"start": 826.08, "end": 830.4000000000001, "text": "parameter field objects here, it's kind of hard to say exactly. Maybe there's some book"}, {"start": 830.4000000000001, "end": 835.0400000000001, "text": "keeping that the network needs to do, but you can feel free to ignore it for now. Making our"}, {"start": 835.0400000000001, "end": 840.64, "text": "notation a little more compact again, I'll call this big matrix W down. And similarly, call that"}, {"start": 840.64, "end": 846.5600000000001, "text": "bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with"}, {"start": 846.5600000000001, "end": 851.6, "text": "this final result is add it to the vector that flowed into the block at that position,"}, {"start": 851.6800000000001, "end": 856.48, "text": "and that gets you this final result. So for example, if the vector flowing in,"}, {"start": 856.48, "end": 862.08, "text": "encoded both first name Michael and last name Jordan, then because this sequence of operations"}, {"start": 862.08, "end": 867.44, "text": "will trigger that AND gate, it will add on the basketball direction, so what pops out"}, {"start": 867.44, "end": 872.8000000000001, "text": "will encode all of those together. And remember, this is a process happening to every one of those"}, {"start": 872.8000000000001, "end": 878.8000000000001, "text": "vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just"}, {"start": 878.8, "end": 884.8, "text": "have 50,000 neurons in it, it has 50,000 times the number of tokens in the input."}, {"start": 888.3199999999999, "end": 894.16, "text": "So that is the entire operation, two matrix products each with a bias added, and a simple clipping"}, {"start": 894.16, "end": 899.3599999999999, "text": "function in between. Any of you who watched the earlier videos of the series will recognize this"}, {"start": 899.3599999999999, "end": 904.0, "text": "structure as the most basic kind of neural network that we studied there. In that example, it was"}, {"start": 904.48, "end": 910.56, "text": "recognized handwritten digits. Over here, in the context of a transformer for a large language model,"}, {"start": 910.56, "end": 916.88, "text": "this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing"}, {"start": 916.88, "end": 922.24, "text": "is heavily intertwined with the idea of encoding information into vectors of a high-dimensional"}, {"start": 922.24, "end": 928.0, "text": "embedding space. That is the core lesson, but I do want to step back and reflect on two different"}, {"start": 928.0, "end": 933.12, "text": "things. The first of which is a kind of bookkeeping, and the second of which involves a very thought"}, {"start": 933.12, "end": 937.92, "text": "provoking fact about higher dimensions that I actually didn't know until I dug into transformers."}, {"start": 941.36, "end": 946.64, "text": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3"}, {"start": 946.64, "end": 952.16, "text": "and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how"}, {"start": 952.16, "end": 958.24, "text": "this up projection matrix has just under 50,000 rows, and that each row matches the size of the"}, {"start": 958.24, "end": 966.0, "text": "embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604"}, {"start": 966.0, "end": 972.24, "text": "million parameters just for that matrix, and the down projection has the same number of parameters"}, {"start": 972.24, "end": 978.88, "text": "just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector"}, {"start": 978.88, "end": 983.12, "text": "also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not"}, {"start": 983.12, "end": 989.04, "text": "even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but"}, {"start": 989.04, "end": 996.32, "text": "96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about"}, {"start": 996.32, "end": 1002.72, "text": "116 billion. This is around two thirds of the total parameters in the network, and when you add it"}, {"start": 1002.72, "end": 1007.28, "text": "to everything that we had before for the attention blocks, the embedding, and the unembedding,"}, {"start": 1007.28, "end": 1011.52, "text": "you do indeed get that grand total of 175 billion as advertised."}, {"start": 1012.8, "end": 1017.12, "text": "It's probably worth mentioning there's another set of parameters associated with those normalization"}, {"start": 1017.12, "end": 1022.24, "text": "steps that this explanation has skipped over, but like the bias vector, they account for a very"}, {"start": 1022.24, "end": 1029.12, "text": "trivial proportion of the total. As to that second point of reflection, you might be wondering if this"}, {"start": 1029.12, "end": 1034.08, "text": "central toy example we've been spending so much time on reflects how facts are actually stored"}, {"start": 1034.08, "end": 1039.28, "text": "in real-large language models. It is true that the rows of that first matrix can be thought of"}, {"start": 1039.28, "end": 1044.56, "text": "as directions in this embedding space, and that means the activation of each neuron tells you how"}, {"start": 1044.56, "end": 1049.6799999999998, "text": "much a given vector aligns with some specific direction. It's also true that the columns of that"}, {"start": 1049.6799999999998, "end": 1055.28, "text": "second matrix tell you what will be added to the result if that neuron is active. Both of those"}, {"start": 1055.28, "end": 1062.08, "text": "are just mathematical facts. However, the evidence does suggest that individual neurons very rarely"}, {"start": 1062.1599999999999, "end": 1067.4399999999998, "text": "represent a single clean feature like Michael Jordan. And there may actually be a very good reason"}, {"start": 1067.4399999999998, "end": 1072.72, "text": "this is the case, related to an idea floating around interpretability researchers these days,"}, {"start": 1072.72, "end": 1078.24, "text": "known as superposition. This is a hypothesis that might help to explain both why the models are"}, {"start": 1078.24, "end": 1084.6399999999999, "text": "especially hard to interpret, and also why they scale surprisingly well. The basic idea is that"}, {"start": 1084.6399999999999, "end": 1089.76, "text": "if you have an end-dimensional space and you want to represent a bunch of different features"}, {"start": 1089.76, "end": 1093.84, "text": "using directions that are all perpendicular to one another in that space,"}, {"start": 1093.84, "end": 1098.0, "text": "you know, that way if you add a component in one direction it doesn't influence any of the other"}, {"start": 1098.0, "end": 1104.4, "text": "directions. Then the maximum number of vectors you can fit is only N, the number of dimensions."}, {"start": 1104.4, "end": 1110.0, "text": "To a mathematician actually this is the definition of dimension, but where it gets interesting is if"}, {"start": 1110.0, "end": 1115.6, "text": "you relax that constraint a little bit and you tolerate some noise. Say you allow those features to"}, {"start": 1115.6, "end": 1120.8799999999999, "text": "be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular,"}, {"start": 1120.8799999999999, "end": 1127.52, "text": "maybe between 89 and 91 degrees apart. If we were in two or three dimensions, this makes no"}, {"start": 1127.52, "end": 1132.1599999999999, "text": "difference that gives you hardly any extra wiggle room to fit more vectors in, which makes it all"}, {"start": 1132.1599999999999, "end": 1138.1599999999999, "text": "the more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a"}, {"start": 1138.1599999999999, "end": 1143.12, "text": "really quick and dirty illustration of this using some scrappy python that's going to create a list"}, {"start": 1143.12, "end": 1149.76, "text": "of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000"}, {"start": 1149.76, "end": 1156.3999999999999, "text": "distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows"}, {"start": 1156.3999999999999, "end": 1162.4799999999998, "text": "the distribution of angles between pairs of these vectors. So because they started at random those"}, {"start": 1162.4799999999998, "end": 1168.08, "text": "angles could be anything from 0 to 180 degrees, but you'll notice that already even just for random"}, {"start": 1168.08, "end": 1173.1999999999998, "text": "vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do"}, {"start": 1173.1999999999998, "end": 1179.1999999999998, "text": "is run a certain optimization process that iteratively nudges all of these vectors so that they try"}, {"start": 1179.1999999999998, "end": 1184.72, "text": "to become more perpendicular to one another. After repeating this many different times, here's what"}, {"start": 1184.72, "end": 1190.08, "text": "the distribution of angles looks like. We have to actually zoom in on it here because all of the"}, {"start": 1190.08, "end": 1196.8, "text": "possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees."}, {"start": 1198.08, "end": 1203.04, "text": "In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number"}, {"start": 1203.04, "end": 1209.36, "text": "of vectors you can cram into a space that are nearly perpendicular like this grows exponentially"}, {"start": 1209.36, "end": 1214.8799999999999, "text": "with the number of dimensions. This is very significant for large language models, which might"}, {"start": 1214.8799999999999, "end": 1220.8799999999999, "text": "benefit from associating independent ideas with nearly perpendicular directions. It means that it's"}, {"start": 1220.8799999999999, "end": 1226.0, "text": "possible for it to store many, many more ideas than there are dimensions in the space that it's"}, {"start": 1226.0, "end": 1231.76, "text": "allotted. This might partially explain why model performance seems to scale so well with size."}, {"start": 1232.32, "end": 1238.16, "text": "A space that has 10 times as many dimensions can store way, way more than 10 times as many"}, {"start": 1238.16, "end": 1243.76, "text": "independent ideas. And this is relevant not just to that embedding space where the vectors flowing"}, {"start": 1243.76, "end": 1248.8, "text": "through the model live, but also to that vector full of neurons in the middle of that multi-layer"}, {"start": 1248.8, "end": 1254.88, "text": "perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing"}, {"start": 1254.88, "end": 1261.1200000000001, "text": "at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly"}, {"start": 1261.1200000000001, "end": 1266.24, "text": "perpendicular directions of the space, it could be probing at many, many more features of the vector"}, {"start": 1266.24, "end": 1271.92, "text": "being processed. But if it was doing that, what it means is that individual features aren't going"}, {"start": 1271.92, "end": 1276.96, "text": "to be visible as a single neuron lighting up. It would have to look like some specific combination"}, {"start": 1276.96, "end": 1283.0400000000002, "text": "of neurons instead, a superposition. For any of you curious to learn more, a key relevant search"}, {"start": 1283.12, "end": 1288.32, "text": "term here is sparse autoencoder, which is a tool that some of the interpretability people use to"}, {"start": 1288.32, "end": 1292.8799999999999, "text": "try to extract what the true features are, even if they're superimposed on all these neurons."}, {"start": 1293.6, "end": 1296.6399999999999, "text": "I'll link to a couple really great anthropic posts all about this."}, {"start": 1297.76, "end": 1302.48, "text": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most"}, {"start": 1302.48, "end": 1307.6, "text": "important points. The main thing that I want to cover in a next chapter is the training process."}, {"start": 1308.32, "end": 1312.6399999999999, "text": "On the one hand, the short answer for how training works is that it's all back propagation,"}, {"start": 1312.64, "end": 1316.8000000000002, "text": "and we covered back propagation in a separate context with earlier chapters in the series."}, {"start": 1317.3600000000001, "end": 1322.3200000000002, "text": "But there is more to discuss, like the specific cost function used for language models,"}, {"start": 1322.3200000000002, "end": 1326.8000000000002, "text": "the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of"}, {"start": 1326.8000000000002, "end": 1332.48, "text": "scaling laws. Quick note for the active followers among you, there are a number of non-machine"}, {"start": 1332.48, "end": 1336.8000000000002, "text": "learning related videos that I'm excited to sync my teeth into before I make that next chapter,"}, {"start": 1336.8, "end": 1346.8, "text": "so it might be a while, but I do promise it'll come in due time."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0001.png", "text": "pesbarge: -\nwhere? cx)", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0002.png", "text": "Attention Multilayer Perceptron", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0003.png", "text": "Structure:\n—_—+ ——_+ ——+\nLinear ReLU Linear Easv", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0004.png", "text": "P\n\ncars\na —_—\n*", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0006.png", "text": "Many\nmmecees aie ee Tepetitions", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0007.png", "text": "fr rs | a fT ro |\n\n: foe\n\n: ny 12,288 J | 7!\n\noe oe tha Peron Numbers | ss", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0008.png", "text": "pt", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0009.png", "text": "= 1/3 of the Parameters = 2/3 of the Parameters\nRae sab wmrae.\n“Tf tag ; Nara\nyp wooo", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0010.png", "text": "Pes Se Micha! | é ,\nfo Cpt yp\nfoe fo,", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0011.png", "text": "And if , oy\nnom Tt", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0012.png", "text": "Michael Jordan|plavs the sport of\n)oyil.ti", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0013.png", "text": "MLP", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0016.png", "text": "Ho is\nR RE\n——k:—] b =a\ncy wah,\nMLP\nLemees 5\n|=", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0017.png", "text": "PN Matec. = TN Tawar\n—k ta a\na Lad\n& F| —|ar\n—k. — ae\nMLP\num 7", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0018.png", "text": "=|", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0019.png", "text": "Bias\n° so D ene ee\n| : : te] Bescetes “Michael Jeadaay\nat 7 ‘\nMLP\nLaas =", "timestamp": "0019"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0020.png", "text": "Bias\na) fa)\nWala] + Pa all ie\nwl) re\nwt) Loe,\nMLP\na.", "timestamp": "0020"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0021.png", "text": "(M+J -E\n. Mich\nRy,\n| _— oso a\naoa oa Nexis", "timestamp": "0021"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0022.png", "text": "Se\nC7 Reeser. Tele .\n. Unit : :\nae!\nMLP\n4 F a\nwe 2] me\n7", "timestamp": "0022"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0023.png", "text": "28 Gator .\nwR a .\nLinear on a\n: Une ’ a\nMLP\n« 13 :\nSH\nww", "timestamp": "0023"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0024.png", "text": "e\ns\nabo e@ — 3 Neurons\n\n°\nMLP\n. .\nwe\nj= on", "timestamp": "0024"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0025.png", "text": "“Dew proyection’ .\nMLP. :", "timestamp": "0025"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0026.png", "text": "Ge ' ‘ : :\n_—\n; toe eo 6 c -\nMLP\n_— ry ty\n. i ne ee\nJ et", "timestamp": "0026"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0027.png", "text": "rr ooo\n: Et ee ¢ ' Ble,\nNar.bet 23 MLP\nVee we | ne tee |e", "timestamp": "0027"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0029.png", "text": "MLP.\nwi mw i |", "timestamp": "0029"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0030.png", "text": "MLP — OG Neural Network\n‘Training in\nprogress. . .", "timestamp": "0030"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0031.png", "text": "Two points of reflection", "timestamp": "0031"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0032.png", "text": "12,288.\n———_. Bias\nsn 7 | |\nMLP\n' :\nbtw |", "timestamp": "0032"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0033.png", "text": "96 Layers\nSaree ali a wwe", "timestamp": "0033"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0034.png", "text": "x06\n; Attention MLP Layer Norm ;\n604M Parametern 1,23 Parameters 49K Paramecers", "timestamp": "0034"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0035.png", "text": "SS\na Multilayer Perceptron\n\n.\n\n—.e °\n\n: 4\n\n‘ °\n\n:\n\n—_— 3\n\n“ :\n—_—_— —- = —_____\n\n°\n\ni", "timestamp": "0035"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0036.png", "text": "Superposition\n‘K", "timestamp": "0036"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0037.png", "text": "Choose multiple vectors,\neach pair 90° apart\nMaxaumum a of vectors. 4\nN-dimensional\nSpace", "timestamp": "0037"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0038.png", "text": "Choose mulnple vectors,\noo w each pair between 80° and 91° apart\nN-dimensional\nSpace", "timestamp": "0038"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0039.png", "text": "i=", "timestamp": "0039"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0040.png", "text": "Choose mulnple vectors,\noo a each pair between 89° and 91° apart\nTe\n>\nN-dimensional\nSpace", "timestamp": "0040"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0041.png", "text": "GPT-2 — GPT-3 > GP 1-1\nMore, dim 766 Model din 12.288 mo\n=", "timestamp": "0041"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0042.png", "text": "oa\na Ane\nby aa\nNt", "timestamp": "0042"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0043.png", "text": ": 3 vt\n:. There's race\nrevearch using\nHow would yo SRA Antorncorens\ntnt this?", "timestamp": "0043"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0044.png", "text": ": ‘atching SBluelPrown makes you\n4\n‘ —\n_—_\n2 =\nCost = -1 =\n: =\nsos\nj\nP\n2 ee\non en at an of at on ar on cd Va —", "timestamp": "0044"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_08_frame_0045.png", "text": "~~ : ~\nNi Clicky Stuffs 7\nThese views are casponsoced. vastead Funded by vpavers\nSpecial thanks to the umes listed bekow | Sab co support\neee a\n(hoa ree none fans See\nbeeen treed nem pore oe\ntoca pane mpuvenoene monn tee\npane ae woe vena feorae", "timestamp": "0045"}], "processing_info": {"audio_duration": 1346.8, "total_segments": 255, "total_ocr_extractions": 41}}
{"video_id": "talk_09", "url": "https://www.youtube.com/watch?v=fNk_zzaMoSs  # 9. Vectors | Chapter 1, Essence of linear algebra", "timestamp": "2025-07-31T17:04:14.524208", "asr_transcript": {"video_id": "talk_09", "language": "en", "text": " The fundamental root of it all building block for linear algebra is the vector. So it's worth making sure that we're all on the same page about what exactly a vector is. You see, probably speaking, there are three distinct but related ideas about vectors, which I'll call the physics student perspective, the computer science student perspective, and the mathematicians perspective. The physics student perspective is that vectors are arrows pointing in space. What defines a given vector is its length and the direction it's pointing. But as long as those two facts are the same, you can move it all around and it's still the same vector. Vectors that live in the flat plane are two-dimensional, and those sitting in broader space that you and I live in are three-dimensional. The computer science perspective is that vectors are ordered lists of numbers. For example, let's say you were doing some analytics about house prices, and the only features you cared about were square footage and price. You might model each house with a pair of numbers, the first indicating square footage, and the second indicating price. Notice the order matters here. In the lingo, you'd be modeling houses as two-dimensional vectors, where in this context, vector is pretty much just a fancy word for list, and what makes it two-dimensional is the fact that the length of that list is two. The mathematician, on the other hand, seeks to generalize both these views, basically saying that a vector can be anything where there's a sensible notion of adding two vectors and multiplying a vector by a number, operations that I'll talk about later on in this video. The details of this view are rather abstract, and I actually think it's healthy to ignore it until the last video of this series, favoring a more concrete setting in the interim. But the reason I bring it up here is that it hints at the fact that the ideas of vector addition and multiplication by numbers will play an important role throughout linear algebra. But before I talk about these operations, let's just settle in on a specific thought to have in mind when I say the word vector. Given the geometric focus that I'm shooting for here, whenever I introduce a new topic involving vectors, I want you to first think about an arrow, and specifically think about that arrow inside a coordinate system, like the xy plane, with its tail sitting at the origin. This is a little bit different from the physics student perspective, where vectors can freely sit anywhere they want in space. In linear algebra, it's almost always the case that your vector will be rooted at the origin. Then, once you understand a new concept in the context of arrows in space, we'll translate it over to the list of numbers point of view, which we can do by considering the coordinates of the vector. Now, while I'm sure that many of you are already familiar with this coordinate system, it's worth walking through explicitly, since this is where all of the important back and forth happens between the two perspectives of linear algebra. Focusing our attention on two dimensions for the moment, you have a horizontal line called the x-axis, and a vertical line called the y-axis. The place where they intersect is called the origin, which you should think of as the center of space and the root of all vectors. After choosing an arbitrary length to represent one, you make tick marks on each axis to represent this distance. When I want to convey the idea of 2D space as a whole, which you'll see comes up a lot in these videos, I'll extend these tick marks to make grid lines, but right now they'll actually get a little bit in the way. The coordinates of a vector is a pair of numbers that basically gives instructions for how to get from the tail of that vector at the origin to its tip. The first number tells you how far to walk along the x-axis, positive numbers indicating rightward motion, negative numbers indicating leftward motion, and the second number tells you how far to walk parallel to the y-axis after that, positive numbers indicating upward motion and negative numbers indicating downward motion. To distinguish vectors from points, the convention is to write this pair of numbers vertically with square brackets around them. Every pair of numbers gives you one and only one vector, and every vector is associated with one and only one pair of numbers. What about in three dimensions? Well, you add a third axis called the z-axis, which is perpendicular to both the x and y-axis, and in this case, each vector is associated with an ordered triplet of numbers. The first tells you how far to move along the x-axis, the second tells you how far to move parallel to the y-axis, and the third one tells you how far to then move parallel to this new z-axis. Every triplet of numbers gives you one unique vector in space, and every vector in space gives you exactly one triplet of numbers. All right, so back to vector addition and multiplication by numbers. Overall, every topigan linear algebra is going to center around these two operations. Luckily, each one is pretty straightforward to define. Let's say we have two vectors, one pointing up and a little to the right, and the other one pointing right and down a bit. To add these two vectors, move the second one so that its tail sits at the tip of the first one. Then, if you draw a new vector from the tail of the first one to where the tip of the second one now sits, that new vector is there some. This definition of addition, by the way, is pretty much the only time in linear algebra where we let vectors stray away from the origin. Now, why is this a reasonable thing to do? Why this definition of addition and not some other one? Well, the way I like to think about it is that each vector represents a certain movement, a step with a certain distance and direction in space. If you take a step along the first vector, then take a step in the direction and distance described by the second vector. The overall effect is just the same as if you moved along the sum of those two vectors to start with. You could think about this as an extension of how we think about adding numbers on a number line. One way that we teach kids to think about this, say with 2 plus 5, is to think of moving two steps to the right, followed by another five steps to the right. The overall effect is the same as if you just took seven steps to the right. In fact, let's see how vector addition looks numerically. The first vector here has coordinates 1, 2. And the second one has coordinates 3, negative 1. When you take the vector sum using this tip-to-tail method, you can think of a four-step path from the origin to the tip of the second vector. Walk one to the right, then two up, then three to the right, then one down. Reorganizing these steps so that you first do all of the right word motion, then do all the vertical motion, you can read it as saying, first move 1 plus 3 to the right, then move 2 minus 1 up. So the new vector has coordinates 1 plus 3 and 2 plus negative 1. In general, vector addition in this list of numbers conception looks like matching up their terms and adding each one together. The other fundamental vector operation is multiplication by a number. Now this is best understood just by looking at a few examples. If you take the number 2 and multiply it by a given vector, it means you stretch out that vector so that it's two times as long as when you started. If you multiply that vector by say 1 third, it means you squish it down so that it's one third the original length. When you multiply it by a negative number, like negative 1.8, then the vector first gets flipped around, then stretched out by that factor of 1.8. This process of stretching or squishing or sometimes reversing the direction of a vector is called scaling. And whenever you catch a number like 2 or 1 third or negative 1.8 acting like this, scaling some vector, you call it a scalar. In fact, throughout linear algebra, one of the main things that numbers do is scale vectors, so it's common to use the word scalar pretty much interchangeably with the word number. Numerically, stretching out a vector by a factor of, say, 2 corresponds with multiplying each of its components by that factor, 2. So when the conception of vectors as lists of numbers, multiplying a given vector by a scalar means multiplying each one of those components by that scalar. You'll see in the following videos what I mean when I say that linear algebra topics tend to revolve around these two fundamental operations, vector addition and scalar multiplication. And I'll talk more in the last video about how and why the mathematician thinks only about these operations independent and abstracted away from, however you choose to represent vectors. And truth, it doesn't matter whether you think about vectors as fundamentally being arrows in space, like I'm suggesting you do, that happen to have a nice numerical representation, or fundamentally as lists of numbers that happen to have a nice geometric interpretation. The usefulness of linear algebra has less to do with either one of these views than it does with the ability to translate back and forth between them. It gives the data analyst a nice way to conceptualize many lists of numbers in a visual way, which can seriously clarify patterns and data and give a global view of what certain operations do. And on the flip side, it gives people like physicists and computer graphics programmers a language to describe space and the manipulation of space using numbers that can be crunched and run through a computer. When I do mathy animations, for example, I start by thinking about what's actually going on in space and then get the computer to represent things numerically, thereby figuring out where to place the pixels on the screen. And doing that usually relies on a lot of linear algebra understanding. So there are your vector basics, and in the next video, I'll start getting into some pretty neat concepts surrounding vectors, like span, bases, and linear dependence. See you then!", "segments": [{"start": 0.0, "end": 15.68, "text": "The fundamental root of it all building block for linear algebra is the vector."}, {"start": 15.68, "end": 19.44, "text": "So it's worth making sure that we're all on the same page about what exactly a vector"}, {"start": 19.44, "end": 20.44, "text": "is."}, {"start": 20.44, "end": 24.96, "text": "You see, probably speaking, there are three distinct but related ideas about vectors, which"}, {"start": 24.96, "end": 28.92, "text": "I'll call the physics student perspective, the computer science student perspective, and"}, {"start": 28.92, "end": 30.8, "text": "the mathematicians perspective."}, {"start": 30.8, "end": 34.96, "text": "The physics student perspective is that vectors are arrows pointing in space."}, {"start": 34.96, "end": 39.2, "text": "What defines a given vector is its length and the direction it's pointing."}, {"start": 39.2, "end": 42.44, "text": "But as long as those two facts are the same, you can move it all around and it's still"}, {"start": 42.44, "end": 44.120000000000005, "text": "the same vector."}, {"start": 44.120000000000005, "end": 48.24, "text": "Vectors that live in the flat plane are two-dimensional, and those sitting in broader space that you"}, {"start": 48.24, "end": 51.760000000000005, "text": "and I live in are three-dimensional."}, {"start": 51.760000000000005, "end": 56.24, "text": "The computer science perspective is that vectors are ordered lists of numbers."}, {"start": 56.24, "end": 60.2, "text": "For example, let's say you were doing some analytics about house prices, and the only"}, {"start": 60.2, "end": 63.24, "text": "features you cared about were square footage and price."}, {"start": 63.24, "end": 67.48, "text": "You might model each house with a pair of numbers, the first indicating square footage,"}, {"start": 67.48, "end": 69.4, "text": "and the second indicating price."}, {"start": 69.4, "end": 72.8, "text": "Notice the order matters here."}, {"start": 72.8, "end": 77.24000000000001, "text": "In the lingo, you'd be modeling houses as two-dimensional vectors, where in this context,"}, {"start": 77.24000000000001, "end": 81.96000000000001, "text": "vector is pretty much just a fancy word for list, and what makes it two-dimensional is"}, {"start": 81.96000000000001, "end": 85.80000000000001, "text": "the fact that the length of that list is two."}, {"start": 85.8, "end": 90.32, "text": "The mathematician, on the other hand, seeks to generalize both these views, basically saying"}, {"start": 90.32, "end": 94.67999999999999, "text": "that a vector can be anything where there's a sensible notion of adding two vectors and"}, {"start": 94.67999999999999, "end": 99.47999999999999, "text": "multiplying a vector by a number, operations that I'll talk about later on in this video."}, {"start": 99.47999999999999, "end": 103.6, "text": "The details of this view are rather abstract, and I actually think it's healthy to ignore"}, {"start": 103.6, "end": 108.44, "text": "it until the last video of this series, favoring a more concrete setting in the interim."}, {"start": 108.44, "end": 112.47999999999999, "text": "But the reason I bring it up here is that it hints at the fact that the ideas of vector"}, {"start": 112.48, "end": 117.88000000000001, "text": "addition and multiplication by numbers will play an important role throughout linear algebra."}, {"start": 117.88000000000001, "end": 121.76, "text": "But before I talk about these operations, let's just settle in on a specific thought to"}, {"start": 121.76, "end": 124.72, "text": "have in mind when I say the word vector."}, {"start": 124.72, "end": 128.76, "text": "Given the geometric focus that I'm shooting for here, whenever I introduce a new topic involving"}, {"start": 128.76, "end": 133.64000000000001, "text": "vectors, I want you to first think about an arrow, and specifically think about that"}, {"start": 133.64000000000001, "end": 139.64000000000001, "text": "arrow inside a coordinate system, like the xy plane, with its tail sitting at the origin."}, {"start": 139.64, "end": 143.2, "text": "This is a little bit different from the physics student perspective, where vectors can freely"}, {"start": 143.2, "end": 145.27999999999997, "text": "sit anywhere they want in space."}, {"start": 145.27999999999997, "end": 150.95999999999998, "text": "In linear algebra, it's almost always the case that your vector will be rooted at the origin."}, {"start": 150.95999999999998, "end": 155.95999999999998, "text": "Then, once you understand a new concept in the context of arrows in space, we'll translate"}, {"start": 155.95999999999998, "end": 159.83999999999997, "text": "it over to the list of numbers point of view, which we can do by considering the coordinates"}, {"start": 159.83999999999997, "end": 160.83999999999997, "text": "of the vector."}, {"start": 160.83999999999997, "end": 165.04, "text": "Now, while I'm sure that many of you are already familiar with this coordinate system,"}, {"start": 165.04, "end": 168.51999999999998, "text": "it's worth walking through explicitly, since this is where all of the important back"}, {"start": 168.52, "end": 172.8, "text": "and forth happens between the two perspectives of linear algebra."}, {"start": 172.8, "end": 176.4, "text": "Focusing our attention on two dimensions for the moment, you have a horizontal line called"}, {"start": 176.4, "end": 180.16000000000003, "text": "the x-axis, and a vertical line called the y-axis."}, {"start": 180.16000000000003, "end": 183.28, "text": "The place where they intersect is called the origin, which you should think of as the"}, {"start": 183.28, "end": 186.36, "text": "center of space and the root of all vectors."}, {"start": 186.36, "end": 190.48000000000002, "text": "After choosing an arbitrary length to represent one, you make tick marks on each axis to represent"}, {"start": 190.48000000000002, "end": 192.24, "text": "this distance."}, {"start": 192.24, "end": 195.88, "text": "When I want to convey the idea of 2D space as a whole, which you'll see comes up a lot"}, {"start": 195.88, "end": 199.92, "text": "in these videos, I'll extend these tick marks to make grid lines, but right now they'll"}, {"start": 199.92, "end": 201.88, "text": "actually get a little bit in the way."}, {"start": 201.88, "end": 206.56, "text": "The coordinates of a vector is a pair of numbers that basically gives instructions for how"}, {"start": 206.56, "end": 210.72, "text": "to get from the tail of that vector at the origin to its tip."}, {"start": 210.72, "end": 214.84, "text": "The first number tells you how far to walk along the x-axis, positive numbers indicating"}, {"start": 214.84, "end": 219.28, "text": "rightward motion, negative numbers indicating leftward motion, and the second number tells"}, {"start": 219.28, "end": 224.35999999999999, "text": "you how far to walk parallel to the y-axis after that, positive numbers indicating upward"}, {"start": 224.36, "end": 228.16000000000003, "text": "motion and negative numbers indicating downward motion."}, {"start": 228.16000000000003, "end": 232.0, "text": "To distinguish vectors from points, the convention is to write this pair of numbers vertically"}, {"start": 232.0, "end": 236.28, "text": "with square brackets around them."}, {"start": 236.28, "end": 241.20000000000002, "text": "Every pair of numbers gives you one and only one vector, and every vector is associated"}, {"start": 241.20000000000002, "end": 244.44000000000003, "text": "with one and only one pair of numbers."}, {"start": 244.44000000000003, "end": 245.76000000000002, "text": "What about in three dimensions?"}, {"start": 245.76000000000002, "end": 252.48000000000002, "text": "Well, you add a third axis called the z-axis, which is perpendicular to both the x and y-axis,"}, {"start": 252.48, "end": 256.56, "text": "and in this case, each vector is associated with an ordered triplet of numbers."}, {"start": 256.56, "end": 260.76, "text": "The first tells you how far to move along the x-axis, the second tells you how far to"}, {"start": 260.76, "end": 265.71999999999997, "text": "move parallel to the y-axis, and the third one tells you how far to then move parallel"}, {"start": 265.71999999999997, "end": 268.28, "text": "to this new z-axis."}, {"start": 268.28, "end": 273.24, "text": "Every triplet of numbers gives you one unique vector in space, and every vector in space"}, {"start": 273.24, "end": 275.96, "text": "gives you exactly one triplet of numbers."}, {"start": 275.96, "end": 280.4, "text": "All right, so back to vector addition and multiplication by numbers."}, {"start": 280.4, "end": 284.84, "text": "Overall, every topigan linear algebra is going to center around these two operations."}, {"start": 284.84, "end": 288.28, "text": "Luckily, each one is pretty straightforward to define."}, {"start": 288.28, "end": 291.84, "text": "Let's say we have two vectors, one pointing up and a little to the right, and the other"}, {"start": 291.84, "end": 293.96, "text": "one pointing right and down a bit."}, {"start": 293.96, "end": 299.0, "text": "To add these two vectors, move the second one so that its tail sits at the tip of the first"}, {"start": 299.0, "end": 300.0, "text": "one."}, {"start": 300.0, "end": 304.88, "text": "Then, if you draw a new vector from the tail of the first one to where the tip of the"}, {"start": 304.88, "end": 308.4, "text": "second one now sits, that new vector is there some."}, {"start": 311.4, "end": 316.2, "text": "This definition of addition, by the way, is pretty much the only time in linear algebra"}, {"start": 316.2, "end": 318.91999999999996, "text": "where we let vectors stray away from the origin."}, {"start": 318.91999999999996, "end": 321.4, "text": "Now, why is this a reasonable thing to do?"}, {"start": 321.4, "end": 324.32, "text": "Why this definition of addition and not some other one?"}, {"start": 324.32, "end": 329.4, "text": "Well, the way I like to think about it is that each vector represents a certain movement,"}, {"start": 329.4, "end": 333.96, "text": "a step with a certain distance and direction in space."}, {"start": 333.96, "end": 337.88, "text": "If you take a step along the first vector, then take a step in the direction and distance"}, {"start": 337.96, "end": 339.8, "text": "described by the second vector."}, {"start": 339.8, "end": 343.68, "text": "The overall effect is just the same as if you moved along the sum of those two vectors"}, {"start": 343.68, "end": 345.24, "text": "to start with."}, {"start": 345.24, "end": 348.76, "text": "You could think about this as an extension of how we think about adding numbers on a number"}, {"start": 348.76, "end": 349.96, "text": "line."}, {"start": 349.96, "end": 354.2, "text": "One way that we teach kids to think about this, say with 2 plus 5, is to think of moving"}, {"start": 354.2, "end": 358.15999999999997, "text": "two steps to the right, followed by another five steps to the right."}, {"start": 358.15999999999997, "end": 361.76, "text": "The overall effect is the same as if you just took seven steps to the right."}, {"start": 361.76, "end": 365.88, "text": "In fact, let's see how vector addition looks numerically."}, {"start": 365.88, "end": 369.64, "text": "The first vector here has coordinates 1, 2."}, {"start": 369.64, "end": 374.32, "text": "And the second one has coordinates 3, negative 1."}, {"start": 374.32, "end": 378.68, "text": "When you take the vector sum using this tip-to-tail method, you can think of a four-step path"}, {"start": 378.68, "end": 381.56, "text": "from the origin to the tip of the second vector."}, {"start": 381.56, "end": 386.32, "text": "Walk one to the right, then two up, then three to the right, then one down."}, {"start": 386.32, "end": 390.36, "text": "Reorganizing these steps so that you first do all of the right word motion, then do all"}, {"start": 390.36, "end": 395.36, "text": "the vertical motion, you can read it as saying, first move 1 plus 3 to the right, then"}, {"start": 395.36, "end": 400.08000000000004, "text": "move 2 minus 1 up."}, {"start": 400.08000000000004, "end": 405.32, "text": "So the new vector has coordinates 1 plus 3 and 2 plus negative 1."}, {"start": 405.32, "end": 409.72, "text": "In general, vector addition in this list of numbers conception looks like matching up"}, {"start": 409.72, "end": 414.6, "text": "their terms and adding each one together."}, {"start": 414.6, "end": 418.40000000000003, "text": "The other fundamental vector operation is multiplication by a number."}, {"start": 418.40000000000003, "end": 421.6, "text": "Now this is best understood just by looking at a few examples."}, {"start": 421.6, "end": 425.96000000000004, "text": "If you take the number 2 and multiply it by a given vector, it means you stretch out"}, {"start": 425.96000000000004, "end": 430.08000000000004, "text": "that vector so that it's two times as long as when you started."}, {"start": 430.08000000000004, "end": 434.76000000000005, "text": "If you multiply that vector by say 1 third, it means you squish it down so that it's"}, {"start": 434.76000000000005, "end": 437.24, "text": "one third the original length."}, {"start": 437.24, "end": 442.20000000000005, "text": "When you multiply it by a negative number, like negative 1.8, then the vector first gets"}, {"start": 442.20000000000005, "end": 447.0, "text": "flipped around, then stretched out by that factor of 1.8."}, {"start": 447.0, "end": 451.24, "text": "This process of stretching or squishing or sometimes reversing the direction of a vector"}, {"start": 451.24, "end": 453.12, "text": "is called scaling."}, {"start": 453.12, "end": 458.44, "text": "And whenever you catch a number like 2 or 1 third or negative 1.8 acting like this, scaling"}, {"start": 458.44, "end": 461.48, "text": "some vector, you call it a scalar."}, {"start": 461.48, "end": 466.76, "text": "In fact, throughout linear algebra, one of the main things that numbers do is scale vectors,"}, {"start": 466.76, "end": 471.56, "text": "so it's common to use the word scalar pretty much interchangeably with the word number."}, {"start": 471.56, "end": 476.56, "text": "Numerically, stretching out a vector by a factor of, say, 2 corresponds with multiplying"}, {"start": 476.56, "end": 479.88, "text": "each of its components by that factor, 2."}, {"start": 479.88, "end": 483.76, "text": "So when the conception of vectors as lists of numbers, multiplying a given vector by a"}, {"start": 483.76, "end": 490.08, "text": "scalar means multiplying each one of those components by that scalar."}, {"start": 490.08, "end": 493.48, "text": "You'll see in the following videos what I mean when I say that linear algebra topics"}, {"start": 493.48, "end": 499.56, "text": "tend to revolve around these two fundamental operations, vector addition and scalar multiplication."}, {"start": 499.56, "end": 503.36, "text": "And I'll talk more in the last video about how and why the mathematician thinks only"}, {"start": 503.36, "end": 508.12, "text": "about these operations independent and abstracted away from, however you choose to represent"}, {"start": 508.12, "end": 509.36, "text": "vectors."}, {"start": 509.36, "end": 513.04, "text": "And truth, it doesn't matter whether you think about vectors as fundamentally being arrows"}, {"start": 513.04, "end": 517.72, "text": "in space, like I'm suggesting you do, that happen to have a nice numerical representation,"}, {"start": 517.72, "end": 522.08, "text": "or fundamentally as lists of numbers that happen to have a nice geometric interpretation."}, {"start": 522.08, "end": 526.0, "text": "The usefulness of linear algebra has less to do with either one of these views than it"}, {"start": 526.0, "end": 529.84, "text": "does with the ability to translate back and forth between them."}, {"start": 529.84, "end": 535.12, "text": "It gives the data analyst a nice way to conceptualize many lists of numbers in a visual way,"}, {"start": 535.12, "end": 539.4, "text": "which can seriously clarify patterns and data and give a global view of what certain operations"}, {"start": 539.4, "end": 540.4, "text": "do."}, {"start": 540.4, "end": 544.4, "text": "And on the flip side, it gives people like physicists and computer graphics programmers"}, {"start": 544.4, "end": 549.76, "text": "a language to describe space and the manipulation of space using numbers that can be crunched"}, {"start": 549.76, "end": 551.76, "text": "and run through a computer."}, {"start": 551.76, "end": 555.96, "text": "When I do mathy animations, for example, I start by thinking about what's actually going"}, {"start": 555.96, "end": 560.6800000000001, "text": "on in space and then get the computer to represent things numerically, thereby figuring out"}, {"start": 560.6800000000001, "end": 562.92, "text": "where to place the pixels on the screen."}, {"start": 562.92, "end": 567.5999999999999, "text": "And doing that usually relies on a lot of linear algebra understanding."}, {"start": 567.5999999999999, "end": 570.68, "text": "So there are your vector basics, and in the next video, I'll start getting into some"}, {"start": 570.68, "end": 575.16, "text": "pretty neat concepts surrounding vectors, like span, bases, and linear dependence."}, {"start": 575.16, "end": 575.4799999999999, "text": "See you then!"}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0001.png", "text": "/ []\nv 2\nPhysies student — Mathematician = CS student", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0002.png", "text": "Vectors © lists of numbers\nCS student", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0003.png", "text": "Mathematician", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0004.png", "text": "ivtw;\nWad * a 2A\n. Lo We'll ignore Inu\nfor now\nMathematician", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0005.png", "text": "Pitt tT APE Tt eT\nLT TTT PAT TT TT TT\nLT TTT PIN TTT tT\nLt} TTT ET TT yt ye\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0006.png", "text": "if\ny", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0008.png", "text": "t\ny", "timestamp": "0008"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0009.png", "text": "' »", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0010.png", "text": "LT TTT ET TT TT TT\nPEE TT TT TAS TT\nLT TTT ET Pe tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0011.png", "text": "LT TTT ET TT TT tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT\nLt} TT TE | Pes tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT", "timestamp": "0011"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0012.png", "text": "2+5\n\n2 5\nSg and RRR a\nee en ee ne en", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0013.png", "text": "L]", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0014.png", "text": "id\nev", "timestamp": "0014"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0015.png", "text": "“Scaling”", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0016.png", "text": "Z pl", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0017.png", "text": "vtw\nMathematician", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0018.png", "text": "LT TT TE tT Pe TTT\n[| TT TT ira\nLit | eee Tt tt\nLt tt | eet tt eT\nLt TT aby TT TT\nLT TTT Pee TE tT", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_09_frame_0019.png", "text": "Next video: Linear combinations. span, and bases\nahee\na\nhms ‘", "timestamp": "0019"}], "processing_info": {"audio_duration": 575.4799999999999, "total_segments": 146, "total_ocr_extractions": 18}}
{"video_id": "talk_10", "url": "https://www.youtube.com/watch?v=k7RM-ot2NWY  # 10. Linear combinations, span, and basis vectors | Chapter 2", "timestamp": "2025-07-31T17:04:37.432751", "asr_transcript": {"video_id": "talk_10", "language": "en", "text": " In the last video, along with the ideas of vector addition and scalar multiplication, I described vector coordinates, where there's this back and forth between, for example, pairs of numbers and two-dimensional vectors. Now, I imagine the vector coordinates were already familiar to a lot of you, but there's another kind of interesting way to think about these coordinates, which is pretty central to linear algebra. When you have a pair of numbers that's meant to describe a vector, like 3-2, I want you to think about each coordinate as a scalar, meaning think about how each one stretches or squishes vectors. In the xy coordinate system, there are two very special vectors, the one pointing to the right with length 1, commonly called i-hat, or the unit vector in the x direction, and the one pointing straight up with length 1, commonly called j-hat, or the unit vector in the y direction. Now, think of the x coordinate of our vector as a scalar that scales i-hat, stretching it by a factor of 3, and the y-coordinate as a scalar that scales j-hat, flipping it and stretching it by a factor of 2. In this sense, the vector that these coordinates describe is the sum of two scaled vectors. That's a surprisingly important concept, this idea of adding together two scaled vectors. Those two vectors i-hat and j-hat have a special name, by the way, together they're called the basis of a coordinate system. What this means, basically, is that when you think about coordinates as scalars, the basis vectors are what those scalars actually, you know, scale. There's also a more technical definition, but I'll get to that later. By framing our coordinate system in terms of these two special basis vectors, it raises a pretty interesting and subtle point. We could have chosen different basis vectors and gotten a completely reasonable, new coordinate system. For example, think some vector pointing up into the right, along with some other vector pointing down into the right in some way. Take a moment to think about all the different vectors that you can get by choosing two scalars using each one to scale one of the vectors, then adding together what you get. Which two-dimensional vectors can you reach by altering the choices of scalars? The answer is that you can reach every possible two-dimensional vector, and I think it's a good puzzle to contemplate why. A new pair of basis vectors like this still gives us a valid way to go back and forth between pairs of numbers and two-dimensional vectors. But the association is definitely different from the one that you get using the more standard basis of i-hat and j-hat. This is something I'll go into much more detail on later, describing the exact relationship between different coordinate systems. But for right now, I just want you to appreciate the fact that anytime we describe vectors numerically, it depends on an implicit choice of what basis vectors we're using. So anytime that you're scaling two vectors and adding them like this, it's called a linear combination of those two vectors. Where does this word linear come from? Why does this have anything to do with lines? Well, this isn't the etymology, but one way I like to think about it is that if you fix one of those scalars and let the other one change its value freely, the tip of the resulting vector draws a straight line. Now, if you let both scalars range freely and consider every possible vector that you can get, there are two things that can happen. For most pairs of vectors, you'll be able to reach every possible point in the plane. Every two-dimensional vector is within your grasp. However, in the unlucky case where your two original vectors happen to line up, the tip of the resulting vector is limited to just the single line passing through the origin. Actually, technically there's a third possibility too, both your vectors could be zero, in which case you just be stuck at the origin. Here's some more terminology. The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is called the span of those two vectors. So, restating what we just saw in this lingo, the span of most pairs of 2D vectors is all vectors of 2D space. But when they line up, their span is all vectors whose tip sit on a certain line. Remember how I said that linear algebra revolves around vector addition and scalar multiplication? Well, the span of two vectors is basically a way of asking, what are all the possible vectors you can reach using only these two fundamental operations, vector addition and scalar multiplication? This is a good time to talk about how people commonly think about vectors as points. It gets really crowded to think about a whole collection of vectors sitting on a line, and more crowded still to think about all two-dimensional vectors all at once, filling up the plane. So, when dealing with collections of vectors like this, it's common to represent each one with just a point in space. The point at the tip of that vector, where, as usual, I want you thinking about that vector with its tail on the origin. That way, if you want to think about every possible vector whose tip sits on a certain line, just think about the line itself. Likewise, to think about all possible two-dimensional vectors all at once, conceptualize each one as the point where its tip sits. So, in effect, what you'll be thinking about is the infinite flat sheet of two-dimensional space itself, leaving the arrows out of it. In general, if you're thinking about a vector on its own, think of it as an arrow. And if you're dealing with a collection of vectors, it's convenient to think of them all as points. So, for our span example, the span of most pairs of vectors ends up being the entire infinite sheet of two-dimensional space. But if they line up, their span is just a line. The idea of span gets a lot more interesting if we start thinking about vectors in three-dimensional space. For example, if you take two vectors in three-d space that are not pointing in the same direction, what does it mean to take their span? Well, their span is the collection of all possible linear combinations of those two vectors, meaning all possible vectors you get by scaling each of the two of them in some way and then adding them together. You can kind of imagine turning two different knobs to change the two scalars defining the linear combination, adding the scaled vectors and following the tip of the resulting vector. That tip will trace out some kind of flat sheet cutting through the origin of three-dimensional space. This flat sheet is the span of the two vectors. Or, more precisely, the set of all possible vectors whose tips sit on that flat sheet is the span of your two vectors. Isn't that a beautiful mental image? So, what happens if we add a third vector and consider the span of all three of those guys? A linear combination of three vectors is defined pretty much the same way as it is for two. You'll choose three different scalars, scale each of those vectors, and then add them all together. And again, the span of these vectors is the set of all possible linear combinations. Two different things could happen here. If your third vector happens to be sitting on the span of the first two, then the span doesn't change. You're sort of trapped on that same flat sheet. In other words, adding a scaled version of that third vector to the linear combination doesn't really give you access to any new vectors. But if you just randomly choose a third vector, it's almost certainly not sitting on the span of those first two. Then, since it's pointing in a separate direction, it unlocks access to every possible three-dimensional vector. One way I like to think about this is that as you scale that new third vector, it moves around that span sheet of the first two, sweeping it through all of space. Another way to think about it is that you're making full use of the three freely changing scalars that you have at your disposal to access the full three dimensions of space. Now, in the case where the third vector was already sitting on the span of the first two, or the case where two vectors happened to line up, we want some terminology to describe the fact that at least one of these vectors is redundant, not adding anything to our span. Whenever this happens, where you have multiple vectors and you could remove one without reducing the span, the relevant terminology is to say that they are linearly dependent. Another way of phrasing that would be to say that one of the vectors can be expressed as a linear combination of the others, since it's already in the span of the others. On the other hand, if each vector really does add another dimension to the span, there's said to be linearly independent. So with all of that terminology, and hopefully with some good mental images to go with it, let me leave you with a puzzle before we go. The technical definition of a basis of a space is a set of linearly independent vectors that span that space. Now, given how I described a basis earlier, and given your current understanding of the words span and linearly independent, think about why this definition would make sense. In the next video, I'll get into matrices and transforming space. See you then.", "segments": [{"start": 0.0, "end": 23.0, "text": "In the last video, along with the ideas of vector addition and scalar multiplication, I described vector coordinates, where there's this back and forth between, for example, pairs of numbers and two-dimensional vectors."}, {"start": 23.5, "end": 32.5, "text": "Now, I imagine the vector coordinates were already familiar to a lot of you, but there's another kind of interesting way to think about these coordinates, which is pretty central to linear algebra."}, {"start": 32.5, "end": 45.0, "text": "When you have a pair of numbers that's meant to describe a vector, like 3-2, I want you to think about each coordinate as a scalar, meaning think about how each one stretches or squishes vectors."}, {"start": 45.0, "end": 62.0, "text": "In the xy coordinate system, there are two very special vectors, the one pointing to the right with length 1, commonly called i-hat, or the unit vector in the x direction, and the one pointing straight up with length 1, commonly called j-hat, or the unit vector in the y direction."}, {"start": 62.0, "end": 74.5, "text": "Now, think of the x coordinate of our vector as a scalar that scales i-hat, stretching it by a factor of 3, and the y-coordinate as a scalar that scales j-hat, flipping it and stretching it by a factor of 2."}, {"start": 74.5, "end": 86.5, "text": "In this sense, the vector that these coordinates describe is the sum of two scaled vectors. That's a surprisingly important concept, this idea of adding together two scaled vectors."}, {"start": 86.5, "end": 93.5, "text": "Those two vectors i-hat and j-hat have a special name, by the way, together they're called the basis of a coordinate system."}, {"start": 93.5, "end": 101.5, "text": "What this means, basically, is that when you think about coordinates as scalars, the basis vectors are what those scalars actually, you know, scale."}, {"start": 101.5, "end": 106.5, "text": "There's also a more technical definition, but I'll get to that later."}, {"start": 106.5, "end": 113.5, "text": "By framing our coordinate system in terms of these two special basis vectors, it raises a pretty interesting and subtle point."}, {"start": 113.5, "end": 120.5, "text": "We could have chosen different basis vectors and gotten a completely reasonable, new coordinate system."}, {"start": 120.5, "end": 126.5, "text": "For example, think some vector pointing up into the right, along with some other vector pointing down into the right in some way."}, {"start": 126.5, "end": 136.5, "text": "Take a moment to think about all the different vectors that you can get by choosing two scalars using each one to scale one of the vectors, then adding together what you get."}, {"start": 136.5, "end": 143.5, "text": "Which two-dimensional vectors can you reach by altering the choices of scalars?"}, {"start": 143.5, "end": 151.5, "text": "The answer is that you can reach every possible two-dimensional vector, and I think it's a good puzzle to contemplate why."}, {"start": 152.5, "end": 159.5, "text": "A new pair of basis vectors like this still gives us a valid way to go back and forth between pairs of numbers and two-dimensional vectors."}, {"start": 159.5, "end": 166.5, "text": "But the association is definitely different from the one that you get using the more standard basis of i-hat and j-hat."}, {"start": 166.5, "end": 172.5, "text": "This is something I'll go into much more detail on later, describing the exact relationship between different coordinate systems."}, {"start": 172.5, "end": 181.5, "text": "But for right now, I just want you to appreciate the fact that anytime we describe vectors numerically, it depends on an implicit choice of what basis vectors we're using."}, {"start": 181.5, "end": 190.5, "text": "So anytime that you're scaling two vectors and adding them like this, it's called a linear combination of those two vectors."}, {"start": 190.5, "end": 194.5, "text": "Where does this word linear come from? Why does this have anything to do with lines?"}, {"start": 194.5, "end": 208.5, "text": "Well, this isn't the etymology, but one way I like to think about it is that if you fix one of those scalars and let the other one change its value freely, the tip of the resulting vector draws a straight line."}, {"start": 208.5, "end": 215.5, "text": "Now, if you let both scalars range freely and consider every possible vector that you can get, there are two things that can happen."}, {"start": 215.5, "end": 222.5, "text": "For most pairs of vectors, you'll be able to reach every possible point in the plane. Every two-dimensional vector is within your grasp."}, {"start": 222.5, "end": 232.5, "text": "However, in the unlucky case where your two original vectors happen to line up, the tip of the resulting vector is limited to just the single line passing through the origin."}, {"start": 232.5, "end": 240.5, "text": "Actually, technically there's a third possibility too, both your vectors could be zero, in which case you just be stuck at the origin."}, {"start": 240.5, "end": 250.5, "text": "Here's some more terminology. The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is called the span of those two vectors."}, {"start": 250.5, "end": 265.5, "text": "So, restating what we just saw in this lingo, the span of most pairs of 2D vectors is all vectors of 2D space. But when they line up, their span is all vectors whose tip sit on a certain line."}, {"start": 265.5, "end": 271.5, "text": "Remember how I said that linear algebra revolves around vector addition and scalar multiplication?"}, {"start": 271.5, "end": 282.5, "text": "Well, the span of two vectors is basically a way of asking, what are all the possible vectors you can reach using only these two fundamental operations, vector addition and scalar multiplication?"}, {"start": 282.5, "end": 296.5, "text": "This is a good time to talk about how people commonly think about vectors as points. It gets really crowded to think about a whole collection of vectors sitting on a line, and more crowded still to think about all two-dimensional vectors all at once, filling up the plane."}, {"start": 296.5, "end": 309.5, "text": "So, when dealing with collections of vectors like this, it's common to represent each one with just a point in space. The point at the tip of that vector, where, as usual, I want you thinking about that vector with its tail on the origin."}, {"start": 309.5, "end": 317.5, "text": "That way, if you want to think about every possible vector whose tip sits on a certain line, just think about the line itself."}, {"start": 318.5, "end": 334.5, "text": "Likewise, to think about all possible two-dimensional vectors all at once, conceptualize each one as the point where its tip sits. So, in effect, what you'll be thinking about is the infinite flat sheet of two-dimensional space itself, leaving the arrows out of it."}, {"start": 334.5, "end": 344.5, "text": "In general, if you're thinking about a vector on its own, think of it as an arrow. And if you're dealing with a collection of vectors, it's convenient to think of them all as points."}, {"start": 344.5, "end": 357.5, "text": "So, for our span example, the span of most pairs of vectors ends up being the entire infinite sheet of two-dimensional space. But if they line up, their span is just a line."}, {"start": 357.5, "end": 372.5, "text": "The idea of span gets a lot more interesting if we start thinking about vectors in three-dimensional space. For example, if you take two vectors in three-d space that are not pointing in the same direction, what does it mean to take their span?"}, {"start": 372.5, "end": 385.5, "text": "Well, their span is the collection of all possible linear combinations of those two vectors, meaning all possible vectors you get by scaling each of the two of them in some way and then adding them together."}, {"start": 385.5, "end": 395.5, "text": "You can kind of imagine turning two different knobs to change the two scalars defining the linear combination, adding the scaled vectors and following the tip of the resulting vector."}, {"start": 395.5, "end": 411.5, "text": "That tip will trace out some kind of flat sheet cutting through the origin of three-dimensional space. This flat sheet is the span of the two vectors. Or, more precisely, the set of all possible vectors whose tips sit on that flat sheet is the span of your two vectors."}, {"start": 411.5, "end": 414.5, "text": "Isn't that a beautiful mental image?"}, {"start": 414.5, "end": 420.5, "text": "So, what happens if we add a third vector and consider the span of all three of those guys?"}, {"start": 420.5, "end": 435.5, "text": "A linear combination of three vectors is defined pretty much the same way as it is for two. You'll choose three different scalars, scale each of those vectors, and then add them all together."}, {"start": 435.5, "end": 443.5, "text": "And again, the span of these vectors is the set of all possible linear combinations."}, {"start": 444.5, "end": 462.5, "text": "Two different things could happen here. If your third vector happens to be sitting on the span of the first two, then the span doesn't change. You're sort of trapped on that same flat sheet. In other words, adding a scaled version of that third vector to the linear combination doesn't really give you access to any new vectors."}, {"start": 462.5, "end": 474.5, "text": "But if you just randomly choose a third vector, it's almost certainly not sitting on the span of those first two. Then, since it's pointing in a separate direction, it unlocks access to every possible three-dimensional vector."}, {"start": 474.5, "end": 485.5, "text": "One way I like to think about this is that as you scale that new third vector, it moves around that span sheet of the first two, sweeping it through all of space."}, {"start": 485.5, "end": 496.5, "text": "Another way to think about it is that you're making full use of the three freely changing scalars that you have at your disposal to access the full three dimensions of space."}, {"start": 496.5, "end": 510.5, "text": "Now, in the case where the third vector was already sitting on the span of the first two, or the case where two vectors happened to line up, we want some terminology to describe the fact that at least one of these vectors is redundant, not adding anything to our span."}, {"start": 510.5, "end": 519.5, "text": "Whenever this happens, where you have multiple vectors and you could remove one without reducing the span, the relevant terminology is to say that they are linearly dependent."}, {"start": 519.5, "end": 532.5, "text": "Another way of phrasing that would be to say that one of the vectors can be expressed as a linear combination of the others, since it's already in the span of the others."}, {"start": 532.5, "end": 539.5, "text": "On the other hand, if each vector really does add another dimension to the span, there's said to be linearly independent."}, {"start": 545.5, "end": 552.5, "text": "So with all of that terminology, and hopefully with some good mental images to go with it, let me leave you with a puzzle before we go."}, {"start": 552.5, "end": 560.5, "text": "The technical definition of a basis of a space is a set of linearly independent vectors that span that space."}, {"start": 561.5, "end": 572.5, "text": "Now, given how I described a basis earlier, and given your current understanding of the words span and linearly independent, think about why this definition would make sense."}, {"start": 573.5, "end": 578.5, "text": "In the next video, I'll get into matrices and transforming space. See you then."}]}, "ocr_extractions": [{"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0001.png", "text": "tar WW", "timestamp": "0001"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0002.png", "text": "[\".", "timestamp": "0002"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0003.png", "text": "rand ; are the “basis vectors”\nof the cy coordinate system\nNa", "timestamp": "0003"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0004.png", "text": "Waat if we chose diferent basis vectors?", "timestamp": "0004"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0005.png", "text": "0506\n124V /\n/", "timestamp": "0005"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0006.png", "text": "LT TTT ET TT TT Ty TT\nLT TTT ET TAT TT tT\nLt} TTT ET Vy TT tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT", "timestamp": "0006"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0007.png", "text": "|\nSE ate to\nLt} TT TE PT eT |\nLT TTT | eee tt\nLT TTT ET TT TT Ty YT\nLT TTT ET TT TT tT\nLT TTT ET TT TT tT", "timestamp": "0007"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0009.png", "text": "Guus @ The oof © and wos the\n1390 setof all thei iear cumb:nations\n, av-bw\nLet a and b vary\nover all real numbers.", "timestamp": "0009"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0010.png", "text": "|\nP| TTT Et TT Le\nLT TTT ET et\nPt tt tt tert Tt TT\nLT Tt Peet TT TT\n| | TT eT TTT TT tT\nfet", "timestamp": "0010"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0012.png", "text": "¢", "timestamp": "0012"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0013.png", "text": "a", "timestamp": "0013"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0015.png", "text": "” |", "timestamp": "0015"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0016.png", "text": "~", "timestamp": "0016"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0017.png", "text": "|| 1|i1 1 | | laa |\n| {| tt | il ||\n|| | | | la | |\n|i | | il\n|| | lg i=\n| | | i=\n| | |\n| | 4 a", "timestamp": "0017"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0018.png", "text": "Lineatly dependent’\nw hav\nFor all values of a :", "timestamp": "0018"}, {"image_path": "/var/folders/md/jx00px7j64jb171dzf32jybr0000gn/T/tmp5fe_z6i2/talk_10_frame_0019.png", "text": "‘Technical definition of basis\nThe Lasts of a vector space as a setof breerhy ice tent\nvectors that span the full space", "timestamp": "0019"}], "processing_info": {"audio_duration": 578.5, "total_segments": 56, "total_ocr_extractions": 16}}
