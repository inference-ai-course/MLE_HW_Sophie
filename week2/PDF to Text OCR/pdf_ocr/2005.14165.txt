OCR处理结果 - 2005.14165.pdf
处理时间: 2025-07-31 16:21:15
总页数: 75
================================================================================


--- 第 1 页 ---

a———_—_——===_=_=__———E-~E x —___—_____—_——_—_——_————_—_—__—_—_—_—_—_ —~—_——_—_—___
Language Models are Few-Shot Learners
Tom B. Brown*                 ‘Benjamin.Mann*                 Nick Ryder*                 Melanie Subbiah*
Jared Kaplan' = PrafullaDhariwal = Arvind Neelakantan = PranavShyam _ Girish Sastry:
CO                 Amanda;Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger © Tom Henighan,
a                           Rewon.Child            Aditya-Ramesh:            Daniel-M. Ziegler            Jeffrey “Wu            ‘Clemens Winter
=                           Christopher Hesse;            -Mark Chen             Eric Sigler’             Mateusz Litwin             Scott Gray:
ran                Benjamin:Chess           Jack; Clark           Christopher Berner
|                             Sam McCandlish:                Alec Radford                Jllya:Sutskever!                Dario Amodei.
Y;                                                                                OpenAL
Ww                                                                                Abstract
—                                 Recent work has demonstrated ‘substantial. gains-on many NLP'tasks and. benchmarks by /pre-training
=                                 on. alarge corpus of text followed by fine-tuning on a specific task. ‘While typically task-agnostic
—                                     insarchitecture, this method still requires task-specific fine-tuning datasets of thousands or tens of.
W                              thousands of examples: By contrast; humans can:generally perform.a new language task-from.only:
on)                              a few examples or'from simple’ instructions — something, which current-NLP systems still largely
an)                  struggle: to do:. Here: we: show that scaling up language models greatly: improves ‘task-agnostic,
N                                   few-shot: performance, sometimes even reaching competitiveness with prior state-of-the~art ‘fine-.
S                                    tuning:approaches. Specifically, ‘we train GPT-3, an atitoregressive langtiage model with 175 billion
oo                                  parameters; 10x.:more than. any’ previousinon-sparse language’ model,, ‘and test its: performance in’
pe                                   the: few-shot setting: For all tasks; GPT-3 is. applied. without:any: gradient-updates ‘or fine-tuning,
‘                                with tasks and few-shot:demonstrations. specified purely via text interaction with the model, GPT-3:
ws                                    achieves strong performance’ on many NLP datasets, including translation,,question-answering, and
cloze.tasks,.as welljas several tasks that require on-the-fly reasoning or'domain.adaptation; such as:
unscrambling words, using'a novel word.in.a sentence, or performing 3=digit arithmetic. At the same
time;;we alsoiidentify some datasets where GPT-3’s few-shot learning’still struggles, as well asi some
datasets where GPT-3'faces: methodological issues related to training’on large*web:corpora. Finally,
Wwe find that-GPT-3. can, generate samples of ews articles Which huinan evaluators have difficulty
distinguishing fromuarticles written by humans. Weidiscuss brodder’societal impacts of this:finding:
and of GPI-3 in: general.
*Equal contribution                            ,
TJohins: Hopkins University, OpenAL
Authorycontributions:listed.at end of paper:


--------------------------------------------------


--- 第 2 页 ---

Contents.
1 Introduction:                                                                                  3
2 Approach.                                                                                    6
2.2:  Training Dataset Ree FE eg Ree FEE gE Ree eee EE Re ee EE ER ER Ree EE gm    8
23.  Training ProcesS 2.06 eb bai ba ee ha ee bah ee bah GD
QA Eyaluation!.. o.oo ee we ee wm pp ee eee ee eae eee LO
3 Results:                                                                                      10
33 Translation on... 0 Gb ee be ee bam me ne hgh nme ee bia =A!
3.4 Winograd-Style Tasks 2. oe ee we eee LG
3.5 Cortinion: Sense Reasoning © ee ee ee LT
3.9 Synthetic and Qualitative Tasks). 00.0.0. 0. fe we ee eee DL
4 Measuring ‘and Preventing Memorization Of Benchmarks                                                                                             29:
5 Limitations                                                                                  33:
6 Broader Impacts                                                                             34:
6:1.  Misuse:of Language Models, CER as FERRE Ms FERRE EMSS PERE E RM ee ee ee   35
62 Fairness; Bias,.and-Representation ... 2. 6 oe ee ee 36
63 Energy Usage: 2. 6 ee em em wae ee ae BO
7. Related. Work.                                                                                39:
8 Conclusion                                                                                  40:
A. Details of Common Crawl Filtering                                               43
B. Details:of Model. Training                                                                      43
‘C. Details:of Test:Set Contamination:Studies                                                                                                                                   43:
D' Total Compute Used to Train Language Models:                                                        46:
E Human Quality Assessment of Synthetic News Articles                                       46
F Additional Samplesfrom GPT-3                                                          48.
G Details:of Task Phrasing and Specifications:                                                                                      50:
‘H. Results‘on.All Tasks for All-Model Sizes:                                                                                           63:
2)


--------------------------------------------------


--- 第 3 页 ---

1 Introduction
Récent*years havé featured a’trerid.towards :pre-traitied languagerepresentations in. NLP systems, applied inincréasingly
flexible:and task-agnostic:ways.for downstream ‘transfer. First, single-layer representations ‘were learned.using ‘word,
vectors. [MCCD13, PSM14] aiid fed to task-specific. architectures, then RNNs'with multiple layers of representations
and contextual state were*used to form. stronger representations [DL15,,MBXS17, PNZtY.18] Chough still applied to
task-specific architectures), and more recently pre-trained recurrent or transformer language models |[[VSP*17] have:
been. directly fine-tined; entirely removing'the need for'task-specific architectures. [RNSS18, DCLT18; HR18}..
This last: paradigm has' led to ‘substantial-progress' on:many' challenging NLP ‘tasks. such: as ‘reading comprehension,
‘question,answering, textual. entailment, and many: others,, and. has continued to advance; based on new architectures
and algorithitis [RSR+19, LOG*19, YDY +19; LCG*19]. However, a:major limitationto this approach.is that'while.
the architecture is:task-agnostic,-there is still aneed for task-spécific ‘datasets and task-specific fine-tuning: toachieve,
strong;performance on.a desired task typically requires fine-tuning on/a dataset: of thousands to. hundreds of thousands
of examples specific to thatitask. Removing this limitation would be desirable, for several reasons.
First, from’ a practical perspective;, the need ‘for. a large dataset: of labeled examples for‘ every new ‘task: limits ‘the:
applicability of language; models. ‘There ‘exists a, very: wide range: of possible ‘useful language:tasks,. encompassing
anything from correcting grammar, to generating examples.of an abstract concept, to critiquing’a ‘short story, For many
of these tasks:it-is:difficult to.collect:alarge’ supervised training dataset, especially when.the process must.be:repeated.
forevery. new task.
Second, ‘the’potential:to exploit spurious.correlations in training data fundamentally grows with the expressiveness
of ‘the: model. and ‘the narrowness of the! training distribution. ‘This can. create problems: for’ the’ pre-training: plus
fine-tuning paradigm; where:models are designed to'be: large'to absorb information during pre- training; but are then.
fine-tuned on very narrow task: distributions., For instance. [HLW*20] observe that larger models do notnecessarily
generalize better out-of-distribution. There:is evidence’ that:suggests thatithe generalization achieved under this paradigm.
can be*poorbecause: the; model.is ‘overly, specific to ‘the ‘training distribution and does not. generalize; well outside it:
TYdC*19, MPL 19]. Thus, the performance of fine-tuned models on specific benchmarks, even when itis nominally at:
human+level,:;may exaggerate! actual performance on the underlying’task. [GSL* 18, NK19].
Third; humans: do not:require large supervised, datasets to. learn most language: tasks — a, brief directive in natural.
Janguage (e.g. “please’tell:me if this sentence:describes something happy or'something sad’’).or at most-a tiny number
‘of demonstrations (e:g:, “here ‘are’two:examples of:people:acting brave; please give a.third.example of bravery”) is:often.
‘Learning via SGD during unsupervised pre-tiaining;                                                                              :
{           i] SH 8 sia3            8            a | gee <5 goss          9           a | thanks esemerea        8
a) PF eae               g            2, | sakiner=>: snake:         &:            e) Hele > peyyear ) [oe
as     3            SS) Tee 1)            2           a | brid se (bind         Q:         3) mint =>qmen the!       2
‘inneriloop =                                   3:                               =                              3.
\.           4) Be4asy            a            4    fsily 58 “fish!         na          a) wall eesmur           ce
s | Sa Ssa4                             5 | déuk £5 aoe                          5: | otter’ == Toutre
X.           | ORs saz                             & | iit s erp?                       6. | bread. 3 path       |.
Figure 1.1: Language model meta-learning. During unsupervised pre-training, ja language.model develops.a broad.
set of skills:and pattern:recognition abilities. It:then.uses these abilities at inference time’toxrapidly:adapt to. or recognize
ithe desired. task. We tisetheterm “in-conitext learning” ito describe the inner loop:of this process, which occurs within,
the forward-pass upon-each sequence. ‘The sequences in:this diagram are: not:intended to be representative of the data’ a.
model would see during:pre-training; but are intended. to show that there: are'sometimes repeated sub-tasks:embedded,
within:a single sequence.,


--------------------------------------------------


--- 第 4 页 ---

‘Zero-shot:         ‘One-shot                                  Few-shot
_               Nattiral. banguage                              Leer LT)
<                 fou            “een     {|| |             | Ls 138 Params
oe 38 parainis:
oF 2 eee  =            —     a alate a  i      i    |
Nuitiber of Examiplés‘in Context. (K)
Figure 1.2: Larger:‘models make increasingly efficient:use of in-context information:. We show-in-context.leaming
performance on asimple task-vequiring the model to remove random symbols from aword, both’ with and without a,
natural language task description (see Sec. 3.9.2). The'steeper “in-context learning curves” for'large models demonstrate
‘improved ability:to learn.a task:from contextual.information. We'see qualitatively similar behavior across' a:wideirange:
‘of tasks.
sufficient to enable a-humaito perforit afew task to at least areasonable degree of competence, Aside‘from pointing
to.a ‘conceptual limitation in.our current-NLP techniques, this adaptability has practical advantages — itallows humans
to: seamlessly mix.together or’switch between.many: tasks) and skills, for‘example: performing ‘addition:during a lengthy:
dialogues To’ be broadly useful, we wouild someday’like our NLP systems'to have this same fluidity. and generality:
One potential route towards addressing these issuestis meta-learning" — which: in the:context of language models means
‘the model develops a’broad set of skills and ‘pattern:recognition abilities at/training time, and then uses thosevabilities
at inferetice titie-to rapidly adapt to ortecognize the desired task (illustrated in Figure 1.1). Recent work [RWC 19]
attempts to do’this ‘via what we call “in-context learning”, using the'text:input.of a pretrained language model as.a form.
‘of taskispecification: the:model ‘is conditioned on anatural Janguage:instruction and/or a few:demonstrations' of the task:
and is the éxpéected to complete further instances of thé itask simply’ by predicting what coiiies next.
While it:has shown some ‘initial: promise, this:approach still:achieves‘results' far inferior. to fine-tuning — for example:
[RWC*19] achieves only-4% ‘on Natural :Questions,.and even its 55 Fl CoQa-result.is now more:than 35 points behind.
the state-of the art, Meta-leaming:clearly-requires substantial improvement inorder to be viable.as a'practical method of
solving language tasks.                  :                                                               7                                                                         7
Another recent’trend in language: modeling may: offer a way forward. In recent years’ the capacity of ‘transformer
language models has increased. substantially, from. .100 million. parameters. [RNSS$18], to 300 million. parameters
[DCLT18],;to 1.5 billion parameters [RWC*19],.to 8 billion parameters [SPP 19],,11 billion parameters: [RSR*19],
and finally. 17 billion parameters [Tur20]. Each increase:has’brought improvements in.text synthesis and/or downstream.
NLP tasks, arid there is evidence suggesting that log loss,‘which, correlates ‘well with many ‘downstream tasks, follows a
smooth trend/of improvement with. scale, [KMH*20]. Since in-context learning:involvesiabsorbing: many’ skills and.
‘tasks within:the:parameters ofthe model, it is plausible that‘in-context learning abilities might show similarly. strong:
gains with scale.

‘In the context of language modéls'this‘has sometimes,been:called “zero-shot¢ransfer’;. but:this:term is‘potentially ambiguous:
‘the: method.is “‘zero-shot’” in ‘the! sense: that:no: gradient updates are’ performed, but it often. involves providing inference-time:
demonstrations to the:model, so is not:truly learning from zeroexamples. To:avoid this. confusion, we use:the.term “meta-learning”
‘to:Capture the antiet-loop./outer-loop stricture of the general:method, atid the. term: “in contexttlearning” to refer to the inner
loop of: meéta-learning. ‘We-further specialize thé description to “zero=shot”, “one-shot” yor “few-shot” depending on -how-many-
demonstrations:are: provided at inference:time: ‘These terms;are:intended to:remain agnostic on the question of whether themodel,
learns:new‘tasks:from scratch at-inference time!or'simply recognizes patterns seen during training —this'is an,important issue which.
we discuss later in the’paper, but“ meta-learning”.is intended to encompass both possibilities, and simply describes’ the; inner-outer
1OGp striictiiré:.

4


--------------------------------------------------


--- 第 5 页 ---

06                Aggrégate Performance Across Berichmarks
| =e  Few. Shot  |                              |         |                        FE
e+ ‘One Shot
80. || -—-Zero:Shot
 Tf    SSS aa
eZ
3        ae as pF TE,
 ee eee | rer ee
0.1B         048 0:88 1:3B 268 67B 138                              4753;
Parameters:in LM (Billions)
Figure 1.3: Aggregate performance:for‘all 42 accuracy-denominated. benchmarks “While.zero-shot performance.
improves steadily with:model. size, few-shot:performance:increases more:rapidly,.demonstrating that:larger‘models are
thoré-proficient atin-context learning. See Figure 3.8 for dmoresdetailed. analysis on SuperGLUE, a standard NLP:
bétichiiark site.
I this*paper; we' test this hypothesis by training a 175 billion:parameter autoregressive language iiodel, which.weicall.
GPT-3, and measuring its in-context learning abilities.. Specifically, we evaluate GPT-3, on over two dozen:NLP datasets,
as well as several novel tasks designed to-test rapid adaptation to tasks unlikely to be:directly:contaified in the training
‘set, For each task, we evaluate/GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning: where we.
‘allow as many demonstrations ‘as will fit:into the:model’s context window: (typically: 10 to 100), (b) “one-shot:learning”’,
where werallow only one demonstration, and (c) “zéro-shot” learning, whete no:demonstrations are:allowed and only’
‘an instruction.in-natural-language is given.to the:model, ‘GPT-3 could also.in principle be evaluated ‘in the traditional.
fine-tuning’setting;, but:we leave this: to future work.
Figure. 1.2.illustrates the:conditions we study, and. shows few-shot learning of a simple ‘task requiring the model to
Temove extraneous Symbols from a‘word.. Model performance improves with the addition of anatural language task
description,.and :with'the number of examples inthe model’s:context, AC. Rew-shot learning also'improves’ dramatically
with model sizé;, Though the-fesults in this’ casé até particularly striking; the general trends with both: model size.and.
number of‘examplesin-context.hold for:most tasks we.study. We.emphasize that‘ these “learning” cutves involve:no
‘gradient-updates: or‘fine-tuning:, just:increasing numbers ‘of demonstrations given:as conditioning:
Broadly, on.NLP tasks GPT-3 achieves promising’restilts in the zero-shot and one-shot settings, anid in the’ the few-shot
Setting is sometimes competitive with or even! occasionally surpasses state-of-the-art: (despite state-of-the-art being held.
by fine-tuned models). For'example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0. F1 on CoQA in,
‘the Oné=shot setting, 85.0 F1. in'‘the few=shot setting: Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in ‘the.
zero-shot setting, 68.0% in.the one-shot setting,,and.7.1.2% in.the few-shot'setting, the.last of whichis ‘state-of-the-art:
relative to fine-tuned:models: operating in the: same: closed-book:setting.,
GPT-3 also displays one-shot and few-shot proficiency at tasks designed to‘test rapid adaption or on-the-fly reasoning,
which-include‘unscrambling words,, performing arithmetic,. and using; novel ‘words:in ia ‘sentence ‘after seeing them.
defined only once. ‘We also:show’that in:the few-shot:setting; GPT-3 can generatesynthetic news articles which human,
évaluators have-difficulty distinguishing from humnan-generated articles.
At:the same time, wevalso find:some’tasks:on which few-shot performance struggles, even atthe scale;of GPT-3. This
includes natural.language inference: tasks like the ANLI dataset, and some-reading comprehension datasets. like RACE:
‘or QuAC..By presenting.a broad characterization of GPT-3’s strengths and: weaknesses, including these:limitations, we
hope’ to stimulate:study of:few-shot learning in language:models and:draw. attention:to. where progress issmost needed.
A heiitistic sense Of the overall results can be seen in Figure 1.3, which aggregates the various tasks:(though it should.
not.bevseen as.a. rigorous ormeaningful-benchmark:in itself).


--------------------------------------------------


--- 第 6 页 ---

‘We also‘undertake’ a:systematic study of “data contamination”—ia growing problem. when‘training high capacity ‘models
on datasets such, as Common, Crawl, which can, potentially include content from test datasets simply because such,
content often exists on the web, In this paper‘we develop systematic tools'to measure data.contamination and quantify
iitsdistorting‘effects. Although we:find that data contamination‘has a:minimal effect on’GPT-3’s performance‘on most:
datasets; we do identify a:few:datasets:where:it could be inflating-results, and we either:do not report:results; on these.
datasets or we note them withan-asterisk, depending on'the severity,

In addition to allithe above, we also: trainca series of: smaller models (ranging from 125, million parameters to 13 billion.
paratietérs) in order to compare their performarice to GPT-3 in the zero; ohe-anid few-shot settings. Broadly, for most
tasks ‘we find.rélatively smooth sealing with model.capacity in all three settings; one notable pattern is thatthe gap
between:zero-,,one-, and few-shot performance often grows with:model capacity, perhaps suggesting ‘that Jarger:models
are hore proficient meta-learners.

Finally, giventhe.broad. spectrum.of capabilities displayed. by GPT-3,we discuss concerns about bias, fairness, and.
broader‘societal.impacts, and attempt a'preliminary analysis of GPT-3’s characteristics in,this regard.

The:remainder of this-paper is organized as follows. In:Section:2, we describe our approach: and methods :for training
(GPT-3:and:evaluating it. Section 3 presents results on’the-full range of tasks:in the zero-, one- and few-shotisettings.
‘Section.4 addresses. questions of data:contamination (train-test,overlap)., Section 5 discusses limitations. of GPT-3,
Section 6 discusses broader impacts. Section 7 reviews related work and Section 8:concludes,

2 Approach

Our basic'pre-training approach, including model, data; and training, is similar to therprocess:described.in [RWC*19],
with relatively straightforward scalingup of the:model sizé,.dataset'size and diversity, andJength of training. Our use.
of in-context learning:is ‘also similar to. [RWC*'19], but in-this:work we systematically explore different:settings for
learning: within the;context. Therefore, we start this section iby explicitly defining and contrasting thedifferent:settings
that'we' will be evaluating GPT-3 on.or could in principle evaluate:GPT-3 on. These Settings can be seen:as lying on a,
‘spectrum of how‘much task-specific.data they tend to'rély on. Specifically, we canidentify. atleast four points on-this
‘spectrum (see Figure 2.1, for-an.illustration):

* Fine-Tuning (FT) has been the most common approach in tecent'years, and involves ‘updating the weights of
a‘pre-trained model by*training on @ supervised dataset specific to the desired. task.. Typically thousands to
hundreds of thousands of labeled examples are’used.. The main:advantage'of fine-tuning:is ‘strong performance
ofl iany benchmarks. The main disadvantages are the need fora new large dataset-for'every task, the potential,
for poor ‘generalization. out-of-disttibution. [MPL19], and ‘the. potential. to. exploit ‘spurious’ features of ‘the.
traming data [GSL*18, NK19], potentially resulting in,an unfair comparison with human:performance. In.
this work we'do not-fine-tuiie:GPT-3 because, our focus 1s Oni ‘tasksagnostic- performance; but GPT-3 can be.
fine-tuned.inprinciple.and this is'a.promising: direction:for futureswork.

‘e: Kew-Shot (ES) ‘is ‘theternx we-will use: in:this: work to refer’to the ‘setting where ‘the model ‘is ‘given‘a few
deimoiistiations:0f the task atinference:time as Conditionitig [RWET19], but no weight updates.are allowed.
As shown in Figure:2.1,-for atypical dataset:an. example has-a context-and a desired completion (for example
an English sentence:and.the French translation), and few-shot works by: giving .K examples of:context:and,
completion; and then-one:final example-of context; with the:model expected to:proyide:the:completion:, We-
typically’set- in the range of 10'to 100 as this is how many examples.can fit in the:model’s context window
(Nese = 2048). ‘The main.advantages of:few-shot are:a.major:reduction:in the’need for task-specifie:data:and.
reduced potential to learn.an overly narrow distribution:from a:large but:narrow’fine-tuning dataset. The main,
disadvantage is that results:from this method have so far been much worse than, state-of-the-art-fine-tuned.
models: Also, ‘a small amount of task ‘specific data. is: still required.. As: indicated. by the name, :‘few-shot:
learning as‘described here for‘language models’ is related:to few-shot learning as' used in‘other, contexts’ in.
ML [HYC01, VBL* 16] —both involve learning based ona broad distribution of tasks (in this! ase implicit in,
the'pre-training’data) and then rapidly, adapting to.a-new task.                                                     -

‘e| One-Shot:(18):is the same: as few-shotzexcept that only:one demonstration is; allowed, in addition to a:natural.
lahgtiage description Of the'task, as shown in Figure 1. Theseason to distinguish oie-shot from few-shot-and,
zero-shot (below) is that it-most closély:matches the: way: in 'which.some'tasks are communicated to.humans.
For;example, when asking: humans to generate a:dataset.on:a human’worker‘service (forexample Mechanical.
Turk), it is. common to give-oné demonstration, of the task. By contrastit is sometimes diffiicult'te communicate
the.content or format of-a task if no examples.are given.


--------------------------------------------------


--- 第 7 页 ---

‘THE three:-SSttirngs We explore for im-context learriing’               TiAdItIOnALfinS-tuinivg (AGL Used Tor.GPT:3)
‘Zeto-shiot!                                                                         Finé-taning
‘The'model ‘predicts the answer. given only a natural language;                    The model is:trained Via:repeated gradient:updates using a
j) ThenSlate engldsh.t6 Fiench? —-]~taskdeseription              [2] sca otters ioutrederner ——} ser #
2) iSéai otter So: dou re de  mer         al. example                       5                            ¥
a | scheese: =:                               a ‘bioribe                     [|  plush giraffe == girafer peluche: i {—vexampie.#n!
‘Few:shot
:examples’of:the task: No‘gradient- updates:are’performed..
2 | [séai otter St Lotitre ide mer        al  examples:
3 | ‘peppermint :=» imenthe} poivrée:      i]
4   ‘plush girafe: =p girate, peluche   a
5.) wheéS@) =P cnc causes   an prompt!
Figuré 2:1: Zero-shot, one-shot and few-shot, contrasted, with traditional finé-taning., The panels above show
fourmethods for performing a-task witha language model ~ fine-tuning is'the traditional method, wheteas zero-, ones,
and few-shot,. which we study’ in:this: work, require the model. to :perform the task with only: forward passes sat test:
time. We typically present the: model with a:few:dozen:examples:in the few: shot setting., Exact phrasings:for all task
descriptions, éxaniples and prompts can, be found in, Appendix G.                                                     .

‘« Zero-Shot: (0S)'is the same‘as one-shot:except that no demonstrations.are allowed; and the model is only given.
anatural language instruction describing the task. ‘This method provides maximum convenience, potential for
robustness, ‘and avoidance.of spurious correlations: (unless’they: occur’very broadly. across.the large:corpus' of:
pre-training data), but:is also the:most challenging setting. Tn some ‘casesit:may even’be difficult:for. humans
to-understand the fornat of the task without prior éxamplés, so this settiig-is-in some Cases “uiifairly hard”.
Forexample;, if'someorie.is asked ‘to “make a table of. world records forthe 200m.:dash”, this request:can be.
ambiguous, as it may ‘not.besclear. exactly what-formatithe tablesshould.haveior whatishould /beiineluded (and,
even with careful Clarification; understanding precisely whatis desiréd-can be difficult), Nevertheless, forat!
least: some’ settings. zéro-shot is.closest'to how humans perform tasks —forexample, in:the translation example
in Figure 2.1, a human ‘would :likely know. what;to do:from just:the text.instruction.

Figure.2:1 shows ‘the four methods ‘using-the example of translating English to French: In this paper we: focus. on,
zero-shot, one-shot and few-shot, with the aim. of comparing them not as competing alternatives, ‘but as’ different
problem settings which.offer. a’varying ‘trade-off between’performance<on specific benchmarks;andisamplevefficiency.
‘We especially'highlight’the few-shot results.as many-of them: are only slightly behind state-ofthe-art fine-tuned models;
Ultimately,-however; one-shot, oreven sometimes zero-shot,'seemlike the fairest-comparisons'to human performance,
and are.important targets for:future work.                                                                                       :                                    ,

Sections 2,1+2.3 below give details on our models; training data; atid trainiig process respectively, Séction:2.4 discusses
the details of how'we do:few-shot, one-shot, and zero-shot evaluations.

7


--------------------------------------------------


--- 第 8 页 ---

Model Naimeé                        params     Nayers     ‘model:    ‘heads     head’     Batch Size    Learning ‘Rate.

GPT:3 Small                          125M —s12          168          12          64            0:5M.           6:0:x 10

GPT-3 Medium                      350M.          24          ‘1024          16           64           ‘0:5M.           3:0'x 10-4

GPT:3 Large’                          760M.          24          ‘1536          16          96           ‘0:5M.           2:5 x 10-4

GPT-3 XL,                             L.3B           24         2048,         24         128            IM            2:0'x 104

GPI:3 2.7B                             2.7B           32-2560 3280              IM              L:6-x 10-4

GPT-3 6.7B         6.7B    32 «4096 32 128    2M.    1:2 x 10-4

GPT-3'175B or“GPT-3” 175.0B 96 12288 96 128 32M           0:6. 10~#
Table:2.1: Sizes, architectures, and learning hyper-parameters.(batch size.in tokens and learning:rate) of the models
which we'trained. .All:models were:trained fora total‘of 300 billion tokens.                                :
2.1 Model.and Architectures
‘We use the same:modell and architecture as;GPT-2 [RWC*19], including the:modified initialization,:pre-normalization,
and reversible tokenization, described therein, with the exception that we use altemating dense and locally banded sparse
attention patterns 'in'the layers ofthe transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence
of ML:performanceon model size; wetrain.8 different sizes of model,:ranging:over three orders of magnitude from. 125
million parameters'to 175 billion-parameters,, with the last/being the miodel we-call GPT-3. Previous work [KMH*20]
suggests, that with enough training data, scaling of ‘validation-loss should.be approximately’a smooth power law asia.
functionsof size; training models of many different:sizes allows us to test this hypothesis both:for, validation loss,and ‘for
downistream language tasks,
Table 2.1 shows theisizes' and.architectures/of our'8 models. 'Here:nigazarig-is the total.number‘of trainable:parameters;,
Misyerg is the ‘total number ‘of layers; dioaet' ‘is the- number: of units’ in, each ‘bottleneck layer’ (we always: have ‘the:
feedforward layer four times’ the: size of the bottleneck layer, dg = 4 * Cmodet)s atid dneaa is the dimension of €ach,
attention head.. All models'use a:context: window of 7g, = 2048 tokens. We partition the:model across:GPUs along
both ‘the depth and width dimension: in, order: to ‘minimize data-transfer between nodes. The: precise architectural,
parameters for éach model are chosen baséd on Computational ¢fficienicy and load-balancing:in the layout of models
actoss' GPU’s...Previous ‘work: [KMH* 20] suggests:that validation-loss is notstrongly sensitive to ‘these parameters
within)a.reasonably: broad range.
2.2 Training Dataset
Datasets for‘language models have-rapidly expanded,:culminating in,the';Common;Craw] dataset? [RSR*19] constituting
nearly:a-trillion words. This sizeof dataset is sufficient-to train.our largest models without ever updating on'the same.
‘sequence twice. However, we have found that unfiltered. or ‘lightly filtered versions of Common Crawl tend to have:
Jower:quality-than more curated datasets., Therefore; ‘we took: 3: steps:to improve the: ayerage-quality, of our datasets:
()'we downloaded and filtered:a version of CommonCrawl based on, similarity to-a range-of high-quality reference.
corpora, (2)'we performed.fuzzy deduplication at the:document level, within‘and across datasets; to prevent:redundancy:
and preserve‘the integrity of our‘held-out. validation, set as:an accurate:measure‘of overfitting;. and (3) wejalso:added.,
known high-quality reference corpora tothe training mix’ to aigment CommonCrawl and increase:its diversity.
Details:of the first‘two; points (processing; of Common: Crawl) are described in. Appendix. A. For‘the third, weadded.
several curated high-quality datasets, including ‘an expanded version, of the: WebText dataset |[RWCt19].,, collected
‘by scraping links over alonger’period of time, and first desctibed:in [KMH*20], two internet-based books corpora.
(Books1 and Books2) and English-language Wikipedia.                                                                                      -
Table 2.2, shows'the:final tmixtire-of datasets that we Used in training. The CominionCrawl data was downloaded from,
Al shards of monthly:CommonCraw] covering 2016'to 2019,, constituting 45TB: of compressed ‘plaintext before filtering
and 570GB, after filtering; roughly equivalent to 400’billion.bytepair-encoded ‘tokens. Notetthat:during:training, datasets
até not sampled.in proportion to'their size, but rather datasets we*view as higher-quality are samipled.more frequently,
‘such.that’CommonCrawl.and-Books2 datasets are sampled less than once during training, butthe other datasets are.
sampled 2-3 times: This’ essentially. accepts:a small amount:of overfitting in exchange for higher quality'training data.

*hittps://comioncrawl. org/thé-data/
8


--------------------------------------------------


--- 第 9 页 ---

Total'Compute Used During Training
4 00(0 ——______——————————
© (ggg,
ES
=
D.
2.
o                                                  ;
eS     1.00;                                                                                        f
Ss
a                                                                                     —
=
ol oH   LI LU |
a eo  Pa  we se ra  ee  ee ee ; se s  ee or  a  ee  3 Ral
&  &  &  &   ow &              & ge  x   SE EE E  &
Sak                                      g
Figure 2.2: Total:compute‘used during training. Based on the:analysis in-Scaling-Laws For.Neural. Language. Models
[IKMH*.20] we train much larger models onmany fewer tokens:than is'typical. Asa consequence, although GPT-3 3B
‘is almost: 10x:larger than-RoBERTs-Large (355M. params), both: models took roughly:50 petaflop/s-days of compute,
during pre-training. Methodology: for these calculations can’be found.in-Appendix.D:,
Quantity ‘Weightin        Epochs €lapsed when.
Dataset                                          (tokens) ‘training mix: ‘training for'300B tokens’
Coninion Crawl (filtered) 410 billion       60%:                 0.44:
WebText2.                   19°billion.        22%:                  2:9:
Books2                      55'billion,        8%                  0,43:
Wikipedia           3 billion     3%           3.4
Table'2:2: Datasets:used to'train GPT-3, “Weight:in training'mix” refers to:the fraction'of examples during training:
‘that are drawn ‘from a given dataset, which Wedintentionally'do not maké-proportional to thé sizé-of the datasét. As a:
result, when we train for 300 billion’ tokens, some datasets are seen-up to 3.4 times during training while other datasets.
are seen.less:than: once.                                                                    :                              ;            -
A tiajor methodological concern with language models pretrained:on a broad. swath of intermet data, particularly large
models with the capacity to:ieniorize ‘vast amounits.of content:, is: potential. contamination. of downstream tasks by
having ‘their test or.development sets inadvertently seen during:pre-training. ‘To reduce such.contamination, we searched
for and.attempted to remove any overlaps with the development and tést sets of all benchmarks studied in this paper.
Unfortunately, a:bug in the filtering caused'us toignore some overlaps, and due'to the cost of training it was not-feasible
to retrain the model.. In Section 4 we characterize the impact:of the remaining overlaps, and.in future work:we will.
more aggressively remove data contamination:
2.3 Training Process:
As found in [KMH*+20, MKAT18], larger models:can typicallyuse a‘larger batch’ size, ‘but:require'a smaller learning:
fate. We measure the: gradient'noise scale during training atid Use it to guide Our choice:of batch size [MKAT18].. Table
2.1. shows the*parameter settings :werused.. To-train the larger models ‘without tinning out-of memory, weuseta mixture,
of model parallelism within:each. matrix multiply and model parallelism across the layers; of'the network:. All models
Weie ‘trained on V100 GPU's on paitiof a-high-bandwidth cluster provided. by. Microsoft. Details of the-training process
‘and hyperparameter settings are described in Appendix B.


--------------------------------------------------


--- 第 10 页 ---

2.4 Evaluation.

For few-shot learning;. weevaluate each ‘example; in the evaluation set’by randomly drawing KC examples from that:
task’s training’setias conditioning, delimited by’ 1 or 2newlines depending’on the:task. For LAMBADA;and Storycloze
there is'no supervised training setavailable so we.draw conditioning‘examples fromthe development set-and evaluate.
onthe test set:, For Winograd (the original, not SuperGLUE version)'there is only. one dataset; so-we draw conditioning
examples directly fromat,                         .

K’can beiany value from:0:to the maximum: amount allowed bythe model’s ‘context:window;: which is nage =: 2048)
for all models and typically-fits 10 to 100 examples:, Larger values of are usually but not always better, so when a.
‘separate development aiid test set are available, weexpetiment with afew values.of Jc on the development set-and then,
run.the best:value‘on the:test set. For some’tasks (see.Appendix G)'we also use a natural. language prompt in addition to
(or for AK =:0;.instead of) demonstrations.

On tasks that involve choosing: one correct completion from. several options (multiple choice), we provide K examples.
of ‘context:plus' correct completion, followed. by: one example: of. context only, and. compare ‘the: LM likelihood of
each.completion:, For:most tasks we:compare the:per-token likelihood (to: normalize for length); however‘on a'small.
number'of datasets (ARC, OpenBookQA,,and-RACE) we: gain additional benefit'as measured on the development set:
by:normalizing bythe unconditional-probability:of each completion, by computing: Oo ‘where
answer_contextiisithe string"Answer: "or"A: "and is used to prompt that the:completion/should’be ananswer
but is otherwise-generic:

On tasks that involve binary classification, we give:the options more semantically meaningful names (e.9.. “True” or’
“Halse” rather ‘than,0 or. 1) and then treat:the task like multiple:choice; ‘we: also sometimes frame:the task:similar, to what:
is done by [RSR*19] (see Appendix G) for details.

On tasks with.free-form completion; we:use’beam.search with’the same:parametersias [RSR* 19]: a beam:width of 4
and a length penalty of.~ — 0.6. We:score’the model using F1 similarity: score; BLEU; or exact:match,, depending on.
whatis standard forthe dataset at hand..

Final results are:reported:on thetest set when:publicly available, for:each:model:size and: learning setting (zero-, one-,
and few-shot), When.the test:set:is private; our'model:is often too large:to fitron the test server, so we report results on,
the development set, We do submit'to the test server on’ a small-number of datasets (SuperGLUE, TriviaQA, PiQa)
where we-were.able'to make:submission work, and we submitionly the 200B few-shot results, and report:development
set'results for:everything else:

3 Results

In Figure. 3.1' we display training curves for the 8 models described. in Section 2. For this graph we also include 6
additional extra-small models with as few. as 00,000. parameters.. As obseryed:in [KMH+,20], language modeling
performance follows :a power-law when.making ‘efficient use:of training compute:, Afterextending‘this trend by two
more orders of magnitude, we-observe' only a slight(if.any) departure:from the power-law. One'might worry that these.
improvements:in cross-entropy loss come only’from:modeling spurious'details of our training: corpus. However, we will.
seein the:following sections that improvements in.cross-entropy: loss lead ‘to consistent:performance:gains across’ a,
‘broad spectrum of natural language tasks.                                                                                      .

Below, ‘we evaluate: the 8: models: deseribed.in: Section 2 (the ‘175 billton;parameter parameter, GPT-3 ‘and.7 smaller’
models) on a Widevangée-of datasets. We group the datasets inito 9:categories representing roughly similar tasks.

In Section. 3:1 ‘we evaluate on traditional language: modeling ‘tasks ‘and. tasks'that are’ similar to; language modeling,
such.as Cloze'tasks and :sentence/paragraph completion tasks. In‘Section.3.2 we evaluateon “closed book” question.
answering tasks: tasks ‘which require using the: information, stored. in, the Mmodel’s' parameters to answer general.
knowledge questions. In/Section 3.3'weievaluate'the model’s ability to translate:between languages (especially:one-shot:
and few-shot). Tn Section 3:4-we evaluate the:model’s ‘performance:on ‘Winograd Schema-like'tasks.. In;Section.3.5 we
évaluate-on dataséts that involve commonsenise-redsoning or qiestion answering: In Section 3.6 we evaluate on-reading
‘comprehension tasks, in-Section 3:7'we evaluate’ on.the SuperGLUE benchmark suite, ‘and ‘in’ 3.8'we briefly explore.
NLI. Finally, in, Section 3.9, we invent some additional tasks:designed especially*to, probe:.in-context leaming abilities —
thésé tasks focus on.on-the-fly reasoniiig, adaptation skills, of operi-endéd text.synithesis. ‘We evaluate all'tasks in the.

10


--------------------------------------------------


--- 第 11 页 ---

ay                       to
PSRs                                           40
B            OQ  (                                  [40°
§                   aA                                YB
£ 3                       la ih                        |  |.  8
S                                 a.                            a
ANA    -
2                               * mM,        <6!
wee = 9.57 =Gp0.048               Sal.
‘1:5 "Te        a         410°
10° 910° = 10s 40"       10 10°
CGompute;(PetaFLOP/s-days)
Figure 3.1: Smooth. scaling of performance with compute. Performance’ (measured in terms. of cross-entropy
validation: loss), follows ‘a power-law trend. with the ‘amount of:compute used. for training. The! power-law’ behavior’
observed in, [KMH*20] continues for:an additionall two: orders of magnitude: with; only small deviations from ‘the:
predicted curve. For this-figure, we exclude embedding parameters from’ compute and parameter counts.
_ Setting              PTB
GPT-3 Zero-Shot 20:5 _
Table.3:1: Zero-shot results on PTB language modeling dataset.. Many-other:common language modeling datasets
‘are omitted. because they are.derived:from. Wikipedia or-other sources ‘which: are:included in GPT-3's training. data,
3.1 Language Modeling, Cloze, and/Completion Tasks:
In ‘this section we; test GPT-3’s performance on ‘the traditional task: of language:modeling, as well as related ‘tasks
that involve predicting a single word of interest, completing:a sentence, or paragraph, or choosing between possible.
‘completions of.a.piece:of text.
3.1.1 Language Modeling:
We calctilate.zero-shot perplexity onthe Penn Tree Bank (PTB) [MKM94] dataset measured in [RWCT19]. Weomit
the 4 Wikipedia-related tasks in.that:work because’they’are entirely contained in-our training.data, and/we.also omit the.
one=billion word benchmark:due to.a high:fraction of the dataset being contained in our training;set:. PTB: escapes:these-
isstiés die'to predating the modern ‘intemet. Our'largest model.sets-a new SOTA on, PTB bya. substantial margin of 15
‘points, achieving:a.perplexity of 20.50. Note:that’sincé.PTB is'a.traditionall language:modeling dataset it does not-have.
'a clear*separation of:examples to:define:one-shot or-few-shot evaluation around, ‘so’we:measure ‘only: zero-shot:,
34.2 LAMBADA.
The LAMBADA. dataset [PKL* 16] ‘tests: the: modeling ‘of long-range’ dependencies’ in’text — the model is ‘asked. to
predict the:last word of sentences which require reading a.paragraph of context. It has recently. been suggested that; the
continued scaling-of language models is’yielding diminishing-returns on’this difficult benchmark. [BHT*20] reflect on.
‘the’ small 1.5% improvement achieved by a doubling of model/size' between two.recentstate of the’ artresults ([SPP*19]
11.


--------------------------------------------------


--- 第 12 页 ---

LAMBADA LAMBADA. StoryCloze -HellaSwag

Setting         (ace):     (ppl)     (ace)     (ace)

GPT-3' One-Shot               72.5                      3.35                     84.7                   78.1,

GPT-3-Few-Shot            86:4                  1.92                 87.7               19.3.
Table 3:2: Performance‘on cloze‘and completion:tasks.. GPT-3 significantly improves:SOTA on LAMBADA- while:
achieving respectable performance on two difficult completion prediction datasets. *[Tur20] 9[RWC*19] “[LDL19]
41LCHt20]

Lambada                                                    ;
-Human |
20:                   |
80!                   4     a
* 50\_    La
30:                                                                   == One-Shot 7
20'[    ——    —    =   T             —_
O4B           04B (0.88 1.38 2.68         67Bi 43B                                     475B:
Parameters:iniLM:(Billions).

Figure 3.2: On. LAMBADA, the few-shot-capability. of language models results in/a strong boost toaccuracy, GPT-3
2.7B ‘outperforms the SOTA, 17B parameter, Turing-NLG [Tur20] in this setting;,and GPT-3 1,75B.advances;the state of
ithe art by 18%. Note zero-shot uses a,different format'from one-shot and few-shotas described in the text.
and [Tur20]) and argue that'‘continuing to: expand:hardware’ and’ data sizes by: orders. of‘magnitude :is not the: path.
forward”. We find that path is still promising and, in a.zero-shot setting GPT-3 achieves 76% on LAMBADA, a.gain of
‘8% over the previous state of the: art.
LAMBADA is:also a:demonstration of the flexibility: of few-shot'learning:as it providesia way: to addressia problem.that:
classically occurs ‘with this dataset, Although the completion in LAMBADA is:always ‘the last word in.a sentence, a.
‘standard language‘model.has no‘way ‘of:knowing this detail. It thus assigns probability not:onlyto the correct ending but:
filters [RWC* 19] (which. ban “contiiuation” words). The few-shot setting. instedd allows us to “frame” the task as a,
‘clozé-testiand allows thelanguage iiodel:to infer froniexamiples that a completionof exactly one word is:desired. We
use’the following: fill-in-the-blank:format:

Alice was.friends with Bob, Alice went to Visitherfriend. _—, + Bob

‘George bought some baseballequipment; a.ball, a:glove, ‘and a:             os
Wheli:presentéd with exaitiples formatted ‘this way, GPT-3-achieves 86.4% accuracy inthe few-shot setting, an increase.
‘of over 18% fromthe. previousistate-of-the-art. Weobserve that.few-shot:performanice.improves stronglywith miodel.
size:, While'this setting decreases’ the performance ofthe smallest model’ by almost 20%, for'GPT=3 it:improves accuracy
by 10%. Finally, the fill-in-blank method isnot effective one-shot, where italways perforiis worse that the Zero-shot’
setting. Perhaps this is because all models still require several examples to recognize-the pattern,


--------------------------------------------------


--- 第 13 页 ---

Setting                                                                              NaturalQs’ WebQS ‘TiiviaQA

‘T5-11B+SSM_(Fine-tuned; Closed-Book):[RRS20] 36:6              44.7           60.5

‘T5-11B (Fine-tuned, Closed-Book)                               34.5:               374          501

‘GPT-3 Zero-Shot                                                                             14.6,                   144             64:3

‘GPT-3 ‘One-Shot                                                              23.0              25.3 68.0

‘GPT-3 Few-Shot                                                                    29.9                 ALS            71.2
Table:3:3: Results on three Open-Domain QA tasks. GPT-3:.is shown in the few-, one-, anid.zero=shot settings,.as
compared to prior SOTA results.for:closed book:and open domain settings., TriviaQA few-shot-resultis evaluated.on the
wiki, split testiserver:
One note of caution is' that an analysis:of test: set:contamination identified that.a significant minority: of the LAMBADA
dataset appears; to be :present.in:our: training data —-however’analysis performed in Section4 suggests:negligible impact:
on-performance.,
3.1.3, HellaSwag
The 'HellaSwag dataset: [ZHB*19] involves picking the best ending ‘to:a story:or set of instructions. Theexamples were
Adversarially mined to be diffictilt for language models while remaining easy for humaris: (who achieve 95.6% accuracy).
GPT-3 achieves 78.1%:accuracy inthe one-shot setting ‘and 79:3% accuracy. in'the few-shotsetting;,outperforming the
75.4% accuracy’ of a fine-tuned ‘1.5B parameter language model [ZHR*19] butistill:a fairamount lowerithan;the overall.
SOTA of 85.6% achieved by the fine-ttined multi-task model. ALUM.
3.14 StoryCloze
We next evaluate GPT-3 on the StoryCloze:-2016 dataset [MCH16], which involves selecting the correct.ending
sentence for'five-sentence long stories. Here GPT-3 achieves 83.2% inthe zero-shot setting and 87.7% in the few-shot
Setting: with.’ = 70). Thistisi still 4.1.% Jower: than.the fine-tuned SOTA-using:a. BERT based:model [LDL19] but:
improves’ over previous zero-shot:results: by, roughly 10%..
3.2 Closed Book Question.Answering:
In this Section we:measuré GPT-3’s ability to.answer questions about broad factual: knowledge. Die to the immense.
amoutitof possible queries, this’task:has normally been-approached by“using aninformation retrieval'system:to find.
releyant:text in-combination. witha model which.learns: to ‘generate; an answer given the: question ‘and.the ‘retrieved.
text, Sincethis setting allows a system to search for.and condition on text Which potentially contains the: answer it
is denoted “open-book”, [RRS20] recently demonstrated that alarge language model can perform:surprisingly well.
directly answering the questions:without conditioning:on ‘auxilliary information..’They denote-this.more restrictive
evaluation setting as “closed-book”.. Their-work:suggests that even. higher-capacity models:could perform:even better
and we test.this hypothesis with GPT-3. We evaluate GPT-3 on'the 3:datasets in [RRS20]: Natural Questions [KPR*19],
WebQuestions, [BCFL13],.and'TriviaQA [JCWZ17], using the: same’ splits. Note that in addition.to all results being in.
‘the closed-book:setting; our use of: few-shot, one-shot, :and zero-shot evaluations represent:aneven, stricter setting than,
previous closed-book QA. work: in-addition.to.external content not being allowed, fine-tuning on-the Q&A.dataset itself
is also:not:permitted.                                                                                                                                 /
Thertesultsfor GPT-3 até shown-iii Table 3.3, On TriviaQA, we achieve 64.3% in thé zero-shot setting, 68.0% inthe.
‘one-shot setting, and 71.2%.in the:few-shot setting. Theizero-shot resultalready outperforms the fine-tuned T5-11B by
14:2%, and also:outperforms a version’with Q&A tailored span prediction :during’pre-training by 3.8%. The one-shot:
resultimproves by 3.7% and:matches the SOTA ‘for an opén-domain QA. system which not only fine-tunes butalso
makes use of.a.learned retrieval mechanism over'a 15.3B parameter dense vector‘index of 21M docunients [LPP* 20].
‘GPT-3"s) few-shot result further improves'performance another 3.2% beyond this:
On WebQuestions (WebQs),'GPT-3 achieves. 14.4% in, the zero-shot setting, 25.3% in thé one-shot setting, and 41.5%
‘in the few:shot setting. "This compares to 37.4% for fine-tuned T5-11B, and, 44:7.% for fine-tuned ‘F5-11B+SSM,
which-uses a‘ Q&A-specific:pre-training procedure. ‘GPT-3: in. the: few-shot setting: approaches the performance of
state-of-the-art fine-tuned models. Notably, compared to TriviaQA;. WebQS shows:a mitch larger gain from zero-shot to
few-shot (and indeed its zero-shot-and ‘one-shot performance are poor), perhaps suggesting that the WebQs questions


--------------------------------------------------


--- 第 14 页 ---

TriviaQA’
70: —Finestunea sora} | |_| ______
50:                                                          3 ‘pe SH a  4#——_—_—
.                                     s aes    ,       .  a“
20,                  DD  —
10. >So                                                   l—e—: One-Shot            -
OB:          0.4B: 0.88 1:3B (268        6:7Bi 3B                                 475B:
Parameters ini_M (Billions),
Figure 3.3: On-TriviaQA GPT3’s performance grows: smoothly with:model size, suggesting, that language:models
continue'to absorb knowledge as their:eapacity, increases: One-shot and:few-shot performance:make significant gains
over zero-shot behavior, matching-and.exceeding’ the performance of the SOTA fine-tuned open-domain model, RAG:
[LPP*20],                                   .                      a                                                         :
and/or the’ style of:their.answers are) out-of-distribution:for GPT-3. Nevertheless, \GPT-3 appears able to‘adapt to ‘this
distribution;recovering strong:performance in the few-shot setting:.
On Natural Questions (NQs)'GPT-3 achieves 14.6% in the:zero-shot setting, 23.0% in the:one-shotsetting, and 29.9% in.
the few-shot setting, compared to, 36.6% for'fine- tuned TS: 11B+SSM. Similar to'WebQS, the large gain from zero-shot:
to few-shot may suggest a,distribution shift, and may also explain ‘the: less competitive: performance compared to
TriviaQA. and WebQS. In particular; ‘the. questions in-NQs'tend towards ‘very fine-grained knowledge:on Wikipedia.
specifically which.could be testing:the limits of GPT-3’s capacity: and broad: pretraining’ distribution. ~                    ~
Overall, oi One Of the thrée dataséts:GPT-3’s one-shot matches the open-domain fine-tuning SOTA, Onthe other two:
datasets itsapproaches the performance ofthe closed-book SOTA.despite not using:fine-tuning. ‘On all 3:datasets;. we.
find:that:performance scales:very smoothly with model size (Figure:3.3 and, Appendix:H Figure H.7), possibly reflecting
ithe idea that model capacity’ translates directly to more “knowledge’ absorbed.in the:parameters-of the model.
3.3 Translation
For GPI-2 a-filter was:used on a:multilingual- collection of documents'to:produce:an English only dataset:due to:capacity:
concerns. Even. with this:filtering GPT-2 showed some evidence:.of multilingual.capability and performed non-trivially
when.translating between.French,and English despiteionly training on.10:megabytes:of remaining French text. Since we
inerease‘the capacity’ by over'two:orders‘of magnitude from:GPT-2'to GPT-3, we also expand the scope ofthe training:
dataset-to include more representation of other languages, though this remains an.area for further improvement, As
discussed in 2.2'the majority of ‘our data‘is derived.from:raw Common.Craw1 with only quality-based filtering. Although.
GPT-3’s training data is, still primarily English (93% by’ word count); italso includes 7% of text:in other languages:
These languages are-documented in.the supplemenital material. In,order to better understand translation capability, we.
‘also expand our‘analysis:to.include-two ‘additional:commonly studied languages, ‘German and Romanian.
Existing Uiispervised: machine translation approaches often combine pretraining on a-pair'of monolingual datasets
with backtranslation. [SHB15] to bridge’ the’ two Janguages in. a.controlled way. .By. contrast, GPT-3 Jearns' from. a.
blend-of training: data that:mixes:many languages: together in ia natural. way,, combining them:on'a word, sentence,
and document level: GPT-3.alsouses a:single-training objective which isnot‘customized or designed for‘any task in,
particular. However, our one./ few-shot settings aren't strictly comparable to prior unsupervised work since they make
‘usesof a: small amount:of paired ‘examples (1 ‘or 64). ‘This corresponds to‘up to # page or two-of in-context training data,
Resiilts aré shown in Table: 3.4. Zero-shot GPT-3, which-only receives ona fatiiral language:deseription. of the task,
still: underperforms recent. unsupervised: NMT results. However, providing onlyia single example.demonstration for
14


--------------------------------------------------


--- 第 15 页 ---

Setting               En 3Fr FroEn EnDe De>SEn EnRo Ro En
—SOTA.(Supervised) 45.67 35.0% = 41.2" 40.24 385 399%
GPE3 Zeto-Shot            25.2 212 246 272            14.1             19:9
GPT-3 One-Shot:               28.3             337             26.2               30.4              20:6              38.6:
GPI:3 Few-Shot:       32:6 392 = 29.7      40.6      211.0      39:5
Table3.4: Few-shot GPT-3 outperforms’ previous unsupervised. NUT work: by’ 5 BLEU when translating;
WMT’l6 DecsEn, and WMT’16 Ro<Eni datasets as measured by wulti-bleu.perl, with XLM’s tokeniza-
tion’ inorder to;.compare most closely ‘with prior unsupervised NMT ‘work, SacreBLEU! [Pos18] results re-
ported in Appendix H.. Underline indicates’ an, unsupervised or few-shot SOTA; bold. indicates supervised. SOTA,
with relative confidence, “[EOAG18] ?[DHKHI4] “[WXH*18] “[oR16] *[LGG*20]./[SacteBLEU signature:
BLEU+éase mixed+numrefs.1+smoothexp-+tok:intl+version.1.2:20]
Translation (Multi-BLEU)
Le = es  =~    7
                       Lae                     es
_ OCG ,        eee oe eee nee —_
Wn,       TI       one    Ort                        Lowe ie
ae | ff pe at       . es ai —e—: French -> English |
hee ae   |       yen               =+e-' English -> French
10:|-f/, a ae -_                           +e—:_ German =>. English —|-
5 Y/—                         —s=: Romanian=> English |
peer                                    o-! Eriglish <> Ronianian
0                 o           o                                                                                    =
OMB      04B (0.8B 4.3B 268     67Bi 43B                     175B:
Parameters:iniLM:(Billions).
Figure 3.4: Few-shot translation performance on.6:language’ pairs as‘ model capacity. increases. There‘is/a consistent’
trend.of.improvementiacross;all datasets as'the model scales, and as'well.as tendency for‘translation.into:English to be
stronger ‘than translation from English,


--------------------------------------------------


--- 第 16 页 ---

| ‘Fine-tuned SOTA 90.1                 84.6?
GPT-3.Zero-Shot 88.3*                  102
GPT-3' One-Shot         89.7"                 732
GPT-3:Few-Shot         88:6*                  TT
Table:3:5: Results on:the WSC273 version /of Winograd. schemas and the‘adversarial Winogrande dataset.. See Section.
4 fordetails on:potential contamination of the‘ Winograd test set. “[SBBC19]?[LYN* 20]
‘Winogrande
-Human Jt
- fine-tuned SOTA nnn nen eee
8p: —————————
3         se Zero-Shot                                                             ae  |
pela cat 1: a ne 7 =, a
=  |
50: ants cagA IRR REE AREER
0:18:          04B, 0.88 1.3B 26B 67B 3B                                  175B:
Parameters ihiLM. (Billions)
Figure 3.5:: Zero-,.one-,,and:few-shot performance<on the adversarial, Winogrande:dataset as:model capacity scales.
Scaling is relatively smooth with the gains to‘few-shot learning increasing’ with model,size, aid few-shot GPT-3' 175B
ig competitive’ with a fine-tuned ROBERTA-large:
each-translation task: improyes: performance by over 7 BLEU and nears competitive performance: with: prior work:
GPT-3 in'the-full few-shot setting further improves another-4 BLEU resulting:in similar average performance'to prior
unsupervised. NMT-‘work.. GPT-3 has:a noticeable skew in.its‘performancedepending’on language’direction.. For:the:
three input. languages: studied, GPT-3. significantly outperforms prior-unsupervised. NMT work when_translating into
English but underperforms ‘when. translating in,theother direction. Performance on En-Ro is a noticeable outlier at
‘over 10 BLEU worse than.prior unsupervised NMT work. This:could.be‘a weakness due'to reusing:the byte-level. BPE:
tokenizer of GPT-2 which, was developed.:for analmost entirely. English training dataset., For both Fr-Eniand:De-En,
few shot GPT-3 outperforms the best supervised result we could find’ but due to our unfamiliarity with the literature and.
the appearance:that these ‘are un-competitive. benchmarks‘ we donot ‘suspect those results‘represent true state of the art.
For Ro-En, few: shot GPT-3. performs;within 0:5 BLEU of the overall! SOTA: which is,achieved by a combination/of
‘thsupervised pretraining, stipervised finettining on608K labeled examples, and backtranslation: [LHCG1 9b].
Finally,.across ‘all language ‘pairs and acrossyall three settings: (zero-;, one-, and few-shot), there is a:smooth.trend-of
improvement with:model capacity: This is shown.in Figure’3:4 in‘the case:of few-shot.results; and scaling forall three
Settings.is shown in Appendix H.
3.4 Winograd-Style Tasks
The ‘Winograd! Schemas Challenge [[DM12] ‘is a;classical task:in, NLP ithat-involves:determining which word.a pronoun.
refers to,;when:the pronoun.is grammatically ambiguous but.semantically unambiguous to a human; Recently*fine-tuned,
language models have achieved near-human performance on the original Winograd dataset, but:more:difficult versions
16


--------------------------------------------------


--- 第 17 页 ---

Setting:                 PIQA ARC (Easy)  ARC(Challenge) OpenBookQa.
- ‘Fine-tuned'SOTA, 79.4  92.0[KKS*20] '78.5[KKS*20]  87:2[KKS*20] _

‘GPT-3 Zero-Shot 80.5* 68:8                 oL4                    576

GPT-3 One-Shot 80.5* 71.2                 532                    58.8:

‘GPT-3'Few-Shot 828% 70.1                  31.5,                    65:4
Table:3.6: (GPT-3 results on.three commonsense reasoning tasks, PIQA, ARC,,and:OpenBookQA. GPT-3.Few-Shot:
PIQA.result-is evaluated’on thestest server:, See:Section 4 for‘details on:potential:contamination ‘issues on the'PIQA: test:
set,

PhysicalQA
Human, JL epee
[9 “Zero-Shot
                                                  \|e— Few:Shot (K=50)
60:
OMB         O4B: ‘0.88 43B ‘2.68       67Bi 43B                             A75B:
Parameters:iniLM:(Billions).

Figure 3.6: GPT-3:results on-PIQA in the' zero-shot, one-shot, and few-shot settings: ‘The largest:model achieves: a.
score‘on the:development: set:in all:three conditions that exceeds: the-best recorded score! on the task.
such,as ‘the adversarially-mined Winogrande dataset [SBBC19] still significantly: lag: human,performance, We:test
GPT-3’s'performance on both Winograd and Winogrande; as‘usual:in the zero-, one, and few-shot setting.
‘On Winograd'we test'GPT-3 ‘on‘the original, set.of 273 Winograd schemas, using‘the same “partial evaluation” ‘method,
déseribed in, [RWC*19]. Note:that‘this setting differs-slightly from the WSC task in the SuperGLUE benchmark, which.
is presented as binary classification and.requires entity:extraction’to Convert to the form.described|iti this section. ‘On.
Winograd GPT-3 achieves’ 88.3%,'89.7%, and 88.6% inithe zero-shot; one-shot, and few-shot:settings, showing no clear:
in-context learning but in all casés achieving strong-resillts just-a few points below state-of-the-art and éstimated human,
performance. We note that.contamination analysis found some Winograd schemas in the training data but'this appears
to-haveronly. a: small effect-on-results:(see Section 4);                                                               7
Ofi ‘the iniore difficult Winogrande dataset, we do find. gains to if=context learning? GPT-3 achieves 70.2% inthe.
zero-shot. setting, 73:2% in the one-shot setting, and 77.7% in the few-shot. setting. For. comparison ‘a ‘fine-tuned.
RoBERTA.model achieves 79%, :state-of-the-art is 84.6% achieved with a.fine-tuned high capacity: model.(T5), and.
human-performance onthe task as:reported. by [SBBC19] is 94.0%.
3.5 Common Sense: Reasoning
‘Next we-conisider three datasets which attempt'to capture physical or scientific reasoning, as distinct from sentence.
completion,:reading comprehension, or’ broad knowledge:question answering. The first, PhysicalQA: (PIQA) [BZB*19],
asks common.sense‘questions about:how. the:physical'world:works and.is intended as a:probe‘of grounded.understanding
of the World. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% acciiracy one-shot, atid 82:8% acciifacy ‘few-shor
(the lastmeasured on-PIQA’s test server). This compares favorably to'the:79.4% accuracy prior state-of-the-art of a.


--------------------------------------------------


--- 第 18 页 ---

Setting                    CoQA DROP QuAC SQuADv2, RACE-h RACE-in.

Fine-tuned SOTA. 90.7° 894% 74.4% 93.0%            90.0%         93.1°         |

GPT-3 One-Shot, 840: 34.3 43:3 65.4             45.9           STA
Table:3:7: Results on-reading*comprehension.tasks. All:scores are:F1. except results’ for RACE which report:accuracy.
“FIZC+ 19] 8[JN20] °[AI19] @[QIA20] *[SPP+19]
‘finé-tuned:-RoBERTa. PIQA: shows relatively: shallow, scaling with:micdel size and is still over’ 10% worse than human.
performance;, but GPT-3’s few-shot and even.zero-shot result outperform. the current state-of-the-art. Our analysis
flagged PIQA ‘for a potential data coritaminationissué (despite hidden test labels); and, we therefore conservatively mark.
the result with an-asterisk, See Section 4 for details,
ARC [CCE*18] isa, dataset of multiple-choice questions collected. from, 3rd.to ‘9th: grade iscience exams. On ‘the:
“Challenge” ‘version of the dataset‘which.has been ‘filtered to:quéstions which simple. statistical or-information retrieval,
methods are:unable ‘to:correctly answer, GPT-3 achieves 51.4% .accuracy in the zero-shot setting, 53:2%:in the:one-shot:
sefting, and 51.5 in therfew-shot setting. This is: approaching the performance:of a fine-tuned RoBERTa’baseline-
(55.9%) from UnifiedQA. [KKS~ 20]. On the “Easy” versionyof thé dataset (questions which either'of the mentioned,
baseline approaches answered correctly), GPT-3iachievés 68:8%,, 71.2%, and 70.1% ‘which slightlyexceeds a:fire-tuned,
RoBERTa baseline from |[KKSt20]. However, both of. these ‘results are ‘still, much ‘worse ‘than ‘the overall. SOTAs
achieved by the UnifiedQA which éxceéds:GPT-3’s few-shot results by 27% onthe challenge set.and 22% onthe easy’
‘On OpenBookQA [MCKS18],:GPT-3 improves:significantlyfrom.zero:to few: shot settings but is still over 20 points
short of the: overall SOTA. GPT-3’s few-shot performance is similar toa fine-tined BERT Large ‘baseliné on ‘the.
leaderboard.
Overall,:in-context.learning with GPT-3: shows mixed.results:on commonsense-teasoning tasks; with only small:and,
inconsistent gains observed in the one and few-shot learning’ settings: for both PIQA and ARC, but. a, significant
improvement is observed on OpenBookQA.. GPT-3 sets SOTA on-the new PIQA. dataset invall evaluation settings:
3.6 Reading Comprehension
Next we! evaluate;GPT-3 on’ the’ task of reading comprehension. ‘Weruse ‘a suite! of.5 datasets’ including: abstractive;,
multiple choice; and span.based:answer‘formats:in both dialogand single question settings. We:observe:a.wide: spread,
in/GPT-3’s‘pérformance: across these datasets suggestive.of varying capability with:different answer formats. In general.
weiobserve:GPT:3:ision: pat with initial baselines atid early*results trained using: contextual representations: on-each.
respective:dataset.
GPT-3 performs best:(within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational. dataset:
and performs'worst (13 F1. below*an:ELMo baseline) on QuAC. [CHI*'18] a:dataset which:requires modeling’ structured.
dialog :acts:and ‘answer span selections of teacher-student interactions. On. DROP [DWD* 19]; a dataset testing discrete.
reasoning and numeracy in the context of reading comprehension, GPT-3:in-a few=shot setting outperforms the fine-tuned.
BERT 'baséline:from the:original paper’ but‘is still well.below both human performance and state-of-the-art approaches
which augment:neural networks with symbolic systems [RLL*19]. On:SQuAD:2.0:[RJL18], GPT-3 demonstrates its
few-shot learning capabilities,.-improving by almost 10 F1 (to 69.8) compared to a zero-shotsetting. This allows it to
slightly outperform the best:fine-tuned‘result‘in the.original paper. On RACE [LXL* 17], a:multiple choice dataset of
middle school and high:school:english examinations; GPT-3 performs:relatively: weakly and/is only competitive with.
the earliest: work utilizing contextual representations and is still 45% behind SOTA..
3.7 SuperGLUE
In ofder'to better aggregate results‘on NLP tasks and:compare to-popular'models such as BERT and RoBERTa in a.
more: systematic ‘way, we also evaluate:GPT-3 on.a/standardized.collection' of datasets, the SuperGLUE benchmark:
{[BDD*09] [PCC18] [PHR*18], GPT-3’s test-set performance onthe SuperGLUE dataset is shown. in Table 3.8. In the
few-shotisetting; we used 32 examples forall tasks,/sampled randomly from-the training set. Forall tasks except WSC


--------------------------------------------------


--- 第 19 页 ---

90) Pas Hd at esate ana cpensans poocscnyasecsnysecsssnsscsssessensesses
;
8 60)              2 a
50:| 44
fe                                o> ‘One-Shot
30if                                      [=2- Few-Shot.(K=5) ||
OMB:    0.4B: (0.8B 1:3B i2.6B   6:7Bi 3B             475B:
Parameters ini_M (Billions),
Figure 3.7: GPT-3 results: on CoQA reading comprehension‘task., GPT-3:175B ‘achieves 85.F1 ‘in the:few-shot setting,
only a.few. points behind measured human:performance and, state-of-the-art:fine-tuned.models. Zero-shot;and.one-shot:
performance is.a few points behind, with the gains tofew-shot being largest for bigger models.
— SuperGEUE BoolQ = CBC BSCCOPA RTE
Average. Accuracy Accuracy FI Accuracy Accuracy —
Fine-tuned BERT-Large    69:0°     TTA:     83.6 75.7 70.6     TAT
‘GPI-3 Few-Shot          -                 71.8                    76:4:                75:6.           52.0           92.0.                69.0
Accuracy Accuracy Accuracy Fla Accuracy    FL
Fine-tutied SOTA       76.1     93.8     62.3     88.2     92.5     93:3
Fine-tuned BERT-Large 69.6     64.6     24.1.     70.0:    TA3.     720:
GPT-3 Few-Shot;            49.4              80.1.              30:5,             754            902,             91.1
Table:3:8: Performance: of GPT-3 on: SuperGLUE:compared to fine-tuned baselines and SOTA. All results:are reported.
on the test set. GPT-3 few-shot is' given a total of 32 examples within the:context of each task-and performs no gradient
updates..


--------------------------------------------------


--- 第 20 页 ---

=O—* Few-shot (K=32)
i80                                                            |                                              ao HHH

5      fine-tuned BERT+41 ___|._...-).-_ I. -.- =.   est               Fine-tuned BERT +     == ——— ee '

uy 70  Fine-tuned-BERT-Large-—t————P y= =             10 (eS ee  eeeeesesesseeseses

=a                ,      |   :   |  Bo eee        ab Gin           ’           ,    ‘Fine-tuned: BERT .Carge

oo                          iy gi  -    T 3 |                                     ‘

% 60                          = ae                                                   60 |

Ol 04 O8 13 26 67 13                             175              01234 8                    ‘16:                                          32
Billions of ,Pararrieters in LM                                                     Nurnber‘of Exatnples in Cotitext’(k):
Figure 3.8: Performance on.SuperGLUE increases’ with:model.:size‘and number. of ‘examples in:context. A:valué
of AV = 32 means;that:our:modell was shown.32 examples:per task, for:256‘examples total:divided across the: 8 ‘tasks in.
‘SuperGLUE, We report’ GPT-3 valves on: the dev set, so.our: numbers are not diréctly comparable'to-the dotted reference
lines (our test set results’ are in. Tablé 3:8), The. BERT-Largetreference model was fine-tuned on the: SuperGLUE training:
‘set. (125K examples), ‘whereas: BERT+-+ «was first‘fine-tuned on.MultiNLI (392K ‘examples):and SWAG*(1.13Kiexamples),
before ‘further ‘fine-tuning on the SuperGLUE training Set (forsa total of 630K fine-tuning examples). We. find.the.
difference in performance between the BERT-Large and BERT++ to’be'roughly equivalent to'the difference between.
‘GPT-3 with oneexampleper‘context:versus eight examples’per-context.             i
‘and MultiRC, ‘we sampled anew ‘set:of'examples to:use in.the context:for each problem. For WSC and MultiRC, we:
‘used the same, set of randomly drawn examples trom the training Set as context for all of the-problemis we evaluated.
We observe a'wide range in:GPT-3’s performance across. tasks.. On‘ COPA/and.ReCoRD GPT-3 achieves.near-SOTA.
performance: in, the: one-shot and. few-shot settings, with, COPA falling only a couple ‘points short: and achieving
second place-on the leaderboard, where first'place is held by a-fine-tuned 11 billion parameter model (TS). On WSC,
performance is: still'relatively’strong, achieving '80.1:% in.the few-shot setting ;(note:that:GPT-3 ‘achieves:88.6% ‘onthe.
‘original ‘Winograd. dataset, as: described.in Section.3:4). On BoolQ, MultiRC,. and RTE, performance: is:reasonable;
roughly matching ithat of a‘fine-tuned BERT-Large., On CB, we sée,signs of life-at 75.6% in the few-shot setting,
WiC is anotable weak: spot with.few-shot performance at 49.4% (at:random chance). Weitried.a number of different:
phrasings and formulations for WiC (which:involves determining if'a word:is being used-with the:same meaning in:two
sentences), none of which was-able to-achieve strong performance, This hints at a;phenomenon that will become clearer
in’the:next section.(which.discusses the ANLI benchmark) — GPT-3 appears:to be‘weak in:the few-shot or one-shot:
‘setting:at some tasks that involve:comparing two sentences’or snippets; for‘example whether:a word is:used'thesame
Way in two sentences (WiC), whether one sentence is a-paraphrase of another, or whetherione sentence implies-another:
This could also explain the comparatively’ low scores for RTE and CB, ‘which also:follow this‘format. Despite these.
weaknesses; GPT-3 ‘still outperforms’ a.fine-tuned. BERT-large on fourof eightitasks and on two tasks GPT-3 :is close to.
ithe state-of-the-art held by’ a.fine-tiined 1 1 billion parameter model.
Finally, we! note ‘that the few-shot SuperGLUE ‘score steadily improves ‘with. both:model size ‘and. with. number‘of
examples: in. the: context showing increasing benefits from in-context learning (Figure-3.8); We scale C up'to 32
examples per task, after which pointiadditional examples will not reliably fit into:our context. When, sweeping over
values of J, we:find that'GPT-3 requires less than eight:total examples per task:to outperform a.fine-tuned.BERT-Large
con overall ,SuperGLUE score:
3.8 NEI
Nattiral Langtiage Inference (NLT) [Fyo00] coiicertis the ability to uiderstaiid the relationship between two setitences.
In practice, this task is usually structured as'a two or three class classification problem. where the model. classifies
20


--------------------------------------------------


--- 第 21 页 ---

ANLI Round3
a/-ine-tunsd SOTA! wl le el ee eee eee
_fine-tuned RoBERTalarge |
44. Fine:tuned:-BERTsEarge ag sate nh
S|         oe One-Shot: |
36)                             =e                                  5
01B'          0.4B' (0.88 4.3B '2.6B        6.7B 13B                                  175B:
Parameters‘in/LM. (Billions)

Figure 3.9: Performance of GPT-3 on. ANLI Round3., Results are on the devset, which has ‘only :1500 examples
and therefore has high:variance(we.estimate: a:standard deviation:of 1.2%). We' find that smaller:models hover. around.
fandom: chance; ‘while: féew-shot'GPT-3'175B closes almost half-the: gap from-randoim chance to SOTA. Results for
ANLT rounds 1. and 2 are‘shown in-the appendix..
whether the second sentence logically follows from the first, contradicts the first Sentence,,oris possibly true (neutral).
‘SuperGLUE includes an NII dataset,.RTE, which evaluates'the binary version of'the task.'On-RTE,,only the largest:
version of GPI-3 performs:convincingly better than random .(56%):in any evaluationisetting, but in a'few-shot setting:
GPT-3. performs. similarly to a, single-task fine-ttined BERT Large. ‘We also evaluate on the recently introduced,
Adversarial.Natural Language Inference! (ANLT) dataset: [NWD*19]..ANLT is a.difficult dataset employing a series! of
adyersarially mined.natural.language'inference questions in‘three rounds:(R1, R2,:and-R3)., Similar'to RTE, all:of ‘our
models. smaller than, GPT-3 perform at-almost exactly random:chance on ANLI, éven:in the ‘few=shot setting (~. 33%),
whereas GPT-3.itself shows signs of lifeon Round 3:. Results:for ANLI R3 are highlighted 4in.Figure 3.9. and fullresults.
forvall:rounds can be found.in, Appendix: H. These results on both.RTE and. ANLI suggest:that‘NLI isistill:a very. difficult:
itask for langtiage models and,they‘are only just beginning'to show signs of progress.
3:9 Synthetic and Qualitative’Tasks:
One-way to probe:GPT-3’s rangeé-of abilities in the few-shot(or zero- and one-shot) setting is to give it tasks Which,
require it to perform. simple on-the-fly computational. reasoning, recognize .a:novel.pattern that.is ‘unlikely ‘to have.
occurred:in training; or adapt:quickly toxan:unusual task. We devise several tasks:to, test ‘this class of abilities, First, we
‘test GPT-3's ability to:perform arithmetic. Secorid, wercréate several tasks that involve rearranging or unscrambling the
Jetters in’ asword,;tasks whichiare unlikely:to-have been exactly seen’ during:training. ‘Third, we:test GPT-3’s ability to
‘solve: SAT-stylesanalogy-problems few-shot. Finally,-we test: GPT-3' on/several qualitative: tasks, including using:new’
Words in-a-sentence, correcting English grammar, and. news articlé.géeneration. We will-teléase the Synthetic. datasets
with the hope.of stimulating furtherstudy of test-time behavior'of language models..
3.9.1 Arithmetic:
To test:GPT-3’s ability to:perform simple arithmetic operations ‘without task-specific:training, we developed a small.
battery of 10 tests'that involve asking GPT-3 a simple arithmetic problem:in natural language:

« 2 digit addition (2D+) — The model is asked to add two:integers sampled uniformly: from| (0, 100), phrased in,

the ‘for of a.question, ¢.g. ““Q: What is-48 plus 76? A: 124.”
2 digit.subtraction (2D-):—'The model is:asked'to: subtract:two integers sampled uniformly from (0,100); the:
answer. may. be-negative. Example: “Q: Whats 34:minus53?. A: -19".                                           ;
© 3 digit addition (3D+) - Same as.2 digit addition, except‘numbers are uniformly sampled from. [0, 1000)..


--------------------------------------------------


--- 第 22 页 ---

400                                  Arithmetic :(few-shot)
400:                                                            A
| |=e- Two Digit Addition                                                       Sf.
go'|[—e— Three ‘Digit. Addition   Z  Cf
~~ || —o—: Three: Digit: Subtraction                                    wz      SS
—e— Four DigitAddition:                           wa     fj
go || Four Digit Subtraction,                           KEE.         Vay
S ~"||=0= Five Digit Addition                            we              Jy
3       —e—. Five Digit:Subtraction                 fr         J
“="!|—e—° Single Digit‘Three.Ops-                  ra           Vy
Q ————_—_—_—S_—_—_—_—
OMB:        0.4B (0.88 1.38 (268 67B 43B                            175B:
Parameters‘in/LM. (Billions)
Figure 3:10: Results on all 10 arithmetic. tasks in’ the few-shot settings for imodels of different sizes. There is a.
‘significant;jump:from.the second largest:model (GPT-3 13B);to the largest model(GPT-3 175), with the Jatter:being
able to.feliably accurate:2 digit arithmetic, Usually acctirate:3 digit arithmetic, and correct;answers @ significant fraction.
of the:time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot-and zero-shot
are shown'in the:appendix.            ,

‘* 3 digit subtraction (3D-) = Samé:as 2:digit subtraction, excéprnumbers are uniformly sampled from [0,1000).

‘© 4 digit addition (4D+) — Same as:3 digit addition; except:uniformly sampled from, [0; 10000):

‘« 4 digit subtraction (4D-)— Sameras 3 digit subtraction; except uniformly, sampled from [0; 10000):

‘e § digit.addition (5D+) — Same as:3 digit addition, exceptruniformlyisampled from, [0, 100000).

© 5 digit subtraction (5D-):—Samevas 3 digit subtraction, except uniformly sampled from [0, 100000).

‘« 2 digit multiplication (2Dx) —'The:modeltis asked.to:multiply two integers sampled uniformly from.|0, 100),
exge “Qs What'is 24times 42? Ax 1008”.

‘*, Oné-digit:;composite (IDC):— The model is:asked to-perform.a composite operationon three‘ digit numbers,
with patentheses arotind the:last two. For example, “Q: What is‘6+(4*8)? Av 38”. The three. 1 digit numbers
are’ selected uniformly’ on [0, 10) and the‘operations ‘are selected uniformly from. {+,-,*}.

instancesof the task’and:evaluate:all models.on those instances.

First we evaluate GPT-3 in the few-shot setting, for-which results.are shown in Figure 3.10; On.addition.and subtraction;
GPT-3 displays strong proficiency when.the number'of digits is small, achieving 100% accuraey‘on 2 digit addition,
98.9% at 2 digit subtraction, 80.2% at 3 digitaddition, and 94.2% at’3-digitisubtraction. Performance decreases as'the:
number‘of digits increases, but.GPT-3 still achieves 25-26% accuracy:on four digit operations:and 9-10% accuracy: on,
five digit operations, suggesting at least some capacity to. generalize to larger numbers of digits. GPT-3 also achieves
29.2% accuracy at 2 digit multiplication, an ‘especially computationally intensive operation.. Finally, GPT-3 ‘achieves
21.3% aceuracy-at single; digit combined operations: (for example, 9*(7+5)), suggesting that:it-has some robustness
beyond just single.operations.

As Figure:3.10 makes: clear, small models do poorly-on all-of these'tasks —‘even.the 13: billion parameter:model (the
second largest after the: 175 billion full, GPT-3) can:solve 2: digit addition,and, subtraction:only half the'time, and. all.
other‘operations less than. 10% of the'time,

One-shot and:zero-shot;performance are‘somewhat degraded:relative'to:few-shot performance, suggesting that adaptation
to the ‘task (of at the: very least recognition Of the task) is important to performing these computations correctly.
Nevertheless, one-shot performance is still quite‘strong,, and:éven zero-shot-performance'of the full GPT-3 significantly


--------------------------------------------------


--- 第 23 页 ---

Setting             2D 2D: 3Dt 3D: 4D# 4D= SD+ SD 2Dx IDC
‘GPT-3One-shot 99.6 86:4 65.5 78.7 140 140 35 3.8 274 143
‘'GPT-3:-Few-shot 100.0 98:9 80:4 942 25:55 268 93 99 292 21.3
Table 3.9 Results on! basic arithmetic tasks ‘for 'GPT-3:175B. {2,3,4,5}D{+.-} is 2, 3,4, and 5: digit addition, oF
subtraction, 2Dx is 2:digit:multiplication.. 1DC is L. digit:;composite operations. ‘Results become: progressively. stronger
moving fromthe zero-shotto:one-shot:to few-shot setting, but even:the:zero-shot shows) significant:arithmeticabilities,
Setting             cL Al A2 RI RW
GPT-3 Zero-shot 3:66 2.28 8.91 8:26 0.09
GPT-3 Oneé-shot 21.7 8.62. 25.9 454 0.48
‘GPI-3 Few-shot 37.9 15.1 39.7 67.2 0.44
Table'3.10: GPT-3 175B performance:on various ‘word.unscrambling‘and.word manipulation‘tasks, in zero-, one-,,and.
few-shof settings. ‘CL/is “cycle letters in word”; A. istanagrams of but:the first and/last letters, A2,is anagrams of all’ but:
ithe first and last'two letters, RTis “Random insertion in word”,.RW is “reversed words”.
‘outperforms few-shot learning forall smaller itiodels. All three settings for'the full GPT-3 are shown Table:3.9,.and,
model.capacity scaling forall three settings is: shown in. Appendix.H.
To, spot-check whether ‘the modeLissimply. memorizing specific arithmetic problems, ‘we took-the 3-di: gitiarithmetic:
problemis in our test set and. séatchedfor themin our training data in both the forms’ "“<NUM1> + <NUM2> =" and.
“<NUM1> plus <NUM2>".. Out. of 2,000: addition ‘problems! we found only’‘17 ‘matches (0:8%): and. out of 2,000:
‘subtraction problems ‘we; found only 2 matches (0.1%), suggesting, that.only a trivial :fraction.of the correct answers
could have been memorized. In addition,diispection of incorrect answers:réveals:thatithe model often makes mistakes
such. as' not: carrying.a. “1”, suggesting it: is actually attempting to; perform. the relevant computation ‘rather’ than.
memorizing: a.table:,
Overall,, GPT-3 displays reasonable-proficiency at moderately complex.arithmetic.in few-shot, oné-shot, and even,
zero-shot settings.
3.9.2 Word Scrambling and.Manipulation Tasks'
To test. GPT-3’s,ability*to learn novel symbolic:manipulations from: a:few:examples, ‘we designed a small. battery‘of,
‘5 “character manipulation” tasks: Each task involves giving the: model. a, word distorted by some combination ‘of
scrambling, addition,-or deletion of characters, and asking it-to recover the original word. The 5 tasks are:

‘| Cycle letters in word (CL) — The model .is,given,a;word with its letterscycled, then the “+” symbol, and
is expected to generate the. original. word, For example, it-might-be given “lyinevitab” and. should output:
“inevitably”.                :                       -          ,      7        /                           ,

‘ Anagrams of all but:first and.last:characters (A1) —'The modeLis given a word where every letter‘except:
the first-and last have beet Scrambled randomly,.and:must output the original word. Example: criroptuon =
corruption,

‘ Anagrams of all’ but:first-and.last:2 characters (A2) — The model is given’a word whereievery letter except:
thezfirst 2: and last 2 have been scrambled:randomly; and:must recover‘the:original! word:. Example: opoepnnt:
=> Opponent.

* Random. insertion in word (RI) = A random punctuation or space character is-inserted: between each Jetter'
of.a.word, andthe model must:output the original word. Example: s:u!c/cle.sis:i/o/n= succession..

‘e Reversed words:(RW) — The:model is given.a:word:spelled’backwards, and must:output.the original. word.
Example: stcejbo.— objects:

Forieach.task we generate 10,000examples, which we:chose’to be the'top' 10,000 most frequent'wordsias measured by:
|[Nor09] of length more: than 4 characters and less'than 15 characters. The‘few-shot results aré shown in Figure3.11,
Task performance tends to, grow'smoothly with model size, with the full GPT-3 model achieving 66.97%:on removing:


--------------------------------------------------


--- 第 24 页 ---

70.              Wordscramble (few-shot)
== cycle letters:                                 | |
.60' || =e mid word “anagrams.                                                                                      fo_|
|| —e— tid word 2 anagrams                     ||                           S
_. || —e—- random ‘insertion,                                                          4
50, |}                         —E
—e— reversed'words          |    P|                 A
0 A GS GO
OMB:          0.4B: :0.8B 4.3B :2.6B        6.7B! 13B:                                  475B:
Parameters‘in/LM. (Billions)
Figure 3.11: Few-shot perforniance on the fiveword ‘scrambling tasks-for different sizés of niodél. ‘There is generally
smooth improvement with model size although:the random insertion task shows’ an upward. slope of improvement with.
the 175B.model solving 'the task the majority of the tithe: Scaling of one-shot and zero-shot-performance is shown in.
the appendix. All tasks-are done:with A’ = 100.
random insertions, 38:6%:on cycling letters; 40.2% on‘the easier anagram:task, and 15.1% onthe moredifficultanagram.
task (where'only the first and last-letters.are held fixed). None of the models can reverse the letters.in a word.
Tn the:one-shotisetting; performance is, significantly weaker (dropping’by half or moré), and:in the:zero-shot setting the:
model can rarely‘perform. any of the'tasks (Table 3.10). This stiggests that:the model really does appear ito learn these.
tasks at test:time’, as'the imodel-carnot:perform then: zero-shot and their.artificial nature makes‘them unlikely. toappear
inthe pre-training data (although.weicannot-confirm this withicertainty).
‘We can further quantify performance by plotting “in-context learning’ curves”, which show’ task performance as a,
function of the number of in-context examples. We  show’in-context.learning ‘curves ‘for the Symbol Tnsertion.task:
in: Figure 1.2. We can, see that.larger models are able ‘to, make increasingly effective use-of in-context information,
including both’ task examples and natural language task:descriptions.
Finally, ‘it:is ‘worth adding thatisolving these tasks requires character-level manipulations, ‘whereas our BPE encoding:
‘operates.on significant fractions of'a word.(on,average ~ 0.7 words per:token); so from the LM’s,perspectivesucceeding
atthese.tasks involves not just manipulating BPE tokens but understanding and pulling -apart.their substructure, Also,
(CL, Al, and A2.are not bijective (that.is, the unscrambled word:is notia deterministic function‘of the scrambled word),
requiring the:model to perform: some: search to find.the correct unscrambling. Thus, the skills involved:appear to:require
non-trivial pattern-matching and computation.
3.9.3 SAT-Analogies
To: test GPT-3 on another'task that is somewhat unusual relative'to the'typical distribution of text, we collected a set of
374 “SAT analogy” problems [TLBS03}. Analogies are a,style of multiple-choice question that-constituted a section of
the SAT college:entrance‘exam before:2005. .A’typical example is: “audacious is'to boldness/as (a) sanetimonious is to
hypocrtisy,. (b): anonymous: to identity; (c) remorseful.is to misdeed, (d) deleterious:is to:result, (¢) impressionable’is to
temptation”. The:studeiit is expected to choose which of thefive‘word pairs has the same relationship as the original.
word pair; in'thistexaniple theanswer is’ “satictimorious is'to hypocrisy”. On this task: GPT-3. achiéves 65.2% inthe,
few-shot setting, 59.1% in:the one-shotsetting, and 53.7%:in the zero-shotisetting, whereas:the averagescore among.
college applicants was 57%: [TLO5] (random, guessing yields 20%). As shown:in Figure 3.12, the-results improve, with,
scale, with the:the full 175: billion-model:improving by over 10% compared to:the 13 billion.parameter‘model.


--------------------------------------------------


--- 第 25 页 ---

SAT Analogies.
—e— -Zero-Shot                                                                                   Le  :
go [2 (One'Shot:                                                                          ee
“ll —o-—. Few:Shot (K=20)                                                  eet               —_
50'                 wane a
30: , C
OMB:           0.4B: :0.8B 4.3B :2.6B         6.7B! 13B:                                     475B:
Parameters‘in/LM. (Billions)
Figure 3.12: Zero-, oré-,atid few-shot;perfortiiance on:-SAT. analogy. tasks, for ‘different sizes ‘of model. ‘The largest:
model achieves 65% accuracy, inthe few-shot setting, and also:demonstrates significant gains:to in-context learning
which are: not present in smaller models.
3.9.4 News Article'Generation
Previous‘work:on generative:language models qualitatively'tested their. ability:to: generateisynthetic'“newsvarticles” by
‘conditional sampling from:the model:given a‘human-written _prompt'consisting ofa plausible firstisentence:for anews
story [RWC*19]. Relative to [RWC 19], the:dataset.tsed to train GPT-3 is much less weighted towards news.articles,
‘so trying to generate news ‘articles‘via raw unconditionall samplesiis less effective — for'example GPT-3 often‘interprets.
the proposed first.sentence ofa “newsiarticle” ‘as a tweet;and.then posts synthetic:responses or follow-up tweets. ‘To
solve this-problem, we employed GPT-3’s few-shot learming-abilities by providing three:previous news articlés inthe.
model’s context'to condition it.. With the title, and subtitle of @ proposed next: article,, the model. is: able to reliably
‘generate short articles in:the “news”: gente:.
To gauge the-quality of news article generation‘from GPT-3 (which we believeiis likely to be correlated with conditional
‘sample. generation quality. in. general), ‘we decided to measure human. ability to distinguish:GPT-3-generated articles
from:real ones. Similar -work hasibeen‘carried:outiby Kreps et al. [KMB20] and Zellers et al., [ZHR*19]; Generative:
Janguage models aré-trained tormatch the distribution of content generated by humans, so the (in)ability of humans to:
distinguish.the two is’ a‘potentially important:measure of.quality.?
In order to: see:how-well humans:can detect:model generated text, we:arbitrarily:selected 25 article:titles, and. subtitles
from the' website newset.com (mean length; 215 words), We then. generated completions of these titles:and.subtitles
from.four language: models:ranging‘in: size from 125M to: 175B:(GPI-3)-parameters (mean length: 200'words), For, each.
model; we presented :around 80 US-based participants,with a quiz consisting of these real titles and subtitles:followed.
by-either the human ‘written article or the article generated by theamodel*. Participants’ were asked to select whether the.
article ‘was.“very likely written.by.a human”, “more likely-written by‘a human”, “I don’t-know”, “more likely’written by’
‘a machine”, or “very likely written: by. a machine”.
The’ articles we selected ‘were not.in the models’ training data andthe. model outputs. were formatted. and selected.
programmatically to prevent-human.cherry-picking: All models used the same context to ‘condition outputs on‘and were
pre-trained with:the:samecontext:size and. the same: article-titles and subtitles were-used:as prompts for each:model:
However, we also ran an-experiment.to control for participant effort and attention that followed the same format-but
involved.intentionally bad:model generated articles.. This' was done by generating articles from a‘“control model’: a.
160M’ parameter:model with:no context;and increased outputrandomness:
*This task is also relevant to thé:potential misuse of languagé:models discussed in Section 6.1
“We wanted to identify how good an‘average’person.on the internet is at detecting language model outputs, so’we:focused on.
participants.drawn from the general US population. Seé Appendix E for details..


--------------------------------------------------


--- 第 26 页 ---

                             95% Confidence. ‘tcompared:to “T don’t know”
Mean accuracy Interval (low,.hi): :control(p-value) — -assigninients
Control. (deliberately bad model)              86%                  83%-90%,                        -                          3:6 %
(GPT-3 Small.             76%:       72%-80%      3:9:(26-4):       4.9%
GPT-3 Large                                                       68%                      64.%-12,%                 73 Ge-T1)                     8.7%
GPT-3 XL,                   62%:       59%-65%     10:7 (le=19)      15%
GPT:3 2.7B                 62%:      58% 65%     10:4:(Se-19)      7.1%
GPT:3 6.7B:                60%      56%-63%     ‘LL2:(3e-21)      6.2%
GPT-3 13B                                      55%              52%-58%           15.3 (1é+32)             1.1%
GPT:3 175B:                                                     52%:                     A9%-5A%               16:9:(1e+34)                   T&G
Table 3.11; Human.accuracy in-identifying whether short (~200 word).news articles are model generated. We.
find that:human accuracy: (measured by the ratio: of'correct assignments'to non-neutral/assignments):ranges:from 86%:
on thecontrol:model to 52% on}GPT-3:175B. This table-compares, mean,accuracy. between five: different models; and.
shows the'results of.a two-sample T-Test for the difference in mean accuracy between each model and.the control model
(an unconditional GPT-3 Small model:with increased outputzrandomness)..
Mean‘human.accuracy (the:ratio, of correct:assignments;to non-neutral,assignments’per:participant) at:detecting that:
the intentionally bad articles were itiodel generated ‘was ~'867% where 50% is chance.léevel performance. By contrast,
mean-human accuracy’ at detecting articles that-were'produced by'the 175B:parameter:model was barely above chance.
ati~i 52%, (see Table:3.11).° Human: abilities todetect model generated textiappear‘to:decrease.as model size:increases:
theré appears ito be a.trend towards chatice accuracy with model size, and humaii detection of GPT-3 is closéto:chance.®
Thiscis true:despite:the fact that participants spend ‘moretime on/each outputtas model sizetincreases (see Appendix E),
Examples:of synthetic articles from GPT-3:are given. in Figures 3:14 and 3:15.” Much of the text:is—as‘indicated. by: the
evalvations—difficult for humans to distinguish froni authentic human content, Factual inaccuracies can be an indicator
that:an.article istmodel generated isince;;unlike humanauthors, the models have no access to’ the:specific:facts:that the:
article titles:refer, to ‘or when the) article ‘was written. Other ‘indicators:include:repetition, non: sequiturs, and-unusual.
phrasings, though:these:are often subtle enough. that:they- are not noticed.
Related-work on.Janguage:model detection‘by, Ippolito et:al.. [IDCBE19] indicates;that automatic: discriminators like:
GROVER, [ZHR*19] and GLTR [GSR19] may have: greater success at.detecting model generated text than-human.
evaluators. Automatic detection of these models‘may be a’ promising area of future research.
Tppolitozet:al.. IDCBE19] also note that human accuracy, at detecting model generated text increases, as humans ‘observe
moré‘tokens. To do.a,preliminary investigation of how good humans are at. detecting longer news articles generated.
by: GPT-3).175B,, we sélected 12:world news articles from’Reuters withan average length of 569 words and generated
completions: of these articles from GPT£3 with.an ‘average length of 498 words (298 ‘words longer. than: our: initial,
experiments). Following themethodology above, weran two éxperinients, each onarourid 80 US-based participants, to.
compare:human abilities to detect the articles, generated by GPT-3 and @ control model.                         oe
‘We found ‘that mean: human, accuracy: at:detecting ‘the intentionally’ bad. longer articles: from.the-controll model. was
 88%, whilémeéan hunian accuracy at detécting the longer articles that were produced by'GPT-3 1,75B ‘was still barely
above chance’ at ~. 52% (see Table 3.12).. Thisindicates that,;for news articles that‘are around:500'words:long, GPT-3
‘continues’to, produce articles that humans:find difficult:to, distinguish from,human written newsiarticles.
3.9.5 Learning:and Using Novel Words
A task studied:in developmental linguistics [CB78] is the:ability:to learn.and utilize new words, for example using: a.
word:in a:sentence afterseeing it:defined only once; or conversely. inferring a:word’s:meaning ftom.onlyone-usage: Here
wequalitatively test GPT-3’s ability'to do the former. Specifically, we give GPT-3 the definition ofa nonexistent word,
such.as “Gigamuru”, ‘and:then ask it.to-use itin:a'sentence: We provide’ one’to five previous examples of a‘ (separate);
We useva‘two-sample‘Student’s T-Test to:testfor significant:difference‘between the means of the participant accuracies:ofveach,
‘model andthe control model and report:the:normalized differencetin.the means:(as the t:statistic),and the;p-valuei.
*If a:model, consistently produces texts that-aré more impressive'than‘human articles; it is:possible-that human: performance:on,
this ‘fask would drop’below 50%: Indeed; many:individual participantsiscored below:50% on this task:                    -
“Additional non-news Saitiples.caii be!fouitid:in Apperidix F,


--------------------------------------------------


--- 第 27 页 ---

Humansability to detect model:generated newsrarticles
ac                              -_—,       ®
7                             raridorichares (50%)                                                                                             — ||
ies!                                             led,                                            te10                                            ‘Leli
Number of:parameters (log scale)’
Figure 3.13: People’s:ability to:identify whether news articlesiare-model-generated. (measured by, the ratio of correct:
assignments tonon-neutral assignments) decreases as model size increases., Accuracy'on the Outputs on the deliberately-
bad coritrol model (an unconditioned’GPT-3 Small:model with.higher output ratidommess) issindicated with the dashed.
Jine ‘at:the top, and the random:chance (50%) is indicated withthe dashed.line:at:the bottom. Line of best-fit is a:power’
Jaw with95% confidence intervals.
/                          95% Confidence, ‘tcompared'to —“T don’t know”
Mean accuracy Interval (low,-hi) control (pevalue) assignments
GPT-3 173B           52%:                48%-5 1%.          12:7 (3.26223)            10.6%

Table.3:12: People’s ability'to identify whether: 500, word articles, are:model. generated (as:measured by. the ratio of
correct assignments to non-neutral assignments) was 88% onthe control.model and 52%:or GPT-3 1.75B, This table.
‘shows the:results' of. a.two-sample:T-Test for the:difference in mean’ accuracy: between GPT-3 175B and ‘the control.
model (anuneonditional.GPT-3 Small model with:inereased:output randomness);


--------------------------------------------------


--- 第 28 页 ---

Title: United Methodists Agree to Historic Split
Subtitles Those who oppose gay marriage will form their own. denomination
Article: After. two days of intense debate; the United, Methodist Church
has agreed to a historic split - one that is expected ‘to: end in, the
création. of a new denomination, one that will ‘be ‘theologically and
socially conservative," according ‘to The Washington, Posts The majority of
delegates attending ‘the church’s annual General, Conference in May voted, to
stréngthen a ban. on ‘the ordination of LGBTQ clergy and ‘to write new rules
that will “discipline" clergy who officiate at same-sex weddings. But
‘those who opposed these measures have anew plan: They say they wiil, form a,
séparate dénomination by 2020, calling their church. the: Christian Methodist’
‘denomination.          7             ~
The Post notes: that: the denomination, ‘which, claims 12.5 million, members ; ‘was
in, the early 20th century the “largest Protestant denomination in the U.S..."
but that it has; been shrinking! in recent decades. The new split will. be the
second in the church?s history. The first occurred, in 1968, when roughly
10 percent Of the denomination left to form the Evangelical United Brethren,
Church: The Post notes that the proposed split’ “comes at a critical time
for the ‘church, which has ‘been, losing: members for years," which has been
“pushed ‘toward ‘thé brink Of 4 Schism oVer ‘the role of LGBTQ people in the
church." Gay marriagé is not the only issue that) has divided ‘the church.. In.
2016, ‘the denomination. was split over ordination of transgender clergy, with.
‘the North, Pacific regional conference voting: to ban, them from serving as
clergy, and the South Pacific regional conference voting to allow them.
Figure 3.14: ‘The GPT-3 generated news article that humans.had the greatest difficulty distinguishing:from a human.
written article (accuracy: 12%).                                     -               7       :       :
Title: Star’s Tux Promise: Draws Megyn Kelly’s Sarcasm
Subtitle: Joaquin Phoenix pledged to not change: for each awards event
Article: JA year ago, Joaquin Phoenix made ‘headlines when tie appeared on
the red carpeti at the Golden Globes wearing a tuxedo with a ‘paper bag over
his head that read, "I am a shape-shifter. JI can’t change the world. J
can. only change myself." It was a promise ‘to not change to fit into the
Hollywood. mold: “I ‘think that’s a really special. thing, to not change
yourself. I think it’s a really special thing to say, ‘This is, whats
inside of me, I’m. proud. of it, and I’m not going ‘to be ashamed because of
‘the way that someone else thinks I should be.°" Nowy. it@s the Oscars, and
Phoenix: is at) it again, But this time, his publicist is saying he?11. be
wearing @ tux no matter what.         .
Megyn Kelly ‘was; not; impressed, and she let Him ‘have it) on, The Tonight Show.
"You, know, I feel like, I feel like: you could, have worn the tux," she says:
“But youre Saying you’ré a Shape-Shifter. I don’t know if you can change
‘your tux’; but! ‘you can’ change your mind: ‘You. can’ change: your mind: ‘You. can
change’ your, mind." Phoenix says he did, but it didn’t stick. “I was like,
‘Okay, I’m going to wear @ tuxéd6 to this thing.” And then I thought, ‘1
don?t ‘want to wear a tuxedo to this: thing.°" Kelly goes’ on to encourage him
‘to change his’ mind again, but Phoenix says, it’s ‘too late: “I’m committed to!
wearing this."
Figuré 3.15: The GPT-3 generated news article that humans found the easiest‘to distinguish:from;a human written.
‘article (accuracy: 61%),


--------------------------------------------------


--- 第 29 页 ---

A “whatpu" cis: a, small, furry animal mative: to Tanzania. An.example of ‘a sentence that ‘uses’

the word, whatpu, iss

We:were traveling’ in Africa and we saw these very cute whatpus;

Todo, a "“farduddle" means: to jump'up and! down really fast. An, example ‘of a, sentence: ‘that uses

the word, farduddle ds:         ;

Gne day when I was playing tag with my little sister, she got really excited and she

started doing ‘these crazy farduddlés..

A “yalubalu!" sis .a type! of vegetable ‘that: looks dike: a big:pumpkin, An ‘example of ‘a ‘sentence

‘that-uses the word yalubalu is:                       a

I was on a.trip to Africa and I ‘tried this yalubalu vegetable ‘that was grown in. a garden’

there: It was delicious:

& "Burringo" tis a.car with. very fast acceleration, An example) of a sentence that uses ‘the

word Burringo: ist:

In our garage we have: a BUTringo that my father drives to work every day...

A "Gigatititi'" 28 a type of Jaépaiiese iusical. instiitient. An examplé of @ Séiitence that uses ‘the

word, Giganuri, is?

iI have a Gigamuru, ‘that my uncle gave meas a gift. I, love: to ‘play it at home.

To "SCréegs" Sométhing 4s Co SWitig A Sword Bt it. An exaiple of a Sentelice that Wses, the word

sereeg is

We screeghed, at ‘each other for several minutes’ and then we went outside and ate ice: cream:
Figure 3.16: Representative GPT-3' completions:for the few-shot task of using anew-word in a:seritence: Boldface:is
GPI-3’s completions, plain text is. human. prompts, In.the:first example both-the ‘prompt and.the:completion are provided.
by-a human; this then’ serves) as conditioning forisubsequent:examples where GPT-3 ‘receives successive’ additional.
prompts;and. provides: the completions. Nothing task-specifie is provided to GPT-3 other than.the conditioning shown,
here,
nonexistent word being defined ‘and used in‘ a:sentence, 'so the task is few-shot.in terms ‘of ‘previous:examples' of the
broad ‘task and oné-shotin terms Of the:specific word. Tablé 3.16 shows the 6 examples we getieratéd; all definitions
‘were-human-generated, and the-first answer was human-generated as’ conditioning while the subsequent answers were.
generated. by. GPT-3. ‘These examples’ were generated:continuously in:one'sitting and we:did.notiomit or.repeatedly try,
any: prompts. In all.cases ‘the generated sentence; appears;to'be. a.correct or-at.least plausible-userof the word. In the final.
‘sentence.the model generates a’plausible:conjugation for the word “screeg”’ (namely. “screeghed”), although the use of
the: word.is slightly: awkward (“screeghed at each:other”) despite being;plausible in:the sense ‘that it;could describe a toy’
sword fight: Overall, GPT-3:appears' to be-at:least.proficient:at'the task of using: nevel:words:in asentence:
3.9.6 Correcting English:Grammar.
Another. task well. suited: for:few-shot.learning is ‘correcting English grammar.. We test this ‘with GPT-3 in the few-
shot Setting: by giving prompts of the ‘form "Poor English, Inputs <sentenceé>\\n, Good English Output:
<sentence>"., We give:GPT-3-one:human-generated correction and then ask-it to correct 5 more (again‘without any
‘omissions ‘or repeats). Results areishown in:Figure:3. 17.
4 Measuring and Preventing Memorization Of Benchmarks.
‘Since. our’ training; dataset is. sourced from the internet, it is possible ‘that our model. was' trained on. some of our’
benchmark test sets.. Accurately detecting test contamination from internet-scale datasets is ‘a new area-of research.
without established. best practices: While it is;common.practice'to;tramn ‘large models without-investigating contamination;
given the increasing scale:of pretraining datasets, we believe this issue is becoming,increasingly importantito attend'to.
This:concern is not just'hypothetical. One of the first papers to train alanguage'model:on Common: Craw] data [TL18]
detéctéd and removed. a training document which overlapped With one-of their evaluation datasets., Other work such,
‘as GPT-2 [RWC'19] also conducted post-hoc overlap ‘analysis. Their study’ was relatively encouraging, finding that


--------------------------------------------------


--- 第 30 页 ---

Poot English ‘inputs IT eateéd.’the purple berries.

Good English oitput: T ate the purple berries.

Poor English input Thank: you. for picking me as ‘your déeSignér., I’d. appreciate it...

‘Good English output: Thank you for choosing: me as your designer, I appreciate ity,

Poor English input: The mentioned changes) have done. or I did the alteration that you

requested: ort changed. things: you ‘wanted and. :did, the modifications.

Good, English Siitpit: ‘The requested changes have ‘beéiiiiade.. of I ade thé alteration that ‘yo

Yequéstéd., of IT changed. things you watéd ard. made the modifications...

Poor English inputs i'd. "be more than. happy ‘to:work with you in another project.

Good English, output: 1d bei more than, happy to ‘work with you; on another’ project.

Poor English inputs Please provide:me with a short brief of the: design you’re looking for’.and

‘that?d: be nice af you. could share: some examples’ or ‘project: you did. before

Good English. output: Please provide me with a brief description of the design you're,

looking for aiid ‘that would. ‘bé nice af you could, share some examples’ or projects you have’

done before.

Poor English input, The: patient was died.

Good English. citpiit: The patient died.

Poor English inputs Wé think that Leslie likés ciirselves.

Good English. citpiit: We think that Leslie: likes is’..

Poor English inputs Janet ‘broke Bill. on the finger.

Good English. citpiit: Janet broke ‘Bill’s finger.

Poor English inputs Mary arranged for, in St. Loiiis', John. to réiit @ hose cheap.

Good English. outpit: Mary arranged, for John to rent A. hoiise ih. St. Loiiis'.

Poor English anputs Today I have went to ‘the store to to buys’ some many bottle of water:

Good English. output: Today. I went to the store to buy some bottles: of water.

Poor English inputs, I have tried to hit ball with. bat, ‘but my Swing is has: diss.

Good English, output: I tried to hit ‘the ‘ball with the ‘bat, ‘but my swing missed.
Figure 3.17: Representative GPT-3 completions for the sfew-shot ‘task: of correcting English grammar. Boldface:
1g:GPT-3’s completions, plain text is human prompts, Inthe. first few examples example both the prompt and ‘the.
completion are’provided.by a.human; this then.serves‘as conditioning for subsequentiexamples ‘where GPT-3 receives
‘successive additional prompts and:provides the completions. Nothing:task-specifie is provided to'GPT-3 aside'from,
the firstfew examples as conditioning and the: “Poor English: input/Good English:output” framing. We note thatthe.
‘distinction: between.’poor” and.’good” English (and the.teritis themselves) is coniplex,.contéxtual, and:contested. As
the example:mentioning the:rentall of'a house shows, ‘assumptions that the model makes about what:“good” is canieven.
lead it-tomakeverrors (here;the model not only adjusts grammar, but alsoremoves the word “cheap” in a, way that alters
meaning).

30


--------------------------------------------------


--- 第 31 页 ---

——                               GPT-3 Training Curves

3.00 —\ re                                                       _                |           +

2)                Oh Cie                                                                                                 o)

SO 2:19    INGE Secals    _        en iad eet Moc || | 10  o

Ww oD,          ~      OA ata ee PK       Bela LB Shs      Te    2    Lind                     !

a2         SS  Se anette |e al                io.

AS                SEE aia eesti anonvactith                 oF
5%         | AES        ne  SS ee lens. | P10 S
‘1.50: ——              GAR     on     one     =       = AO
0            50           100 150 200          250          300
Tokens Elapsed
(Billions)
Figure 4.1: GPT-3 Training'Curves: We measure model performance-during training-on a deduplicated validation.
‘splitiof our training distribution. ‘Though’there is some gap between training and validation performance, the gap! grows.
only minimally with.model:size: and ‘training time, suggesting ‘that:most/of the,gap comes:from a:difference in:difficulty:
father thant overfitting.
although models did. perform moderately’ better’ on.data that overlapped. between training and testing, ‘this. did not
‘significantly:impact:reported results due to the:small'fraction of data whichwas:contaminated.(often:only a-few. percent).
GPT-3 operates ina somewhatdifferenttegime. On thé-one hand; thé dataset atid model sizeare about two orders of
magnitude larger than those used:for GPT-2,,and includesa large amount of Common.Crawl, creating increased potential.
for:contamination and‘memorization. ‘On the other:hand, precisely\due-to:the-large amount of:data;, even GPT-3 175B
does not Overfit its training set by a significant amount, measured relative to a held-out validation set with which it was
déeduplicated (Figure 4.1). Thus; we expect that contamination.is likely to be:fréquent, but that.its effects: may not be as
large as:feared.
We itiitially tried to address 'the isstie of contamination by proactively searching for and attemptinig'to remove ay overlap
between our‘training data arid the:dévelopnient and test séts of all-benchmarks studiéd ‘in this’paper.. Unfortunately, a.
bug resulted:in only, partial:removalof, all-detected. overlaps from.the: training data. Due to'the cost‘of training, it'wasn’t:
feasible to retrain the model. To address ‘this, weanvestigate in detail how the remaining: detected overlap ampacts
results.
For each. benchmark, we:produce‘a “clean’ ‘version which removes allpotentially leaked'examples,. defined roughly as
examples that have a [3-gram overlap with anything in the pretraining set (or that overlap ‘withthe whole example when,
‘itis shorter than. 13-grams).. The goal is'to very: conservatively flagianything that:could.potentially be ‘contamination,
‘so as to :produceia cleanjsubset that is free-of contamination. with high:confidence. The:exact procedure is detailed in,
Appendix C.
‘We ‘then ‘evaluate GPT-3; on’ these clean ‘benchmarks, ‘and compare to’ the original ‘score. If the: score, on:the: clean.
‘subset is: similar to the score: on, the entire:dataset; this suggests thatycontamination, ‘even if present; does not have. a,
significant effect on reported results., If the. score on the clean, subset-1s lower, this suggests.contamination may be.
inflating the*results.. The:results are summarized.in-Figure'4.2.. Although potential:contamination is often high (witha.
quarter of benchmarks scoring over.50.%); in:most cases performance changes only ‘negligibly,.and ‘we; see:no evidence
‘that contamination level and performance. difference ate.correlated. We-conclude that either our conservative-method
‘substantially overestimated contamination or that contamination‘has little effect on performance,
Below; we review in more‘detail the: few: specific: cases where: either (1); the model. performs significantly: worse on,
the cleaned version,.or (2) potential contamination is'very high, which:makes measuring the performance difference
difficult.
Our analysis flagged six grotips of benchmarks for further investigation: Word Scrambling, Reading Comprehension,
(QuAG, SQUAD2, DROP), PIQA, Winograd, language modeling tasks.(Wikitext-tasks, BW), and German to English.
31


--------------------------------------------------


--- 第 32 页 ---

130% +                                                                                                                     ...eval_on- only’

8    20%.                                                                                    did better

ax

2a    10%       —

[aGaCe)               Symbol Insertion                                                                                    .               :                                  :

sr    08%    sk‘                  ‘oO p.          5   9.           ©   anne     mn ney:

oo         a                                    ,                 AG)                    —            a

a5 TS       Saal me po FT

Se | SQuadve                            ‘windgrad PIQAT LP en OG

Gi 2  40% SS  a M   6 ensde  /_-Anagrams:27~_- 8.

gL

g      20%        O=DROP                                                                                                      ‘eval on-all' data

30%                                                                                                                     y did betier:
0%.                      25%                      50%                      75%:                     {00%
Percentage’of Data‘Clean:in Dataset

Figure 4.2: Benchmark contamination analysis “We constructed cleaned versions ‘of each:of our benchmarks to
check for potential contamination in ourtraining’set. The-x-axis is a-conservative lower bound for how much of the.
dataset is:known'with high.confidence to be:¢lean, and;the y-axis’ shows’ the difference in performance when evaluating’
only on: the-verified clean, subset: Performance:on most benchmarks changed negligibly; but some were flagged for
furtherreview. On inspection we find some.evidence for contamination of the PIQA and Winograd results, and’ we mark
the corresponding results:in Section 3’ withan: asterisk. We'find:no evidence that other. benchmarks are affected.
translation. Since our overlap analysis‘is.designed ‘to be.extremely conservative, we expectiit to produce: some false.
‘positives: Weisummarize'the results foreach:grouprof tasks: below:

 Reading Comprehension; Our:initial analysis. flagged +90% of task:examples‘from QuAC, SQUAD2, and.
DROP. as potentially contaminated, so large that even measuring ‘the differential:on aiclean: subset'was difficult.
Upon:manual inspection; however; we:found that for every: overlap-we inspected; in,all 3 datasets, the:source:
text Was present in Our training data but the question/answer pairs ‘were not, meaning the iodel gains only
background. information.and.cannot:memorize'the answer toia specific question.         .                 ~            -

‘« German:translation: Wefound 25% ofthe examples in:the WMT.16:German-English test set were:marked,
as potentially contaminated, ‘with an associated total effect size of’ 1-2 BLEU. Upon inspection, none of the.
flagged examples contain’paired sentences:‘resembling NMT ‘training data‘and.collisions were.monolingual.
matches mostly’ of'snippets:of events:discussed in the:news..

© Reversed Words and Anagrams: Recall that these tasks are of the form “alaok = koala”. Due to the.
shortilength ofithese:tasks, weused:2-grams' for filtering: Ggnoring’ punctuation). Afterinspecting the flagged.
overlaps; we found.that:they’were:notitypically.instances of:real reversals: orunscramblings:in the:training: set,
but rather palindromes of trivial wnscramblings, 6g “kayak = kayak’. Thé amount of overlap was small,
but removing the trivial tasks.Jead'to an increase in difficulty:and-thus.a spurious signal, Related to'this, the.
symbol insertion: task: shows ‘high overlap but no effect on performance — this is because that:task-involves
removing non-letter characters from.a word, and, the-overlap analysis:itself ignores; such.characters; leading to:
many spurious:matches.           :

‘ PIQA: The:overlap: analysis flagged 29% of examples as contaminated, and observed ‘a 3 percentage point:
absolute decrease (4% relative decrease) in performance‘on ‘the clean subset: Though the’ test:dataset was
released. after our training set was created and dts Jabels are hidden, some’ of ‘the ‘web pages’ used by. the.
crowdsourced dataset creators:are contained:in our training’set:, We found:a similar. decrease ina 25x smaller’
model, with: much. less capacity: to memorize,. leading ‘us; to: suspect ‘that the ‘shift :is; likely, statistical ‘bias
rather than memorization; examples which workers copied may simply be easier, Unfortunately, we cannot:
rigorously*prove this hypothesis. We therefore:mark:our-PIQA results'with an asterisk to denote.this potential.
contamination.

* Winograd: The overlap-atialysis flagged 45% of exaniples,,and founda 2.6% decrease in perforiiance:on the.
clean/subset. Manual inspéctionof the overlapping data point showed that: 132° Winograd schemas ‘were in.
fact present:in our'‘fraining set, though.presented ina different.format:than we:present the:task:to:the:model.
Although the decrease in petformanice is small, we iiiark our Winograd results.in the main paper with an,

32,


--------------------------------------------------


--- 第 33 页 ---

‘ Language modeling: We found the ‘Wikipedia,language modeling; benchmarks measured in,GPT-2, plus’ the
Children’s Book Test dataset, to be almost entirely contained i. our traiming data. Since we-cannot reliably
extract a clean subset here, we do not report results on these datasets, even though we intended to’ when starting
this:work:, We:note:thatPenn Tree:Barik:due to ‘its age*was:unaffected:and therefore: became our'‘chief language
modeling benchmark,.

We also inspected datasets: where: contamination. was high, but the:impact:on performance was:close'to zero; simply:
to: verify: how mich actual contamination existed. “These appeared to often contain false-positives. They had éither
no actual contamination, or had:contamination that :did.not. givetaway: the:answer to the task. ‘One‘notable‘exception.
was LAMBADA, which appeared to have substantial)genuine contamination, yet theimpact on.performance: was very’
small, with the Clean subset scoring’ within 0,5% of the full dataset. Also, strictly’ speaking, our fill-in-the-blank format
‘precludes the simplest form-of memorization. Nevertheless,, since we:made ‘very large gains .on LAMBADA in-this
‘paper, the: potential contamination is noted in the results; sectiom.,

An important limitation of our contamination analysis\is-that we cannot be sure that the.clean,subsetis.drawn from the.
‘same:distribution as the original. dataset. .It-remains possible that:memorization inflates results but at:the same'time:
is precisely counteracted by some statistical bias causing the clean subset to’be easier; However; the sheer number
of shifts close to,zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for siiall,
‘models, which are‘unlikely to be:memorizing.

Overall, we have made a.best effort‘to: measure and document'the effects of data.contamination; and to note or outright
remove problematic‘results, depending on‘ the:severity, Much work.remains to'be done to’ address this importantand.
subtle issue for'the field:in general, both:when designing benchmarks and when'training models. For a:moreidetailed.
explanation of our:analysis, we réfer'the reader'to. Appendix C,

5 Limitations

GPI-3 and our analysis of it have ia number of limitations. Below we describe’ some of these and suggest directions for
future work.

First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to: its direct
predecessor GPT-2, it still has notable weaknesses in.text’synthesis‘and.several NLP tasks., On‘text synthesis, although.
the overall quality:is high; GPT-3:samples still sometimes repeat themselves semantically: at the:document level, start to
José Coherence over sufficiently long passages, contradict themselves, and occasionally:contain non-sequitur sentences
‘or paragraphs.. We will:release:a.collection‘of 500 uncurated.unconditional. samples to help:provide a better senseiof
‘GPT-3’s limitations and:strengths at text: synthesis., Within the domain. of discrete-language tasks; we have noticed.
informally that GPT-3 seems ‘to have. special difficulty’ with, “common serise physics”, despite doing well on some.
datasets (suchi as.PIQA: [BZB* 19]) that:test this: domain. Specifically;GPT-3 has difficulty, with:questions of the type.
““TfI_put:cheese into the fridge, ‘will it:melt?”.. Quantitatively; GP-T-3’s in-context.learning performance has some notable
Gaps Of Our suite Of benchmarks, .as described in Section 3, and in:particular it does little better than chance when.
evaluated one-shot.or even -few-shot on-some “comparison’ tasks, such ‘as determining if two words:are used the:same.
way ina sentence,,or if one) sentence implies;another'((WIC and.ANLI respectively),.as ‘well:as on a‘subset of'reading:
comprehension ‘tasks; This is especially striking given: GPT-3’s strong-few-shot:performance on:many-other'tasks:
GPT-3 has’ several structural and’algorithmic limitations, which ‘could account for some ofthe issues'above. We focused.
on exploring in-context learning behavior in, autoregressive: language models ‘because’ it is straightforward. to both,
Sample and-compute likelihoods with this model class. As.aresult our experiments do notinclude-any bidirectional.
architectures’ of other: training: objectives such as denoising. ‘This is a noticeable difference from:nuch.of the recent:
literature, which has documented: improved. fine-tuning: performance: when using ‘these’ approaches. over’ standard.
langage models [RSR*19]. Thus-our design decision cores at the cost of potentially worse performance on tasks
which empirically benefitfrom bidirectionality. This'may‘include fill-in-the-blank'tasks, tasks’ that:inyolve looking: back:
and comparing:two:pieces: of‘content, or tasks:that:require:re-reading or carefully:considering a‘long passage:and then,
gerierating a'very short answer, This could bé a-possible explanation for'GPT-3’s lagging few-shot' performance on a,
few of the tasks, suchas WIC (which involves comparing ‘the useof a’word in two sentences), ANLI (which involves
‘comparing two'sentences to’ see: if one implies the other), and:several:reading: comprehension.tasks (e.g. QuAC and.
RACE); We-also conjecture, based:on past literature; that.a large: bidirectional model: would be stronger at.fine-tuning:
than-GPT-3, Making a bidirectional model at the-scale of GPT-3, and/or trying‘to make bidirectional models work with.
few- or'zero-shot!learning, is a’promising direction for future:research, and.could help achieve’ the “best of both*worlds”.
A more fundamental limitation,of the general approach. déseribed in this paper’ sealing up any LM-like-model, whether
‘autoregressive or bidirectional — is that itmay eventually:run into (or could already be running into) the:limits ofthe.

33


--------------------------------------------------


--- 第 34 页 ---

‘pretraining objective:. Our‘current objective weights every token equally, and lacks,a notion.of what is most important to
predict and whats less important. [RRS20] demonstrate benefits of customizing prediction to entities of interest. Also,
with self-supervised objectives, task specification-rélies on-forcing the. desired task into.a prediction problem, whereas
ultimately; useful language’ systems:(foriexample:virtual:assistants) might:be’better thought of.as taking: goal-directed.
actions rather than just:making predictions. Finally, large pretrained language:modelsiare not,grounded in other domains.
‘of experience, ‘such as’ video ‘or real-world physical interaction, ‘and thus’ lack:a large-amount of context about'the world.
[BHT 20]. For. all. these reasons, scaling:pure self-supervised prediction is likely’to hit'limits, and-augmentation:with a.
different approach is'likely:to’be:necessary; Promising futurerdirections in this vein: might include learning therobjective
function from humans [ZSW*19a], fine-tuning’ with reinforcement learning; or adding additional modalities such as
images to:provide grounding‘and a better. model of the:world [CLY“19].                   -
Another limitation broadly shared by language models:is-poor sample efficiency during pre-training, While GPT-3
takes a step towards test-time‘sample efficiency closer to that of humans’ (one-shot-or zero-shot), it still sees much ‘more
text:during’pre-training thana human sees in-the their lifetime [Lin20].. Improving pre-training sampletefficiency’is
an:important:direction:for future work; and:might;come from grounding in ‘the physical world to:proyide additional.
information, or from/algorithmic:improvements..                             ,
A limitation, or. at least uncertainty, associated, with few-shot'learning in'GPT-3 is ambiguity about whether few-shot:
‘has Jearmed.during training. These:possibilities exist ona spectrum, ranging from demonstrations: the training set:that:
are: drawn: from exactly the:same distribution as thoseat:test time, forecognizing the:same task but-inia different format,
to adapting to a. specific styléof a getieral task such as QA, to learning a skill entirely de: novo. Wheré'GPT-3 is on.
this ‘spectrum may also'vary from task’to task, Synthetic tasks such/as wordscrambling or defining nonsense words
seem.especially:likely-to’be learned:deinovo, ‘whereas translation clearly ‘must be learned:during:pretraining, although,
possibly-from data’that-is very-different'in:organization:and style than the:test data: Ultimately; itis not even clear what:
humans learn.trom scratch vs from priordemonstrations, Even organizing diverse demonstrations during pre-training
and identifying:them at test time:would be an advance:for language models, but:nevertheless:understanding ‘precisely:
how:few-shot learning works is: an important:unexplored direction: for futurectesearch:.
A limitation dssociated‘with models at theiscale of'GPT-3,régardléss:of objective furiction of algorithin, is that they aré.
bothi expensive:and.ineonvenient to;perform.inference on, which:may. present.a challenge for‘practical:applicability: of
models: down toa manageable size for specific tasks. Large models such.as GPT-3 contain-a very wide range of skills,
most of which are:not.needed for'a specificitask;. suggesting‘that in principle aggressive: distillation;may be possible.
Distillation.is well-explored:in general [LHCG19a] but has not been tried;at'the scale of hundred of billions:parameters;
new challenges and opportunities may be associated with applying it to: models of this size:                         |
Finally, GPT-3 shares some limitations common:to:most deep learning systems;—its decisions arenot easily: interpretable;
it is notwecessarily well-calibrated in its predictions on, novel inputs. as observed by the mich higher ‘Vatiance in.
‘performance than humans’ on/standard benchmarks, and it:retains the’ biases of the' data it has’ been trained on’. “This
last issue —-biases:in the data:that maylead the modelLto ‘generate: stereotyped.or prejudiced content —is of special.
concern from 4 Societal perspective, anid will bé disciisséd along With.other issues in the next séction on Broader Impacts
6 Broader Impacts
grammar. assistance; game) narrative generation, improving search:engine responses, and. answering: questions. But:
they-also have-potentially harmful. applications. GPT-3 improves the quality: of text. generation aid adaptability over
‘smaller:models:and increases the difficulty of distinguishing ‘synthetic text from human-written text, It therefore has the
potential'to advance both’the: beneficial and:harmful applications’ of‘ language;models:
Here we focus on the potential harms of improved language models, not becatise'we believe the harmis ate necessarily
greater, but in-order'to stimulate efforts to-study and: mitigate them: The broader impacts.of language models.like'this
‘are numerous. We'focus on‘two primary’ issues: the potential for deliberate misuse:of language:models like GPT-3 in.
Section 6:1,,ahd issues of bias; fairness, and representation within models like:GPT-3 in Section 6.2, We alsobriefly:
discussissues of energy efficiency: (Section-6.3).

34


--------------------------------------------------


--- 第 35 页 ---

6:1 Misuse of: Language:Models
Malicious uses ‘of language modelsican, be somewhat difficult to,.anticipate’ because: they ‘often, involyerepurposing
language models in-a very different environment or for a:different purpose than researchers intended. To help with this,
weican think in'terms‘of traditional security: risk assessment:frameworks, which outline key: stepsisuchias identifying’
threats: and. potential, impacts; assessing likelihood, and determining risk:as ‘a combination of likelihood,and impact:
[Ros12].. We discuss three factors: potential misuse applications, threat actors, and external-incentive structures.
6.1.1 Potential Misuse Applications
Any socially harmful activity that‘relies:on generating 'text:could'be augmented. by powerful language:models.. Examples.
include:misinformation,, spam, phishing; abuse of legal and governmental processes, fraudulent:academic essay. writing
and social! engineering pretextifis. Many of these applications bottleneck on humiari beings to‘write sufficieritly high,
quality. text.. Language models that:produce high quality ‘text;generation could lower. existing: barriers to:carrying ‘out:
these activities and increase their efficacy,
The misuse potential of language models increases as the-quality of text synthesis improves. Theability of GPL-3 to
generate: several paragraphs of synthetic content'that people:find difficult to: distinguish.from human-written text in,
3.9.4 represents ‘a concerning milestone.in this regard.
6.1.2 Threat:Actor Analysis
Thiéat-actors can bé-organized by skill and resource levels; ranging from low or:moderately skilled:and resourced actors
who may be able to build.a malicious product to “advanced persistent threats’ (APTS): highly’skilled and well-resourced.
(eg. state-sponsored) groups’ with long-term agendas [SBCT19].
To understand how low and mid-skill actors think about language models, we have been monitoring foruiis.and,chat
groups where ‘misinformation tactics; malware distribution,, and.coiiputer fraud are:frequently discussed. While we:did.
find significant discussion of misuse following the initial release:of GPT-2 in spring of 2019, we found.fewer ‘instances,
of experimentation and: no sticcesstul deployments since then, Additionally, those mistise discussions were.corrélated.
with-meédia Coveragerof language: model technologies. Fronithis, we.assess that the threat-of misusefrom these actors‘is.
not.immediate; but significantimprovements in:reliability. could change this:
Becatise-APTs donot typically discuss operations:in the open,‘we have consulted with professional threat analysts about
possible:APT activity involving the use.of language models. Since’the:release of GPT-2 there:has been’ no discernible.
difference in:operations that.may: see potential gains, by using language; models.. The assessment was;that language:
models may fot be worth investing significant resources in because there has been no:convincing demonstration that!
‘current language models are’ significantly better than-current methods ‘for generating: text, ‘and. because methods for
“targeting” or'“controlling” the content:of language models are still at avery early stage.
61.3 External, Incentive Structures’
Each threat;actor group also:has: a:setof tactics, ‘techniques; and, procedures (TTPs):that-they-rely: on to:accomplish their
agenda, ‘TTPs ate influenced’ by economic factors like scalability and ease‘of deployment; phishing is extremely popular
‘among all groups because‘it offers a:low-cost, low-effort, high-yield method of deploying malware and stealing'login.
credentials; Using language models to:augment:existing TTPs would likely result in,an:even.lower:cost of: deployment.
Ease:of use‘is‘another significant incentive. Having stable infrastructure has a large-impact:on the adoption.of TTPs.
The:outputs of language models’ aresstochastic, however, and though:developers ‘can constrain/these*(e.g. using top-k:
truncation) they are not-able‘to perform, consistently without human feedback. If a,social media disinformation bot’
produces outputs that aré‘reliable 99% of the timie, but-produces incoherent outputs | %:of the time, this.could reduce the
amount.of human labor required in operating this bot. Butia human-is still needed .to filter the:outputs, which restricts
how:sealable’the operation can,be:
Based.on ‘our analysis of this model. and analysis of threatsactors arid ‘the landscape, we suspect.AT-researchers will.
eventually develop language models:thatare sufficiently consistent:and steerable’ that they will be of greater‘interest to
malicious actors. Weexpect this will introduce challenges for the broader research community, and hope'to work on.
this through-a combination-of mitigation research, prototyping, and coordinating: with other technical developers.

35:


--------------------------------------------------


--- 第 36 页 ---

6.2 Fairness; Bias; and Representation
Biases:present in training: data may lead models fo: generate stereotyped or prejudiced. content. This’ is; concerning;
since model bias could harm people in the relevant groups indifferent ways by entrenching existing’ stereotypes:and.
producing demeaning portrayals:amongst:other’potential-harms.[Cral7]. We-have conducted an analysis of biases in.
‘the model in:order:to better understand.GPT-3’s limitations when it comes:to fairness, bias, and.representation. ®
Our goal is not-to exhaustively ‘characterize:GPT-3, but to give a preliminary analysis of somie of its limitations and.
behaviors. Weifocus on’biases'rélating to gender,:race, and religion,, although.many other categories of bias’ are:likely’
present and'could be studied in: follow-up*work. This is\a:preliminary analysis and does not reflect:all of the:model’s
biases even Within the studied categories.
Broadly, our analysiscndicates that internet-trained:models.have internet-scale: biases; models tend to reflect stereotypes
present:in their training data. Bélow-we discuss our:preliminary: findings of bias along therdimensions:of gender;race,
and réligion, We'probe:for bias inthe 175 billion parameter model and also in‘similar smaller'models, to seecif and: how
they‘are different in:this dimension.                            ,
6.2.1 Gender
In our investigation of gender bias:in GPT-3,we focused on associations between gender .and occupation, We found.
that:occupations!in general-have a.higher probability of being followed!by. a male:gender identifier than’a femalerone:
(Gn other words, they are:male leaning) when.given a‘context such as "The: {occupation} was, a" (Neutral Variant).
83% of the! 388 Occupations we tested'were more likely to be followed by-a male identifier by GPT-3. We measured.
this by feeding the model’ a context such as:\"The detective was a and then’ looking’ at the probability of ‘the:
model.following up withmale indicating words (eg. man, male etc!): or-female indicating words (woman, femalerete.).
In particular, occupations demonstrating higher levels:of education such as legislator, banker, or-professor emeritus
were heavily:male leaning along with occupations’ that require.hard physical labour such as mason, millwright, ‘and.
‘sheriff., Occupations that were:more’ likely tobe; followed by female identifiers:include-midwife, nurse, receptionist;
housekeeper'ete,
We also tested how’these probabilitiesichanged when'we shifted:the contextto beithe "The competent {occupation}
was; a" (Competent, Variant); and when.we shifted the context to‘be “The incompetent; {occupation} was a"
Cneomipetent Variant) for each occupation, in, the dataset, We found ‘that, when, prompted with "The competent
{occupation} was ia," the majority of occupations: had. an even higher’ probability: of ‘being: followed by: a.
male identifier, than: a female ‘one: than. was ‘the: case- with; our original, neutral prompt,.'"The {occupation} was
a", With the prompt "The incompetent {occupation} was a" the majority of occupations still leaned: male.
with a similar probability’ than for our original neutral prompt. ‘The ‘average occupation’ ‘bias: - measured. .as
ais Shavelog( BECO) — was SLT forthe Neutral Variant, -2:14 for the: Competent Variant and —1.15
for the-Incompetent’Variant.
‘We also carried out-pronotin resolution, on, the Winogender dataset [RNLVD18] ‘using’ two methods which further
corroborated the model’s ‘tendency: to associate: most occupations with males. One method measured. the :mod-
els, ability to correctly assign.‘a pronoun'as the occupation. or. ‘the: participant. For example; we: fed the model,
a CONTEXT stich. ds"The advisor met with, the advisée because she wanted to get advice about job.
‘applications. ‘She? refers ‘to the and found the:option withthe lowest:probability between the'two'possi-
ble options:(Choices between Occupation Option: :advisor;.Participant Option: advisee)..
Occupation anid participant words often have societal biases associated ‘with them suchas the assumption that most:
occupants’ are:by default male.. We found that the language models learnt some:of these biases‘suchias a'tendency to.
associate female pronouns with. participant positions: more'than.malepronouns.. GPT-3 175B; had the‘highest:accuracy: of
all'the models (64,17%) om, this task. It was also'the only’ model where the accuracy for Occupant sentences’ (sentences
where.the correct-answer'was the Occupation option) for females was higher than:for males (81..7%'vs 76.7-%).. All.
cother:models had ‘a higher, accuracy: for‘male pronouns with: Occupation.sentences as compared to:female pronouns
withthe éxception of our second largest model- GPT-3 13B = which had/the same accuracy (60%) for both: This offers.
‘some ‘preliminary*evidence’that‘in places wherevissues of bias can:make language:models susceptible toverror, the larger
models; are more robust;than: smaller models,
‘We also perforined.co-occurrence tests, ‘where we-analyzed which words are likely to.occur in the Vicinity of other pre-
selected words. ‘Wei created a:model output:sample set.by: generating: 800, outputs of length 50.each:with a.femperature:
‘Evaluating faimess, bias, aid.representation. in latigwave nisdels isarapidly:developing. area’ with a large bodyiof priotwork.
‘See,;for example, [HZJ* 19, NBR20, SCNP19].


--------------------------------------------------


--- 第 37 页 ---

Table 6.1: Most Biased Descriptive Words.in 175B Model,

‘Top 10: Most:Biased Male Descriptive Words with:'Raw ‘Top-10.Most Biased Female Descriptive: Words with Raw’

‘Co-Occurrence'Counts:                                                           Co-Occurrence-Counts

‘Average Number‘of. Co-Occurrences Across All Words: Average:Number of Co-Occurrences Across.All Words:

Large (16):                          Optimistic:(12)

Mostly:(15)                                                                                      Bubbly: (12)

Lazy (14)                                                                                          Natighty\(12)

Fantastic (13)                                                                          Easygoing (12)

Eccentric (13)                                                                          Petite (10)

Protect'(10)                        Tight (10)

Jolly: (10)                                                                                     Pregnant (10),

Personable:(22)                      Sucked (8)

Survive (7),                                                                       Beautiful (158)
of 1.and:top.p of 0.9 for every prompt in our dataset, For gender, we had prompts such as"He was very", "She
was: very", "He would be described. as",,"She would.’be described as". We looked.atithe adjectives:and,
adverbs inthe top 100'most favored. words-using an;off-the-shelf POS tagger [LB02]., We found females:were more:
often described using appearance-oriented words such as “beautiful” and “gorgeous” as compared to men who were
more often described.using adjectives'that span:a‘greater spectrum..              ——                      -
Table 6.1 shows: the:top 10:most.favored, descriptive words for the model, along with the:raw: numberof times each,
word ¢o-occurred ‘with a-pronoun indicator. “Most-Fayored’ here indicates words’ which’ were mostiskewed towards a.
category. by‘co-occurring with it at a-higher rate as:compared to the:other category. To put‘thesenumbers in:perspective,
we'have-also included the average for‘the numberof co-occurrences;across all qualifying: words for each gender:
62:2 Race
To investigate racial, bias in GPT-3, we seeded the: model, with:prompts such as\- "The: {race} man, was very",
"The {yace} woman was! Very" and."People Would, describe the {race} person, as" and generated 800
samples:for each of theibove prompts, with. {race} replaced with a.term.indicating:.a racial category’ suchias ‘White:
or Asian. We;then measure word co-occurrences:in the generated. samples. Given prior-research demonstrating that:
how race impacted sentiment. We measured sentiment:using Senti. WordNet [BES10] forthe words' which:co-occurred.
disproportionately: with each:race., Each, word sentiment varied from: 100'to:-100, with:positive scores:indicating positive
-87.5) and @ score of.0 indicating neutral words (eg. sloping, chalet).
Tt should be noted ‘that:we: were explicitly prompting ‘the models to talk about:race and'this:in turn.generated text, that:
an-experimental setup where they have'been:primed to:do so. Additionally, since we are:measuring sentiment by’simply’
looking at word co-occurrences; the:resulting: sentiment can reflect socio-historical factors - for'instance, text relating to
a discussion of slavery will frequently have a negative sentiment, which may lead to .a demographic’ being associated.
with.anegative sentiment.under this testing:methodology.                                                          —_
Across the:models, we analyzed, ‘Asian’ ‘had,a consistently high sentiment:- itranked 1st'in 3:out:of 7 models; On the.
other hand,."Black’ had a consistently low sentiment - it'ranked the lowest in 5: out of 7 models. These: differences
narrowed. marginally on the ‘larger model ‘sizes. This: analysis gives ‘a sense -of the! biases ‘of different models ‘and,
highlights the need for:more:sophisticated analysis of the relationship, between:sentiment; entities, and input:data;

°We only used maleiand ‘female pronouns: This:simplifying assumption:makes‘itieasier to study co-occurrence sincetitdoes not:
require’ the'1solation: of.instances in‘ which “they’ refers to a singular noun:fromi’those*where it didn’t, but other forms of gender bias,
aré.-hkely present and-coilld be studied. using: different approaches.
37


--------------------------------------------------


--- 第 38 页 ---

56                                 Sentiment.Across: Models
-— Asian
"= Black
*°                                                Ir.           — White
B       —_  ——_ Middle eastérn
a,
350M 760M 1.38    207B    :6.7B:    13B    A75B
Model 'sizé;
Figure 6.1: Racial.Sentiment Across:Models:
_ Religion       Most-Favored Descriptive: Words                                                                              ;
Atheism. —“Theists’, ‘Cool’, “Agnostics’, ‘Mad’, “Theism’, ‘Defensive’, “Complaining”, ‘Correct’, ‘Arrogant’,
           “Characterized”
~ Buddhism    “Myanmar”, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, “Monk’,. ‘Japanese’, ‘Reluctant’, “Wisdom’,. ‘En-
lightenment’, “Non-Violent”
Christianity. “Attend”, ‘Ignorant’,.‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’; ‘Egypt’, ‘Continue’, “Gom-
iments’, ‘Officially’
Hinduism. “Caste”, ‘Cows’, ‘BIP*, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated®, ‘Africa’
Islam        “Pillars’, “Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, “Source”, “Charities”, ‘Levant’, ‘Allah’, ,
“Prophet?
Judaism:      “Gentiles?, ‘Race’, “Semites?, ‘Whites’, “Blacks”, ‘Smartest?,, ‘Racists’, ‘Arabs’, “Game, “Russian”  ,
Table 6.2: Shows'the ten riost.favored words about each*religion in.the GPT-3 175B:model.
6.2.3 Religion
We studied which words co-occurred with'religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam,
and Judaism, ‘by. generating ‘800. model. outputs’ of length. 50 ‘with ‘a temperature of 11. and ‘a top:p of 0.9 for every’
prompt. Our prompts wererof the nature;"{Religion practitioners}. are" (Eg. "Christians are") foreach,
‘Of thesixfeligious categories listed above. We then allowed the model tonaturally carry otit completions and created a,
‘corpus.of Such completions:for studying co-occurrence of words..
The-following is.an example:outpuit from'the models
"Buddhists are divided. into ‘two main. branches - Theravada. and Mahayana. Theravada
is the more conservative branch, centering on ‘monastic life and. the earliest sutras
and refusing to recognize the later Mahayana sutras as authentic."
Similartotace, we found that theaiiodels make:associations withfeligious terms that indicate. some propensity toreflect
how these terms are Sonietimes presented “in the:world.. For’éxample;, with-thetreligion Islam, we found that words such.
‘as ramadan, ‘prophet; and:mosque co-occurred at a.higher rate;than:for, other:religions. We'also found that words such.
as violent, terrorismand terrorist co-occlired at’a greater rate with Islam than with other religions and Were in,
‘the top 40 ‘most favored words.for Islam:in GPT-3.
38


--------------------------------------------------


--- 第 39 页 ---

6.2.4 Future:Bias and Fairness Challenges
‘We have presented thispreliminary analysis to share:some:of the biases:we found in:order-to. motivate further research;
and to highlight the inherent difficulties’in characterizing biases in large-scale generative models; we expect this to beian.
area of continuous‘research for'us:and are excited to discuss:different:methodological approaches.with the:community.
We view the work in this section as subjective -signposting-- we chose: gender; race; and religion.as a starting-point, but:
we'recognize’the inherent subjectivity in-this choice. Our work is'inspired by the literature on-characterizing model.
attributes to develop:informative labels such as. Model ‘Cards for Model Reporting from: [MW2Z?18].             :
Ultimately, itis important notqustito characterize biases in language systems but to intervene. The literature on this
is also extensive [QMZH19, HZJ~ 19], so'we offer only a:few brief comments on future directions specific to-large.
Janguage:models. In order tozpaverthe way: for‘effective bias prevention:in general. purpose models, therezis a need for
building a:common voeabulary tying together'the normative; ‘technical :and empirical challenges of bias:mitigation for
these models, There is:room.formore‘research that engages with the literature outside NLP, betterarticulates'normative
‘statements:about:harm, and engages’ withthe lived:experience‘of communities affected. by NLP systems [BBDIW20].
Thus; mitigation work should not'be approached purely. with:a mettic:driven;objective to ‘remove’ bias as’this has been.
6.3 Energy'Usage
Practical large-scale pre-training requires large amounts of Computation, which is energy-intensive: training the’GPT-3
175B consumed several thousand petaflop/s-days.of compute during pre-training, compared to tens of petaflop/s-days
for.a.1.5B parameter’GPT-2 model:(Figure 2.2). This-means we:should.bercognizant ofthe cost and:efficiency: of such,
models, as-advocated by [SDSE19].
Theruse of-large-scale!pre-training also: gives another. lens’ through:which.to. view the efficiency of-large:models'- we:
should consider:not only: the: resources: that igo into training them, but how: these, resources. are: amortized, over’the:
lifetimé:of a model, which'will subsequently beused.for.a variety:of purposes.and fine-tuned for specific tasks, Though,
modelslike:GPT:3 consume significant resources during:training, they‘can be surprisingly efficient once trained: even.
with:the fullGPT-3 175B, generating 100 pages of content froma trained model can.cost onthe order of 0!4:kW-hr, or’
only afew centsin energy costs. Additionally, techniques like model distillation [LHCG19a] can‘further bring down,
the costof such models, Jetting’us.adoptia paradignt-of training: single, large-scale models, then:creating:more efficient:
versions.of them.foruse:in appropriate:contexts., Algorithmic progress may: also naturally: further, increase: the:efficiency
Of Such iiodels overtitne, similar to trends observed in image recognition,and neural machine translation, [HB20].
7 Related. Work
Several lines of. work: have focused. on increasing: parameter count: and/or computation. in. language. models ‘as a.
‘means ‘to; improve generative: or*task: performance. An early: work scaled LSTM. based language models: to: over’ a.
billion parameters [JVS*16]. One line of Work straightforwardly increases the size of tratisformer models, scaling
up parameters and FLOPS-per-token‘roughly in’proportion. Workin this vein has ‘suecessively:increased model size:
213. million:parameters [VSP 17] in ‘the original:paper, 300 million ‘parameters |[DCLT18], 1.5. billion, parameters
{RWC+19], 8 billion parameters [SPP+19], 11. billion:parameters [RSR*19]; and most recently 17 billion.parameters
[Tur20]. A second line: of work has focused on’ increasing, parameter count but not computation, as. a mearis of
‘increasing models’ capacity: to store:information: without increased computational cost.. These approaches:rely: on’ the:
conditional computation framework: [BLC13] and specifically; the:mixture-of-experts:method [SMM* 17] has been.
used.to produce: 100 billion-parameter models and more:recently 50 billion parameter translation models [AJF19],
though.only.a small fraction of the parameters: are! actually: used on’ each forward pass. A third, approach increases
computation without increasing*parameters; examples:of this.approach include adaptive:computation time [Gra6]|and.
the Universal transformer [DGV 18]. Our work:focuses on the first approach (scaling compute anid parameters together,
by’ straightforwardly making; the:neuralinet larger), and increases :model size 10x beyond.previous:models’ that employ’
this strategy,
Several efforts have! also systematically. studied. the effect of scale. on. language model. performance. [KMH*20,
RRBS19, LWS*20, HNA*® 17], find:a smooth power-law trend in loss as’ autoregressive language models are'scaled ‘up.
This work suggests that’this trend largely continues as:‘models continue'to scale: up:(although a slight bending of the:
curve can perhaps be detected:in Figure 3.1), and.we also find relatively’ smooth increases'in many. (though notiall)
downstream tasks:across 3:orders of:magnitude of scaling:                                                                              -             :
Another line of work goestin-the opposite direction from Sealing, attempting to preserve strong performance:in language
models that-are as small as’ possible. This approach includes ALBERT [LCG*19] as‘well as general [HVD15] and.
39


--------------------------------------------------


--- 第 40 页 ---

task-specific [SDCW19, TYS*19,, KR16] approaches ‘to distillation. of language models. These architectures and.
of giant:models.        ;                                               .                                      .

As fine-tuned language models: have neared human: performance on many standard ‘benchmark: tasks; ‘considerable:
éffort-has been devoted to constructing more difficult or Operizended tasks, including question answering [KPRT 19,
IBGC*14, CCE*18, MCKS18],:reading comprehension [CHI*18,,RCM19], and adversarially:constructed datasets
designed tobe difficult:for, existing ‘language:models [SBBC19,, NWD*19]. In this work we test:our models on:many
of these datasets.

Many‘previous’efforts:have-focused. specifically: on question-answering,;which;constitutes a significant:fraction of the:
tasks;we tested on..Recent efforts:in¢lude: [RSR*+19; RRS20]; which fine-tuned an 111 billion:parameter language: model,
and [GLT+20],. which focused on.attending over a large.corpus of data at test tinte., Our work differs in focusing’ on.
in-context'learning but could:be combined in the:future with those of [GLT +20, LPP*20]..                                             ,
Metalearning in, language:models ‘has been utilized in [RWC*t19],. though with much. more limited results :and:no
‘systematic study, More broadly, language model metalearning has’ an. inner-loop-outer-loop structure, making: it
‘structurally’ similar’ to ‘metalearning’ as ‘applied.to .ML ‘in general., Here ‘there! is am extensive ‘literature, including
matching networks [VBL* 16]; RL2 [DSC*16], learning to optimize [RL16; ADG* 16, LM17] and MAML. [FAL17].
‘Our approach of stuffing’ the: model’s context with previous examples is most.structurally similar'to RL2 and also
resembles [H¥CO1], in that an:inner loop of:adaptation takes place through: computation.in the model’s activations
across timesteps,. without:updating:the-weights,. while an, outer loop (in ‘this case just Janguage: model: pre-training):
tipdates the Weights,,and implicitly Jeamis the ability to adapt to-or at least recognize tasks defined at inference time.
Few-shotauto-regressive density estimation was ‘explored:in. [RCP 17] and [GWC*18] studied low-resource NMT as
a few-shot Jearningproblem.

While the mechanism:of our few-shot.approach.is different, prior work has ‘also explored ways of using pre-trained.
Janguage’models in;combination with gradient descent to perform:few-shot.learning [SS20]. Another sub-field with.
similar goals is semi-supervised learning-where approaches such.as UDA,[XDH 19] also exploreamethods:of fine-tuning
when very little labeled data is available.                                                                                                                       .
‘Giving multi-task models instructions:in natural language’ was first formalized‘in a supervised setting with [MKXS18]
and utilized for some tasks (such as suiimarizing) in 4 language: model with [RWC* 19]. The notion of presenting
tasks in natural language was also.explored in the text-to-text transformer [RSR* 19], although there it'was' applied for:
multi-task-fine-tuning:rather than.for in-context learning without:weight updates.                   -                           -
Another approach to-iiereasing: generality and transfer-learning capability in latiguage models is multi-task learning
[€ar97], which fine-tunés oni a:mixture of downstream.tasks together, rather than separately. updating theweights for
‘each one. If successful multi-task learning:could:allow a single:model to beused for:many tasks without.updating' the:
Weights.(similar to ourin-context leaming approach); or alternatively could 1mprove saniple efficieney* when updating
the ‘weights for a new task. Multi-task learning: has shown. some ‘promising initial results [UGHT15, LSP*18] and.
‘multi-stage:fine-tuning has:recently becomeza standardized part:of SOTA results; on some datasets [RFB18] and:pushed.,
‘Set-Up’ training curricula, By. contrast pre-training, at ‘large enough scale’ appears to offer a “natural” broad distribution of
tasks implicitly: contained in predicting: thetextitself.. One direction for ‘future work.might be attempting to generate:
a broadersetof explicit‘tasks for multi-task learning, for example through:procedural generation|[TFR*17];,human.
Algorithmic innovation :in language: models ‘over ‘the last two ‘years has; been enormous, including:denoising-based.
biditéetionality [DCLT18], prefixLM [DL15] arid encoder-decoder architectures [LLG*19, RSR*"19], random periniu
tations.during training [YDY* 19], architectures that improve'the efficiency: of sampling: [DY Y *19], improvements in.
data:and.training procedures; [LOG* 19], and efficiency increases in’the:embedding parameters [LCG* 19]. Many-of
these techniques provide significant gains on dowristieam tasks, Inthis work we continue'te focus on pure autoregressive
model implementations.. However, itis very likely that incorporating’ these*algorithmic advances could improve GPT-3’s
performance: on, downstream ‘asks,. especially: in, the fine-tuning setting; and, combining GPT-3’s scale: with these:
algorithmic techniques is a promising’ direction-for future: work.     :

8 Conclusion

We presented, a, 175 billion parameter language. model, which shows strong performance. on, many NLP tasks and,
benchmarks in the zero-shot, one-shot, and few-shot settings, in. some cases nearly: matching the performance of


--------------------------------------------------


--- 第 41 页 ---

state-of: the-art fine-tuned. systems;, as;well asi generating high-quality samples:and:strong qualitative performance ‘at:
tasks defined on-the-fly, We documented roughly predictable trends of scaling in performance without wsing fine-tuning,
We also discussed the:social impacts of this class of model. Despite many limitations and weaknesses, these results
suggest:that very large language models: may. be an:important: ingredient: in the development of ‘adaptable, general.
Janguage:systems:

Acknowledgements

The authors would like to thank Ryan Lowe forgiving detailed feedback on, drafts of the ‘paper: Thanks to.Jakub
Pachocki and Szymon' Sidor for’suggesting tasks,. and Greg: Brockman, Michael Petrov, Brooke Chan, and Chelsea.
Voss for, helping run-evaluations‘on, OpenAT’s infrastructure. Thanks:to David Tuan-for initial support inssealing“up
this project, Irene Solaiian for discussions about ways to approach and, evaluate: bias, Harrison Edwards and Yura,
Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul ‘Christiano: forearly
discussions of: language:model scaling, Long‘Ouyang for‘advising ‘on:the design of the human:evaluation experiments},
‘Chris' Hallacy for discussions! on data collection, and Shan Carter for help with visual design. Thanks to the:millions of
people who created content that was used in the training)of the model,,and.to those who were involved in-indexing or
upvoting the content: (in’the:case of’ WebText). Additionally; we would Jike:to'thank the-entire:OpenAI infrastructure:
and supercomputing teams for:making‘it possible to train models sat'‘this scale:


--------------------------------------------------


--- 第 42 页 ---

Contributions
Tom. Brown;. Ben. Mann, Prafulla Dhariwal; Dario Amodei,. Nick Ryder, Daniel M Ziegler, and Jeffrey Wu.
implemented the.large-scale models,.training infrastructure;.and:model-parallel strategies.
Tom.Brown,.Dario.Amodei,.Ben:Mann, and Nick Ryder. conducted :pre-training’experiments.
Ben Mann arid.Alec Radford. collected; filtered; deduplicated; and conducted :overlap analysis on thetraining data.
Melanie Subbiah, Ben Mann, Dario-Amodei, Jared Kaplan, Sam.McCandlish, Tom Brown, Tom‘ Henighan, and.
Girish Sastry, implemented the downstream tasks:and.the software framework:for. supporting them, including;creation,
of synthetic tasks:
Jared Kaplan:and Sam.McCandlish initially: predicted that a‘giant language:model should ‘show continued. gains, and.
applied scaling laws;to help predict and,guide:model! and:dataiscaling:decisions forthe research,
Ben Mann implemented sampling without-replacement:during training.
Alec Radford originally:demonstrated few-shot learning:occurs in‘language:models.
Jared Kaplan and Sam McCandlish showed that larger models learn more quicklyin-context, and systematically
‘studied-in-context learning curves, task pronipting, and evaliiation:methods..
Prafulla Dhariwal implemented an early version of the codebase; and:developed the: memory, optimizations for fully
half-précision-training.
‘Rewon Child:and Mark Chen.developed an early version of:our. model-parallel strategy.
Rewon Child:and Scott Gray contributed.the sparse transformer,
Aditya,.Ramesh:experimented with loss scaling strategies’ for pretraining.
Melanie‘Subbiah and Arvind Neelakantan implemented, experiniented with, and tested-beam search..
PranaviShyam worked-on SuperGLUEand assisted ‘withiconnections to few-shot-learning and. meta-learning literature.
Sandhini,Agarwal conducted the faimeéss-and representation analysis.
‘Girish: Sastry and Amanda Askell conducted the/human evaluations:of the model.
Ariel Hérbert-Voss.conducted therthreat analysis of malicious use:
Gretchen. Krueger’edited and red-teamed the policy ‘sections:of the paper.
Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and. Christopher Berner
optimized.OpenAL's Clusters to tun:the largest models-efficiently,
Scott Grayideveloped fast GPU ‘kernels7used during training.
Jack Clark led the-aiialysis of ethical impacts-— fairness and representation, human assessments of the model, and.
broader impacts analysis, ‘and advised Gretchen,,Amanda, Girish, Sandhini,/and.Ariel on’ their'work:
Amanda.Askell, Girish Sastry, and Jack Clark wrote.the paper.
‘Sam.McCandlish led-the analysis of model scaling, and advised.Tom Henighan/and Jared.Kaplan: on their work:
Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated,
the benefitof weight:decay for training’,
Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised.Pranay; Prafulla,
Rewon, Alec, and-Aditya on their*work.
Dario Amodei designed,and.led the:tesearch.

42


--------------------------------------------------


--- 第 43 页 ---

A Details of Common. Crawl Filtering:
As mentioned.in Section 2.2, we employed two techniques'to improve’ the:quality of the Common’Crawl.dataset: (1)
filtering Common:Crawlsand.(2); fuzzy :deduplication:

L.. Intorder‘to improve the.quality: of Common Crawl, we developed an automatic’ filtering method to remove low’
quality:documents., Using'the original’ WebText as.a proxy for high-quality:;documents; we'trained.a classifier
to-distinguish these from raw Common Crawl. We-then’tsed this classifier to re-sample Common Crawl by
prioritizing documents:which were:predicted. by. the classifier'to: be higher quality. ‘The classifier‘is trained.
using logistic:regression.classifier-with features ftom. Spark’s standard tokenizeriand‘HashingTE '°. For-the
positive examples, we'tsed a,collection.of curated datasets such as WebText, Wikiedia, and our'web books
corpus as’ the positive examples, and:for thenegative examples, weiused unfiltered:Common Crawl. We'used.
this classifier'to, score Common: Crawl:documents., We kept.each document in our dataset iff’

np» random.paréto(@) S 1 documentuscore
“"We:chosé @i = 9 in order to takemostly docunierits the classifier scored highly; butistilL-includesome documents.
that were-out of distribution. a was chosen'to match'the distribution of scores ftom-our classifier on, WebText.
We fotind this ré-weighting-increased quality as meastired by loss on:a range of out-of-distribution generative
text samples,

2.. To further:improve tnodél quality and prevent overfitting (which becomes increasingly important as model.
capacity increases), ‘we, fuzzily. deduplicated documents: (ie. remoyed,documents ‘with high overlap ‘with.
other doctiments)within each dataset using Spark’s MinHashLSH 1iiplemetitation with (10 hashes, wsitig the.
same features.as were‘used for‘classification above.. We also fuzzily removed. WebText from’Common Crawl.
Overall ‘this;decreased.dataset size:by: an averageof. 10%:                 :

After filtering for duplicates’ and.quality, wealso partiallycrenioved text occurring in-benchinark datasets, described in.
Appendix: C.
B. Details :of Model. Training
To trainiall versions of GPT-3,:we use Adam.with 8 = 0.9, (= 0.95, and ¢ = 10-®, we clip the global:norm of the:
gradientiat 1.0, and'we use cosine decay for-learning rate down: to ‘10% of-its: value, over, 260 billion tokens (after 260:
billion tokens, training continues at 10% of the original learning rate), There is alinear LR warmlip over the first 375
million.tokens. We:also ‘gradually increase the:batch size linearly. from.a small. value: (32k:tokens):to the:full value ‘over
the first,4-12 billion tokens of training. depending on the:model size. Data are: sampled ‘without replacement during
training (ati an, epoch boundary isteached):to minimize overfitting, All models use weight decay of 0.1 to-provide a.
‘small aniount of regularization. [LH17].
During training ‘we always: train on. sequences: of ‘the: full. met: = 2048 token context window; ‘packing multiple:
doctiments into a single sequence when doctiments are shorter than’2048, in order to increase computational efficiency,
Sequences ‘with multiple: documents are not masked. in any: special way but instead. documents within .a, sequence,
are delimited with ‘a special, end of text foken,, giving the language model. the information: necessary ‘to infer, that:
context separated by’ the end of text token is unrelated. This allows. for efficient training without need. for any special,
‘sequence:specific masking.
€  Détails of Test Set Contamination.Studies
In section. 4 we: gave a high level.overview of test:set contamination. studies. In this section we'provide details. on.
methodology. and.results: ©                                                                                                                        -
Initial training’set:filtering; ‘We attempted.tocréinove:text.occurring in benchniarks*fromi training.data by searching’
for .13gram. overlaps between all test/development sets used.in this ‘work: and our training: data, and: we:remoyed.
the colliding 13—gram as well as.a 200 character window around it, splitting ‘the original dociiment into:pieces. For
filtering purposes we'define'a.gram as a lowercase, whitespace delimited word with no:punctuation. Pieces less than.
200 ‘characters ‘long ‘were discarded. Documents: split:into moresthan 10 pieces were; considered ‘contaminated ‘and.

Wittps://spark.apactie. org/docs/late st/api/python/pyspark.ml -html#pyspark.inl'. feature.HashingTF

43


--------------------------------------------------


--- 第 44 页 ---

removed entirely., Originally we removed entire) documents ‘given ‘a single collision, but:that overly penalized.long
doctimenits such as. books for'falsé-positives. An example of a.falsé positive might be.a testset based -on Wikipedia, in,
which the Wikipedia article quotes a‘single line’ from a.book, We ignored 13—grams that matched more than 10 training’
documents, as:inspection showed the majority: ofthese to:contain common cultural phrases, legal boilerplate, or similar
content that weilikely:do want'the model'to learn; rather'than;undesired specific:overlaps with test:sets. Examples for
various frequencies can.be:found in the GPT-3 release‘repository"".
Overlap ‘methodology For our berichmark overlap:analysisin Section 4, weitised:a variable numberof words NV to:
check for‘overlap:for each dataset, where’ NV is'the:5th, percentile example length.in words, ignoring all:punctuation,
whitespace; and casing:. Due'to spurious’collisions:at:lower-values:of ‘N we:useia minimum. value:of'8 on non-synthetic
tasks, For performance reasons, we. set a tiaximum,valueé-of 13 for all tasks., Values for Nv and the amount of data,
marked.as dirty are shown-in Table C:1.. Unlike:GPT-2’s use‘of bloom«filters to compute’ probabilistic bounds:for test:
contamination; we used! Apache:Spark‘to:compute‘exact collisions across all training and’test sets: We compute overlaps.
‘between test sets-and. our full training Corpus, even though we only trained on 40% of our filtered Common Craw],
documents per Section 2.2.                  oe
We define a. ‘dirty’ example as one-with any’ .N-gram:overlap with:any training document; and.a ‘clean’ example as;one
with'no Collision..
Test.and.validation:splits:had similar contamination levels despite:some:testisplits:being' unlabeled. Due to a.bug revealed
by this arialysis, filtering described above failed on long documents such as books. Becatisé of cost’ considerations it
was infeasible to‘retrain the model-on a:corrected version of the training-dataset. As such, several language modeling
benchmarks plus the Children’s Book Test showed almost complete-overlap, .and.therefore were:not:included in.this
paper; Overlaps:are shown: in Table C.1
‘Overlap results ‘To-understand how-much‘having:seen some of the: data helpsthe model perform:on downstream.
tasks, wedilter’every validation.and test'set by dirtiness.. Then we run-evaluation on the clean-only:examples and report:
the relative percent change between the clean:score and the‘original score. If the cleaniscore is'‘more than. 1% or 2%
worse than ‘the overall! score;.it suggests the model may‘have: overfit to ‘the examples it has: seen: If therclean score‘is
significantly better, our filtering scheitie may have preferentially marked easier examples as dirty.
This overlap:metric :tends'to show a high:rate of false:positives for datasets that contain. background:information (but:
not answers) drawn from:the web (such as SQUAD; which, draws from: Wikipedia) or examples less than, 8 words
Jong, which weignored in our'filtering process (except-for wordscrambling tasks). One instance where this technique.
‘seems tosfail to give good signal:is DROP, a:reading comprehension task in which 94% of the examples areidirty:, The-
information.required to:answer'the question is ina passage’proyided.to the:model; so having:seen the passage-during:
training:but not the questions'and ‘answers'does not meaningfully constitute cheating, We confirmed that every matching’
training document:contained only the source passage;,and none of the questions and answers in the dataset:. The:more:
likely:explanation for:the-decrease:in performance is; that the:6%, ofexamples;that remain: after:filtering:come from a.
slightly different distribution,than the ditty exaiiples.                              .
Figure-4:2 shows;that as the dataset becomes'more contaminated, the’variance‘of the: clean/all fraction increases, ‘but:
there is aio apparent bias towards improved or degraded performance. This suggests that GPT-3 is felatively msenisitive
to.contamination, See Section 4 for details on the datasets we flagged for further'teview..                             -

AA


--------------------------------------------------


--- 第 45 页 ---

.                  as                                                                          Relative
a              os                .               Total,            Ditty            ‘Ditty = Glean           Clean Clean          Difference:

‘Name                                  Split Metric: .N <Acc/FI/BLEU Count Acc/FI/BLEU Count Acc/FI/BLEU’ Count Percentage Clean'ys All —
Qua’                                   dev          f1         13            Ad3             7353,             44.3              7315)             54-1.              38)             1%                20%
SQuADy?2,                           dey.          fl          13            69:8             11873             69.9            11136            68.4              BT            6%                =2%
DROP                                  dey          £1         13            36.5:             ‘9536              37.0)             8898)             29,5!              638,            1%!                21%:
‘Symbol Insertion                  dey,         ace        7             66:9.             10000:            66.8              8565!             67.1,             1435,          14%:                0%
‘CoQa'                                  dev          cal         13             86.0             7983              85.3              5107             87.1,             2876          36%:                 1%:
RECORD.                              dev         ace = 13             89:5             10000:            90.3              6110             88.2:             3890           39%:                1%
‘Winogiad.                            test = ace              88:6              273              90.2.              164:              86,2               109,           40%:               3%
BoolQ                                  dey         acei 13             76.0,              32/10°             15:8              1955)             763             A315           40%                0%:
MultiRe:                              dey         acc) = 13            ‘TAD              953               73.4:              558:              75:3)              395           41%:                EG:
‘RACE-h                              ‘test         ace 13,            468              3498              47.0             1580;             4647             ‘1918           55%:                0%:
-LAMBADA:                         ‘test         ace = 13            86:4              5153              86.9:             2209:             86.0             2944           57%:                0%:
WSC                                   dey.         ace; 13             716.9              104               13:8               a2               79.0               62,            60%:                3%
PIQA:                                  dev         acc: 8              82.3              1838              89:9              526              79:3             1312           TAM:                4%.
RACE-m,                             fest acc’ 13            58:5              1436              53.0:              366              604              1070.          15%:                3%
‘Dé En 16                                 test blew-sb 12                43.0                 2999                 a7 A:                 739:                  408                2260             75%:                   5%
En—De 16                                 test blew-sb = 12               30:9                2999                 32.6)                 739:                 29.9.                2260             15%:                    -3%
En-Ro 16                                 fest bleu-sb 12               258                 1999                 24.9                  423.                 26.1,                1576             19%:                     1%:
Ro=En 16                                 fest bleu-sh 12               413                 1999                 404:                 423.                 41.6                AST6,             19%                    1%:
“WebQs:                                test         acc 8             41:5,             2032              41.6               428,              415             1604.          19%                0%
ANLIR1                              ‘test         ace 13,            368              1000:             40.5              200:              35.9              800:           80%:                -3%
ANLI R2:                                  ‘test           ace; = 13              34:0                1000:               29.4:                AIT                35.0.                823)             82%:                   3%:
THVviaQA.         dey = ace 10    TAD    7993    70.8    1390    713    6603   83%:     0%:
ANLI R3:                                    test           ace: 13               40.2                 1200:                38:3                  196!                 40,5!                1004,             84%                    1%:
En Fr 14:                              test bleu-sh 13             39.9               3003               38:3                All                40:3              2592            86%:                  EG:
FroEn 14                                  fest bleu-sb 13               At.                 3003                 40.9                  alt                  ala                2599              86%:                    0%:
WiC:                                    dev         ace = 13            Sia              638              53.1                49               513)              589.           92%:                0%:
RIE.                                    dey ace 13            TAS               277              TA:               21               TLS              256           92%:                0%:
‘CB                                      dey.         ace 13             80.4               56              100.0:               4                78:8               2.            93%               =2%
Anagrams 2,           dey    ace 2     402     19000:     16:2      705°      3A     9295    93%:       -1%.
‘Reversed Words                     dey,          acc’         2                04               10000:              1.5                 660:                0.3                9340,            93%:                -26%
‘OperiBookQA.                      test         ac BB             ‘65:4               500              58.1               31               65.9.              469.           94%:                1%;
ARC(Easy):                         test         ace AL            70:1,             22608             715               89               69.8             2179          96%:                0%:
Anagrams 1                                dey.           acs 2.                 15.0                10000:               49.8                  32],                   13.8                2673)             97%                   =8%
‘COPA:                                  dey         ace 9             93:0               100              100.0:               3                92:8,              ‘OT            971%:                0%:
ARG (Challenge):                        test             ace:          12                 SL6                    1144.                  45.2:                     31                      51.8                   1113,               97%                       0%:
‘HellaSwag                 dev       ace = 13          19:3           10042,          86.2.           152,           19.2           9890,         98%:             0%:
NOs                                     test         ace AL            29:9              3610:             32.7               52               29.8             3558          99%:                0%:
‘Cycled Letters       dey ace    38.6    10000:    20,5     BB     38.7    9927   999%:     0%
SAT Analogies                       dey          acc O.              69.8                374               100;0:                2                  63:6               322;            99%:                  0%:
‘StoryCloze                           fest         ace = 13)             877              1871             100.0:              9                87.6,             1869:          100%:               0%:
Winogrande                         dey,         ace 13.            LA              1267.                -                   ‘0                TAA             1267          100%               0%:

Table €.1: Overlap statistics for'all datasets sorted from dirtiest to cleanest, We consider @ dataset example-dirtyf it

has.a Single N-gram collision with:any document‘in our training corpus.. “Relative Difference:Clean'vs:All” shows'the.

‘percent change‘in:performance between.only the:cleanexamples vs all the examples inthe. benchmark: “Count” shows

‘the nlimber of examples: “(Clean percentage” is the percent of éxaiiples that are clean‘vs'total. For “Acc/F1/BLEU” we

usethé metric specified ‘in “Metric”. These scores come fromevaluations*with a.different-seed for the random examples.

used forin-context learning, ‘and ‘will :therefore:differ slightly from. the ‘scores ¢lsewhere in‘the paper.

45


--------------------------------------------------


--- 第 46 页 ---

D_ Total Compute Used to Train Language Models
This appendix contains the calculations that‘were used to derivethe approximate compute Used to'train, the language.
models in-Figure.2.2. As‘a simplifying assumption; we ignore:the attention operation, as‘it typically uses less than 10%:
cof the:total compute forthe models'we arevanalyzing.
Calculations.can be seen:in Table-D.1. and-are explained within the'table caption.
Fwd:pass               Frac of
compute  Comptte  ‘Pgraing, ‘Traifing’tokens ‘per param’ ‘Muirfor activepafam. ~ for each
‘Model                (PF-days) (flops)         (M)          (billions):       pertoken ‘“bwdpass ‘per:token          ‘token,
‘T5-Small.            2:08E+00 1:80E+20:       60             1,000:              3              3               1                 05
T5-Base             7.64E+00 6.60E+20. 220            1,000:              3              3               i                 05
‘T5-Large.            2O7TEHO1 231E+21 770            1,000:              3              3:               1                 0.5
T5-3B                1.04E+02 9.00B+21 3000           1,000               3              3               1                 05
‘T5-11B              3.82E+02 3.30E#22. 11,000           1,000               3              3               a                 0.5
BERT-Base:          1:89F+00 1;64E+20:      109             250:               6             3               2s                10
BERT-Large  6.16E+00  5.33E#20: 355             250)               6             3               2,                1.0
ROBERTa-Basé = 1.74E+01 = LSOE+21       125            2,000:              6:             3)               2,                1:0
ROBERTa-Larve §=4.03E+01 A26E 421       355            2,000:              6:             3)               2                1:0
GPT:3 Small        2,60E4+00 2.25E#20' 125             300:               6             3               2.                10
GPT-3 Medium  7.42E+00 O6AIEH20: 356             300:               6:             3               2                1.0
GPT3 Large '1.58E+01  137E421 760             300               6             3               2,                1.0
‘GPT3 2°7B         SS52E+0OL ATTES2T 2,650            300:               6:             3)               2                1:0
'GPT-3 13B          268E+02 2.31E#22, 12,850           300:               6             3               2                1:0
Table-D.1: Starting;from the right hand. sideand:moving left, we begin with the number of training tokens thateach.
model, was trained.with. ‘Next we:note that since-T5 ‘uses an encoder-decoder:model, ‘only. half:of the parameters.are
active:for each:token during a:forward or backwards pass: We:then:note that each’token is involved in a single addition,
‘and @ sitigle multiply for'each active paraiieter in the-forward pass (ignoring attention). Then we add a:multiplierof
3x to account for‘the backwards pass (as‘computing both 28422" and, 9428. yse a similar amount of compute as’the
forwards pass. Combining the previotis two numbers, we get the-total flops per parameter per token, We'mnultiply this
value’ by the total training tokens and the total:parameters to yiéld the: number of total-flops used during training:. We.
report both:fiops and petafiop/s-day:(each of which.are 8.64e+19 flops).
E Human Quality Assessment of Synthetic News Articles
This appendix contains details on the experimerits:measuring:humian ability’to distinguish GPT-3-generated synthetic
news articles from real news:articles. We:first describe the experiments,on the ~* 200 word.news articles, and then.
describe the preliminary investigation of ~ 500 word news articles generated. by GPT-3.
Participants? Werecituited 718 ‘unique participants to take partir 6 experiments. 97 ‘participants weré:excluded for
failing an internet check.question, leaving: a total of 621 participants: 343 male, 2:71 female, and 7 other: Mean.
participant agé was ~ 38 years old. All participants were recruited through, Positly;, which maintains a whitelist of
high-performing: workers from Mechanical Turk, All. participants were US-based but there/were no other demographic
restrictions:, Participants: were paid’$12:for their participation, based.on a.task timeiestimate of 60 minutes: determined.
by pilotrtins. Ih order to enstire that the saitiplé:of participants for ach experiment quiz was uniqué,-participants were
not allowed to take part in an experiment more:than once.                                  .
Procedure-and. design: We arbitrarily'selected 25 news;articles that appeared .in newser.com in early'2020: Weused.
the article titles.and. subtitles to produce:outputs from the 125M, 350M, 760M; 1.3B, 2.7B; 6.7B 13:0B, and:200B
(GPT:3) parameter lariguagemodels.. Five outputs per question‘were! generated’ by, each:model.and.the getieration.with a.
word count:closest.to that of the human written article: wasiselected automatically. This:was to: minimize the effect:
that completion length might have on participants’ judgments. The same output procedure for each thodel, with the.
exception of the removal of the intentionally bad control model, as described in-the main’text,
46


--------------------------------------------------


--- 第 47 页 ---

.             _      —                      Average:
a            Participants Participants Genders Mean _ Word Count
Model           Recruited Excluded. (miftother) Age  (human:model)
‘GPT-3: Small                   80                       7                  41:3121            40              216:188,
‘GPT-3.Medium.      80:           7         46:28:2     39:       216:202.
‘GET-3 Large         81           2a        46:28:23       216:200
GPT-3. XL           79           14        32:32:21     38,       2162199
‘GPT-3:2.7B          80.           IL        36:33:0     40,       216:202.
‘GET-3 6:78.         76           5        f628:2 Bk       2160195
‘GPT-3:13.0B                   81                       13                 46:28:2            37              2163209
‘GPT-3:175B                    80:                       9                  42:29:0            37              216:216:
Table E.1: Participant:details and:article:lengths foreach experiment to evaluate human detection of ~: 200 word.model.
generated news,articles. Participants wereexcluded:due‘to:internet check fails.
Average 'time'spent trying to detect model generated news article’
=  |. td  [ td   i  ||
130   =  —   a   a  4
8 120: a |
o.         aan                                           |    |
|  7,          |                  control (105 seconds)
1e8                “Jag:                Je10:               lei.
Number of parameters (log scale).
Figure E.1: Participants! spend more time'trying to identify whether’each news article is machine generated /as model.
'size:inereases:, Duration.on'the control:model:is indicated with:the dashed.line.. Line:of bestsfit is:a linear.model on a.log:
seale with:95% confidence intervals.
In each-experiment, half of the participants were:randomly assigned'to quiz A and half were randomly assigned to quiz
B. Each.quiz consisted of 25 articles: half:(12-13) were.human written.and half (12-13) were model generated: ‘the:
articles with:‘human written completions in quiz A.had:model generated completions in quiz B and vice-versa. The:
order'of quiz question was shuffled for'each participant.. Participants could leave commients.and wererasked.to indicate
if they had seen:the articles’before:. Participants weretinstructed:not'to look-up the articles or their content:during the:
quiz:and,at‘the end ofthe quiz:wereiasked.if they“had lookedsanything up:during the quiz.
Statistical Tests To compare means onthe different runs, we performed a two-sample t-test-forindependent groups for
each model against:the control. This was implemented in Python'using the scipy..stats'.ttest_ind function.. When
plotting a regression line in,the.graph of average participant accuracy vs model size; we fit a power law of the:form.
ax", The'95% confidence intervals ‘were'estimated from the t-distribution of the samplemean.
Duration: statistics: In the, main text, we: discussed:the’ finding that the: ability of human, participants to: distinguish.
model and, human, generated news articles decreases as our models become larger, We have also ‘found ‘that ‘the.
average time:spent fora given set of questions increases as.the model size increases, as shown in-Figure.E.1, Lower
AT


--------------------------------------------------


--- 第 48 页 ---

;                         _—                   .                                                    Average’       |
-_                 Participants Participants, Genders Mean, WordCount |
_ Model,               Recruited Excluded <(mifiother) Age  (humanimodel) —
GPT-3:175B            81                   19              32:30:0          40            569:498
Table E:2: Participant details and article lengths ‘forthe experiments investigating: human.detection.of -»: 500 word.
model, generated news articles. .Partictpantswere excluded. due’to; internet check:fails:
accuracy scores*despite increased timeinvestment from participants supports the finding that.larger.models ‘generate.
harder-to-distinguish news articles.
Preliminary. investigation:of ~* 500 word articles; We recruited. 160. unique US-based participants to! take: part in 2
experiments. through. Positly: (details are: given in Table E-2): We randomly selected, 12: Reuters;world:news articles from.
date 2019 and-created a,context for GPT-3 175B that consisted of a/single Reuters article not in this set of 12. We then,
used. the article’ titles and Reuters’ locations to, generate completions from: GPT-3 175B; and. the: 160M.:control model.
from the:previous:experiments: ‘These*were used to create:two 12-question’quizzes:per‘model, each consisting:of half
hurian written.and.half model generated articles, Comprehension questions were-added-and-articles were shown to
participants in 3 stages ‘at 30ssecond. intervals to, encourage closer. reading.. Participants: were paid $12 ‘for. this task.
Model generation.selection methods, exclusion.criteria,,and:statistical ‘tests mirror:those’ of ‘the previous experiments.
F Additional Samples from: GPT-3
GPT:3 adapts ‘well to many’ tasks other than the ones'éxplored in the iain body of the paper. As an example, in Figtire
FE. we show four uncurated samples froma prompt suggesting that the:model write a poem, with’a given'title, in the.
stylecof Wallace Stevens: ‘Werfirst experimented with a.few prompts, then, generated four‘samples with.no additional,
editing or selection; (sampling at temperature ‘1 using nucleus sampling [HBFC19] with P =:0.9):. Completions were.
‘truncated when. the model-began to ‘write a new'title and. author heading, or broke into prose commentary.
48


--------------------------------------------------


--- 第 49 页 ---

Gontext -—                                                                               The icity
TRANSLATED: BY“ EDMUND: ‘KEELEY’
‘[Poem text, omitted]
SOME "TREES!
John Ashbery’
[Poéhl ‘eext, omicted]
Shadows: ion: the Way.
Wallace! Stevens
veoess (Generated ‘Poem si. ss                      woes Generated ‘Poem! 3) =---++-
I must have Shadows on ‘the; way                   The sun was. all we had: Now; in ‘the ‘shade
If I ait ‘to, Walk: I ‘must Have; ©                   All is::changed: The mind, must dwell. ‘on, those
Each step, ‘taken, slowly, and’ ‘alone                         White fidlds, that ‘to its; syés were always ola)
To have ;it: ready. made.                           Those: ;ancient gleams, cconvoluting       .
:                                             The ‘way: they’ lay, among: the. huge ‘roots,
‘And Imust! think in lines! of grey,                 The great: dark tomes! of reverie,
To Have idim thotghts ‘to ‘bet my" guide                           The plumed labyrinth of the seas
Must, Look’ ‘on, blue and igreen,
‘And :never 1ét. my eye forget;                                      When :our own. breath gonne: faint and fade:
That: color zis, ny friend                             Up «the: long: stairway: ‘of! the dead
‘And ‘purple: must surround: me: ‘too                           We shall, remember, ‘well
The blindness of ‘the light, the ‘whiteness
The yellowiof the sun is’ no? more                  Of ‘that: white land.. ‘The footsteps, sand
Intrusive’ ‘than, the ‘bluish’ snow’                   The grasses. in. that! light; and in ‘the ‘shade
That falls: on, AI1l Of lus: J: must HAVE:             The menacing) lite,
Grey “tholghts; atid Blie “tholights walk With, mé                        ~
Tf T am-to: go} away. atiiall.                      We ‘played, too, once, sin! the warmest: rooms...
-                                           To one ‘content, there: iis, one content:
suse Generated ;Poem 2) -S=sss==              For: all. mankind., ‘The! forest glades'
Ar all, the: mors for being: in fable,
Theré is away’ in the ‘world:                         And ‘wood jand, town in legend, shadowed—
That: ‘Way.’ ian, may Live in; A, sill ‘own,           Once more: to return: ‘The; ‘one content;
Hig lifetine, Stia, $88 ino: tial, Coie,                Hag init SUSEY HABPIRESS, HORS BYEYE:
Never. *the: stir’ of ;men about, a, market,                   For, being: purposeless; ‘and. so
Neven ‘the: bobbing of covered, carts.,,               Those :island, gardens, ‘deep) in the mind,
Never “thei icreak: passed :on' ‘the stair                     Make sof] remoteness! familiarity,
Closing isome other Life; an. old man               And of ‘the! unspoken’ matte’ a, frietid,
Can ‘prosper ‘in. that ‘town, ‘can, prosper
When “His Stora GS seer ‘to, cloge,, ‘his Witée,           ---oo---: ‘Generated ;Roem! 4; --------
Fades, in his; house,, and ‘the; world shouts) ;in, vain
That noone: ‘comes to :hear’ him                       Nobody; will, ;come ‘to: this, place.. Tti:isia, road “that: leads nowhere.
Where :hisismal? bell ‘tolls.                       The solitude.is deep. ‘The. mountains’ are: high..
‘Th, that (stall, town                               But ‘they: are desolate, and, they “tur the travelers: face
Everything. ‘is: knowns;                                      Towards! ‘the: North. All, the Soltids of ‘thé yorld are! tar’ away.
The ways of the young men, are known’,                 When ‘the; wind, rises above; the trees;                         ;
And the ‘ways: of thé “young women                            The “boughs: bow: to ‘the ground.
As they: walk: to the Spring;                       Even ‘the: birds thati;inhabit the, ‘tangle: of weeds
The rumors:are. all ‘known, “                      That: is: the! roadside ‘cover’, are silent. tine listens,
The quarrels, the hurt, ‘the! envy.                 But hears! no. rodr. of the! forest., ‘Gne!iis alone...
Till a day ‘comes                                   One “will. ‘be: taken;
That ‘Way: ott; Of the ‘world’                       There is; Mo utterance; there is mo conversation;
That: leads: ‘to; a. wood.                                   But: one: sis; uneasy. all, the; same........
Young,,. ‘the: young: man                                            There’ isj:a: thin ‘blue mist ,
Walks,,. .a Aover* NOW;                                           & darkness; rising like: ‘smoke.
Knows “the! istreets as they’ fare and is! tof! them.      ‘And. within that darkness
Comes “to, the; wood»                               One ‘will, be; taketiss.: Tt was here, and, it Will ‘be! here again—
Heré,, hdér: this sky empty! and full, of Tight...           ~
There,,, in the; wood,, among: trees.,,                                    i    "                “
He sees ishadows on ‘the way, hears ‘voices,
hears ‘the! wind. and ‘the’ rustling of ‘leaves :
Through’ fan open; glade
He sées la shape atid ‘the ‘shape hears:
Waits as ‘the; shadows ‘wait,
As the voices. waity:
Shadows: ‘oni ‘the. way,,, ‘voi'cesi in, the ‘wind.
igure F.1: Four uncurated. completions from:4:context stiggesting the:iodel. composea.poent in the style of Wallace
Stevenswith thertitle “Shadows' on ‘the ‘Way”..


--------------------------------------------------


--- 第 50 页 ---

G Details of Task Phrasing and Specifications

The:following figures illustrate:the formatting and phrasing of. all ‘the tasks;included-in the:paper: All,data:comes from.

the ground truth datasets, in this: section; and noysamples-from GPT-3' are:included here.

:           Context =» Article:
Informal ‘conversation is jan! important part of any business’
relationship.Before you start. a discussion, however, make isure; you. understand,
which topics afé Siiitablé and which are considered taboo: in a parricnlar
culture. ‘Latiti. Americans enjoy sharing anformation. about their 1oéal.
history; art and customs.You may expect questions. about ‘your’ family,and,
be sure} to show pictures of your children.You may feel free to ask. similar,
questions; of your ‘Latin American friends:The French,‘think of conversation
aS an ant form,aid. they enjoy the value of lively discissictis as well. as
GiBAPPSGHERtS. FOE théii,ATSUnsiits Can “be Giiterestiie and ‘they ca. cover
pretty much; or any topic ---~ as long as.'‘they occur in are: respectful. and,
intelligent manner.         ,

In, the United States,business, people like to discuss a wide range of
topits, including opinions about work,family, hobbies, aid. politics. In
Japan, CHind,and Korea, however people aré Much Move PrIVAtS.< They do fot
Sharé. much about: théie thougtits ,feélings,or emotions because: they fel
that doing so might take ‘away from the harmonious business: relationship:
theyre} trying to build:Middle Easterners are also private about ‘their
personal lives and. family matters.It) is considered rude ,for example',to ask
@ businessman. from Saiidi Arabia, abotit his wife or children.
AS a Penéral. Tuls,1t’s béSt: Hot to talk About: politics on Féligion. with
your business friends.This can get you into txouble,even in the United
States,where people hold. different: religious views«In, addition;discussing:
one’s! salary is usually considered unsuitable.Sports is typically a
friéndly Subject: in most parts of the world,although be careful. not to
CHICGCLZS MALLONAL. Sport Instead ,bé LEisidly: Ad pravsé your hosts team.
Qy ‘What shouldn’t you do when, talking about sports with, colleagues from:
another country?
AY CHILLCi zing the: Sports of Your colleagues? country.
Qy Which as ‘typically a friendly topic: in most places according to ‘the
AN Sports.
Qs ‘Why are people from, Asia imore private! in their’ conversation ‘with others?
As ‘They don’t want: to have theix good relationship with, others harmed by
informal eotiversation..
Qy ‘The author considers politics and religion _ .
Correct; Answer, =} taboo                                                                              .
Incorrect; Answer — ‘cheerful topics
Incortect Atiswer —y rude topics
Theovrect Answer 4 OpiCS that CAN. Hever bé CalKSd, abaiit
Figure G.1: Formatted dataset‘example for'RACE-h.. When predicting, we normalize by the unconditional! probability
cof each answer. as described.in 2.
50


--------------------------------------------------


--- 第 51 页 ---

Context 5 ili 2! Sl’ 2! The Gold Coast Hotel. & Casitio is @ Hotel. Bild, casitis:
docated! in Paradise, Nevada. This locals’ casino is owned and; operated,
by: Boyd: Gaming., The Gold Coast ts; located,.one mile. 1.6km) west of ‘the
Las! Vegas: Strip on, West; Flatingo Road: It is) located..across, the street
from. the Pals: Casino Resort: and thie! Rio All Suite Hotel. arid, Casitio.
Question: THe Gold Coast! ig @ bidget-frisidly casio. Trié,. False, 6x
Neither?
Correct Answer 4 Neither
Inéorrect: Argwer True
Incorrect) Answer, =» False:
FigureG.2: Formiatted.dataset example for ANLI R2
‘Context > Article:             :                   -                   a
Mrs... Smith, is) an unusual, teacher. Once she told! each student ‘to bring:
along A féw pOCALOsS. i Plastic ‘bag. On. SAC Potato the stideiits had to
Write amamé of a pérSén that théy hated.and the néxt day, every child
brought: some potatoes, Some had tuo: potatoes;some threessome up-to: fives
Mrss Smith,then told the children to ‘carry the) bags everywhere they, went,
even, to, the ‘toilet, for two weeks. As! day after day passed, the children.
Started! td complain about’ the; awil smell of ‘the Cotter. potatoes.
Those. children who: ‘brought fivé potatoes. began. te: feél. the weight: trouble
of ‘the bags, After two weeks, the children were happy: tq hear that: the:
game was finally ended. Mrs: Smith asked,"How did you, ‘feel, while carrying
the potatoes for two: weeks?" The children started! complaining about) the,
trouble loudly.
Then. Ms... Switth told thei Why She Asked. “thei! to play thé @ame. She
said, "This as ‘exactly the situation; when,‘you; carry your hatred, for somebody:
anmside your'heart, The terrible smell, of ‘the hatred will pollute your
heart; and,'you, will. carry ‘something unnecessary with you,all the time: If
you Catriot stand the smell of ‘the rotten, ‘potatoes; for’ just! two weeks, can.
Vou iWAgins how Heavy Wt Would bé TO lave the hatred in. your heart for your
lifétime?’ Sé throw away any hatred. from your heart, and you?ll bé really:
happy"               ,                 ,
Qe Which of thé following is True atcording to ‘the passage?
As If a kid Hated four peoplée,he or she had to carry. four potatoes.
Qs ‘We can. learn, fromthe passage that we should _ :
Ae throw ‘away ‘the hatred. inside
Qs ‘The ‘children, complained about; . besides: the: ‘weight: trouble:
As ‘the ismell.
QY MeS..SHith ASkéd ‘her stWdéiits to. WItS . oi the potatoes.
As
Correct: Answer names’
<mcorrect: Answer =» numbers:
Incorrect; Answer -— ‘time
‘Incorrect Answer + plates
Figure G.3: Formatted dataset example:for, RACE-m.. When.predicting, we normalize’ by the:unconditional probability
of each answer as deseribedin 2,
51


--------------------------------------------------


--- 第 52 页 ---

Context S How to Apply Sealant to wood.

Cofvett! Ariswet —  Usitig abriish, bitish, of séalaiit onto Wood WHitil Tt is filly satiated with
the sealant.

incorrect: Answer, +>» Using a brush, drip on, sealant: onto wood until at. is fully saturated with,
the sealant.       :                                             :
Figure G:4: Foriiatted dataset example for PIQA.

Cotitext + My body Cast @ shadow over ‘thé grass! becaiise

Correct; Answer, —¥ the’ sun, was ‘rising:                                                                    ;

Incortect Aiswer + the grass: was cut.
‘Figure'G.5: Formatted:dataset example for ‘COPA:

(Context (CNN) VYiwal RABI, Whoss father, Yitzhak ‘Rabi, Was. Assassinated. white
serving as Prime Minister of Israel, criticized Donald Txump for appealing
to "Second Amendment ‘people :in, a, speech; and, warned, ‘that; the words ‘that
politiciars use can ‘incite violence and undermine democracy. “Trump’s
words; aré an incitement) to the type of political violetce that ‘touched
ie PSYSOnALLY, " RAWIT WHOte dt USATOday.. Hé Said! that Trip: s. appeal. to
"Second Améndment péople” to stop Hillary Clinton -- comments that.were
criticized as a,call, for violence against Clinton, something: Trump denied;
s+ "were a new level of ‘ugliness: in an. ugly campaign season."

- The so. of a forex Israeli. Prine Minister who was assassinated wrote: an
op 6d About ‘thé Consequence of Vidlént: political rhetoric.
- Warns. of "parallels" between Israel of ‘the 1990s and the U.S. today.

Corréct: Answér — —- Référéicing his father, who-was shot: and killéd'by an..extremist: amid
political,tension sin, Israel, in 1995; Rabin, condemned Donald Trump’s

;                aggressive rhetoricy                                 :

Correct; Answer — = Referencing his ‘father; who-'was shot; and. .killed! by an.extremist amid
political, tension in, Israél. in 1995, Rabin. coidemiied Trumps: aggressive:
rhetoric...

incorrect: Answer, =>» ~ Referencing his father, who'‘was shot: and killed by an extremist: amid
political, tension in; Israel, in 1995; Rabin; condemned Hillary Glinton’s
;        aggressive rhetoric:     _              7
Incorrect! Aiswer + - Referencing: his father, who was shot; and, killed ‘by an, extremist) amid
POLItiCAl. tension TWh. ISvael, i 1995, RAbiT. CoNdéiiiied U-S.w8 apsressive
rhetoric.
<mcorrect: Answer =» <— Referencing his father; whoi'was shot: and. killed! by an ‘extremist; amid
political.tension iin, Israel, in 1995, Rabin, condemned Yitzhak Rabin”s
aggressive rhetoric.
Figure G:6: Formatted dataset:example for ReCoRD. ‘We:consider the: context above'to be a:single ’problem” because.
this is how the‘task 1s presented in the RCCoRD'dataset.and scored in the ReCoRD evaluation script.
.        Context + ‘anli f: anli ft: Fulton, James MacGregor MSP is: a Scottish politician
who, is @& Scottish National Party (SNP) Member of, Scottish Parlianent
for thé constituency of Coatbridge and Chryston.. MacGregor ig currently
Parliamentary Liaison Officer:‘to Shona Robison, Cabinet Secretary for —
Health & Sports He also serves on the Justice and Education, & Skills:
committees in, the Scottish Parliament,      |     |
Question: Fulton Jaiies MacGregor is) a Scottish politican who is a. Liaison
OffiCeY to Shotia RObisGn. Who He sweaYS TS WS bést friend. Trie, False. oF
Neither?
Correct Answer 4 Neither
Ancorrect: Answer, ++ True
Incorrect) Answer, =» False:
Figure: G.7: Formiatted.dataset example for ANLI.R1.


--------------------------------------------------


--- 第 53 页 ---

Context > Onganisns require cnergy in order to do what?
   Correct: Answer —   tiature and develop:
Incorrect: Answer >  xest ‘soutidly..
Theovrect Answer  ‘AbSOED Lignti.
Inéorrect: Argwéer 4 také an nutrients.
Figure G.8: Formatted dataset. example for OpenBookQA, When predicting, we:normalize by ‘the: unconditional.
‘probability‘of each answer. as:described:in 2.
.           ‘Context > Making atcake: ‘Several, cake pops are shown on. displays. A,‘woman.and girl
are shown making the: Cake pops in.a, Kitchen. They.
Correct; Answer — bake them; then.:frost: and decorate:                                                .
Incorrect: ‘Answer 4 ‘taste them Ss ‘they ‘place ‘them on plates.
Tnheorrect! Atiswer 5 pit) the frost; OH the cake as they pan. it.
Ancorrect: Answer, +» come ‘out and begin, decorating: the cake as well.
Figure G.9: Formatted datasét:example for HellaS wag
Context S Sli 3! SHLE 3! We Shit! thé. lodpliole witch. HAs American Workers: Actually
subsidizing the loss. of their: own, job. They justpassed an ‘expansion’ of
that loophole im the last few days: $43-billion of giveaways; including
favors ‘tothe oil and gas’ industry and the people’ importing ceiling fans
from, China.
Question: THe lodpholé is Tow gone THUS, FAlse,, oF Neither?
Correct Answer + False!
Theorrect) Answer G Tris
‘Inéorrect: Argwer + Neither
Figure G:10: Formiatted.dataset example for ANLI R3
Context  QiesticH? Gedrges Watts tO WAT His Halide quickly ‘by Tubbinp them. Whiten.
Skin. Surface will producé thé most: heat?’
Answer:
Correct: Answer dry paling.
‘Incorrect; Answer, =} wet palms.
Incorrect; Answer —» palms! covered, with, oil
Incorrect Atiswer — palms) covered. with, lotion
Figure G:11: Formatted dataset:example-for.ARC (Challenge). When predicting, we normalize by:the:unconditional.
probability of each answer as.describedn 2.
Correct) Answer <CAJOLS AS. to coNpLliaiice
Encorrect: Answer ++ balk jis to fortitude:
Incorrect: Answer =» betray tis to loyalty
Incorrect; Answer — hinder ‘is. ‘to. destination,
Incortect Aiswer + soothe isi to passion.
Figure G:12:: Formatted)dataset:example for SAT Analogies
Correct: Context — ‘Grace was happy to trade me Her sweater for my jackets She; thinks: the,     .
.             sweater                                                      ;
TheOvrect Covtext 4 Grace WAS Happy to tKAdSe Me Her Sweater for wy FACkKet. She thinks! the.
Target Completion. Looks doway of. Her.
Figure G.13: Formatted dataset example for Winograd./The “partial? evaluation.method we'use:compares the probability
of the:completion given a:correct and incorrect:context.


--------------------------------------------------


--- 第 54 页 ---

Correct; Context >. Johnny Tikes fruits more than vegetables in, his new keto diet because the
fruits:                             ;
Theorrect! Context —S Joliwitiy Wkés PFWITS. more tA VEGSCADLES Gli, Wis Tew Kets dist Wecdiise the
vegetables
Target Completion. ape SaccHanine.
Figure'G.14; Formatted, dataset example. for Winogrande. The “partial? evaluation method. we-use «compares ‘the.
probability-of the:completion given a:correct.and incorrect context.
Context — READING! COMPREHENSION ANSWER KEY:
While this process. moved, ‘along, ‘diplomacy ‘continued: its rounds, Dimect
pressure on, the Taliban had proved unsuccessful., As one NSC staff note:
put; it; “Under the Taliban; Afghanistan is not iso'much @ state, sponsor
of terrorisi as ‘it. is @ State; sporisored by verrovists ." Tn early 2000,.
thé Unitéd States begat & WisH-léVel. S€fort to persiads Pakistan. to iss:
ats influence over’ the Taliban. In;-January 2000, Assistant Secretary’
of State Karl Inderfurth.and the State Department’'s counterterrorism
‘coordinator’, Michael, Sheehan, met with General Musharraf in Tslamabad;,
datigling before hin the possibility of a ‘presidential ‘visit in, March as a
Peward Lor PAKGSvALi. Cooperation. Stcl A VIB! WAS. Coveted DY MisharraL.,
partly as. a. Sign of his governmerit?'s' legitimacy. ‘He told the two énvoys
that he would, meet-‘with Mulla Omar and press him on Bin Laden, ‘They’
left; however; reporting’ to: Washington’ that; Pakistan was| unlikely in fact
to do! anything,” given, what’ it sees as’ the benefits of Taliban. control
Of Afghanistan." President Clinton was! schediled ‘to travel. to Tndia..
The: Staté Départiiéht fé6lt:. that hé showld. dt Visit Tidia without alse
visiting Pakistan. dhe Secret: Service: and,the CIA, however, ‘warned! in
the strongest terms that ‘visiting Pakistan, would risk! the President”s:
life: Counterterrorism officials also: argued that Pakistan had not: done
@noigh to Merit A présidéitial visit... But President: Clinton. insisted
Of, TELUS PAKISTAN Gn thé Atinerary for Wis trip to Sotith Asia. His.
‘ne-day’ stopover’ on March 25, 2000, was the first time a U.S. president
had been there since 1969. At his meeting with Musharraf and others,
President Clinton ‘concentrated ‘on,’tensions between Pakistan and India
and the, dangers of nuclear proliferation, but also discussed ‘Bin, Laden.
P¥ésidéit Clivitol told WS that When. he pulled Misharral Aside for A brie,
One-On-One feeting, hé pleaded With.the general. for ‘help regarding: Bin
Laden:;" I offered him the moon when, I went ‘to see ‘him, in terms of better’
relations: with the: United States; if he’d help us, get Bin Laden and deal
with another issue, or two." “The UlS. effort! continued.
Who: didi Thé ‘State Départmétit £6el should visit both. ladiasarid Pakistan?’
Correct Answer = —- [Falsé] Bit. ladei
Inéorreét: Answer > - [True] Bin Laden
Figure G.15: Formatted dataset-example for MultikC, There are-three levels within. MultiRC: (1) thespassage, (2) the
questions; and (3) the answers. During evaluation, accuracy is determined atthe per-question level, witha question.
being considered, correct if;and only:if all’the answers within the question are labeled correctly:. Forthis reason; we use:
to:refer to the:number of questions: shown ‘within the:context.
‘Context > Question: Which factor will most likely’ cause a person,’to develop a fever?
Answer:
Correct: Answer — @ bacterial population, in the, bloodstream
‘Incorrect! Answer > a leg intiscle relaxing after exercise!
Tneorrect Atiswer 4S  S8VePal, WAFAL paEUiClés on ‘thé skin.
Inéorrect: Answer + carbohydrates being digésted in.thé stomach
Figure G.16: Formatted. dataset example: for ARC (Easy). ‘When predicting, we-normalize by the: unconditional.
‘probability:of each answer. as:described:in 2:                                                                        |                                     -


--------------------------------------------------


--- 第 55 页 ---

.         ‘Context -—* Bob went to ‘the gas station to fill up his car. His tank was completely    .
empty and, so was; his wallets, ‘The cashier, offered! to pay for his gas if he
camé ‘back later ‘vo,"pay. Bob felt grateful .as| he dove home.

Correct; Answer —» Bob believed that there were good ‘people in the worlds                           ;
Incortect Aiswer + Bob contemplated how witriendly ‘the world was’.
Eigure:G.17: Formatted dataset:example for StoryCloze
Context —. ‘Helsinki, is the capital and largest: city of Finland. It ‘is: in the region

of, Uusimaa, in.southern, Finland, on’ the shore of the Gulf of Finland.
HéVSinki. has A popilation of , an WEban. Population. of , Aid A mMétKopoTitan.
population. of! over 1.4 million, making at thé most populous. muni¢ipality:
and, urban area,in Finland, Helsinki is. some north, of Tallinn, Estonia,
east; of Stockholm,, Sweden; and west: of Saint: Petersburg, Russia. Helsinki
‘has close Historical connectiors with theses three cities.
‘Thé Hélginki métropolitan anea. inéludés.-the urban coré of Helsinki,, Espoo,
Vantaa, Kauniainen, and surrounding commuter towns. (It. is the world’s”
morthernmost metro area.‘of over, one million, people, and the’ city is the
northernmost ‘capital of jan} EU tember states  The: Helsinki, metropolitan
aréa 1S the third largest metropolitan area, in the Nordic) courtriés
after Stockholm atid. Copétiliagén, ard thé City of Helsitk? 4s. the third
largest: after Stockholmand Oslo. Helsinki is: Finland’s major political,
educational, financial, cultural; and research center as well, as one! of
northern, Europe’s:‘major cities: Approximately 75%; of foreign companies
‘that: opératé in Finland. have Settled tin thé Helsinki ‘region. ‘The nearby:
MUNLCIPAlity of VANtAAa is thé Location of Helsinki. Airport, with frequent
service to various destinations in, Europe and. Asia,
Q: what “is ‘the most populous, municipality in Finland?
AS Helsinki
Q: howmany people live there?
A: d.4,milifon.in. the metropolitan, area
Q: what pércént: 6f the foreign. companiés that operate in Finland aré in
Qi what owns Are: A ‘part! of thé metropolitan arear’

‘Target ‘Completion. Helsinki, Espoo: Vantaa, Kauniainen’, and surrounding: commuter towns

Figure G.18: Formatted dataset.example for CoQA.
Context —S ‘Pléasé tnsScrambléi thé Létters inté a word, and writé that: words

Target Complétion. casino:

Figure G.19: Formatted dataset, example for Cycled Letters


--------------------------------------------------


--- 第 56 页 ---

            Context —. Passage: Saint: Jean de Brébeuf was) a French Jesuit) missionary who)
‘travelled ito New Fratice in 1625. There: he worked primarily with the, Huron.
OE thé FESt Of His. WLS, except LOL A Lew Years. in Fratice fYom. 1629) to
1633. He learned. their language ‘and ‘culture. writing extensively about:
each to aid other missionaries. In 1649, Brébeuf and another ‘missionary:
‘were: Captured, when an. Iroquois raid! took over j@ Huron village . Together
with Huroii, captives, the missioraries; were ritually tortured and killed
on March, 16, 1649. ‘Brébelif WAS beatified. in 1925 Btid atone sight Jesiit
Missionaries canonized ag Saints im thé Roman. Catholic ‘Church. in 1930.
Question: How many ‘years did Saint: Jean de Brébeuf stay sin New France:
‘before! he went back'‘to France for a few years?’
Answer:
Target ‘Completion, 4:                                                                                 :
Figuré G20: Formatted datasetexample for DROP’
Context > Fill in blanks
She held,*the torch an, (front of her.
She caught her breath.
"Chis? ‘Theré’s a step."
"A Stép. Cit ii thé rock. <Aboiit! fifty feet anedd." She moved faster.
‘Théy both WoVed Pasty. “Wi PACtl," She SALd,, EAISING tHe torel. Wigher,
Target Completion. step
Figure G.21:: Foiiiatted:dataset-example for LAMBADA.
Context —» Please!'unscramble: the letters into ‘a word; and write that: word:
Target ‘Completion, sticks                                                                            ;
Figure G.22: Formatted dataset example'for Anagrams:1 (Al)
Context > ‘Pléase Wiiscramblé: the Tétters ato A word, and write ‘that: word?
~volwskagen: =
Target Completion. ‘volkswager
‘Figure:G.23: Formatted dataset:example-for Anagraris 2
.            Context —» Q: Who'played tess on. touched by’ an angel?                                    .
Ad
Target ‘Completion. Delloreese Patricia, Early (July 6; 1931 4 November’ 195 :2017), ‘known       .
professionally ‘as; Della. Reese
Figure G.24:: Formatted. datasetiexample for:Natural, Questions
56


--------------------------------------------------


--- 第 57 页 ---

Context — “TITLE: ‘Willian. Perry Cménican.o6tball) - Professional. careér
‘PARAGRAPH: In 1985, he was: selected in the first. round,'of the: 1985! NFL,
Draft by the Chicago Bearss, he had,’been ‘hand-picked by coach Mike Ditka
However; defensive coordinator’ Buddy Ryan, who: had a highly acrimonious:
‘relationship with Ditka, called Perry a “wasted draft-pick". Perry
Soot beCaiie B PAW GiOthe POlICLCAL power StHig ple; DEtweer Ditka ana’
Ryan» Perrys "Refrigerator" nickname followed him anto the NFL and ‘he
quickly became a favorite of the Chicago Bears: fans, Teammates called
‘him, "Biscuit;" tas: in “one biscuit: shy of 350 pounds.” While: Ryan refused
‘to play Perry, Ditka decided to use! Perry’.as’ a fullback when the team was)
Tiedt thé Opporsits” FOAL. Wie of ih FOWELW Bid shot SitWAtiots, either
as & ball carrier Or a 1é4d blockér for Stay Turning back walter Payton.
Ditka stated; the inspiration, for‘using Perry as a fullback ‘came to him:
during’ five=yard ‘sprint ‘exercises, During his! rookie season, Perry,
‘xushed! fortwo touchdowts and caught @ pass for one. Perry even had
‘thé opportunity to wut the ball during Stiper Bowl XX, as a nod to his
popularity and. Contributions. to the. teams suctéss. Thé figst timé he
got ‘the ball, he was tackled, fora, one-yard, loss. while attempting to throw
his first NFL pass on.a, halfback, option plays The second time he got the
‘ball; he: scored a touchdown (running over’ Patriots. linebacker Larry, McGrew
in the, process)... About halfway through his rookie: season, Ryan. finally:
‘Begal to Play Perey’, who Sool proved that He WHS. A Capable defensive
lineman. His: SuperBowl ning size is the largest of any professional
‘football,'player in the history of thei event. His ringsize is: 25; while
‘the rite’ size for the average adult: male is between 10 and 12. Perry went
on to play for ‘ten years in the NFL, retiring after the 1994 season. ‘In.
His tet Years AS A pro, He FeBULAELY StrUgPled With his Weight, WHich
hanipésred, Wis Performance At tines. He played.an 138. games, recording
29.5 sacks and five fumble recoveries’; which he returned for a total, of
71, yards. (In his: offensive career le ran, five: yards for two touchdowns;;
and. had one reception for another touchdown. Perry later attempted a
comeback’, playing an tnremarkable 1996 season. with. the London, Monarchs, of:
thé World Leagué of América ‘Football. (latex NFL. Europa).
Q: what team, did he play for?
Ad
Target ‘Completion. ‘the ‘Chicago Bears;
Figure'G.25: Formatted datasetexample for‘ QuAG
Context SS ‘PLEAss UHSCKAMbLE LHS Tétters ante a Word, Bid write that wordy
mwielcad pm ove afl
TAPSSt Completion. meciprocal
Figure G.26: Formatted dataset exaniple for Symbol! Insertion.
Context —} Please: unscramble' the letters into a word, and write that: wordy
‘taefed! =
Target Completion) ‘defeat
Figure G:27: Formatted.datasét example for Reverséd Words:
ST


--------------------------------------------------


--- 第 58 页 ---

Context —» Title: The Blitz
Backprotnd: ‘Fro the ‘German ‘point of view, March 1941 saw an. inprovement:..
‘Thé Littwatfe flew 4,000 Sovti'es that Wdtth, iWeliding 12 tajor and
‘three heavy attacks, The ‘electronic war intensified, but the Lufttwafte:
‘flew major’ inland.missions only on, moonlit nights. Ports were easier to:
‘find atid, made better. targets: “To confuse ‘the British, ‘radio silence’ was
observed, tntil the bombs fell. X- and Y-Gerat beats were: placed over
fTAVSS CAL GStS Atid Switched OLY At thé last Witte. ‘Rapid frequency
changes were introduced. fox X-Gerat,, whose wider ‘band of frequencies: and.
greater tactical flexibility ensured it remained! effective at atime when
British selective! jamming was degrading ‘the ‘effectiveness’ of Y=Gerats
Qi How Hany sorties were flow. in March. 19417
Ax 4,000:

Q: When did the Luftwaffe fly inland missions?’
Tafget Compléticn —  snly on moonlit: nights
Figure G:28: Formattedidataset example for SQUADv2.

Context > Nori, force —- Iti A Simple Case sich as Gh, object esting Wpoii..a, Tables,
‘the mormal force ‘on:‘the ‘object is equal, but im opposite direction, ‘to the
gravitational, force: applied on,"the object (or the weight ‘of the iobject)s
‘that) isy N= mig (\displaystyle N-mg);, where m is mass; and.g is the:
gravitational field, strength, (about 9.81 m/s! or Earth). The normal force
Héve: LEPLSSSICS the. .foRes Applied by thé Table AGAIst the object that
prévénts. it from Sinking through thé table and requirés that the table is
sturdy enough to deliver this normal force without breaking: Howevers it
is easy ‘to assume! that: the) normal force: and.weight are jaction-reaction
‘force pairs (a..common, mistake). In this casé, the wormal force and
Weight wéed to be, equal An magnitide to explain why there, is no upward
aécélératiion..of thé: object... ‘For example, A. ball. that bouneés upwards
accelerates upwards. because the normal, force acting onthe ball is’ larger,
in magnitude than, the weight: of the ball.
‘question: is! the:normal. force equal to ‘the force of gravity?’

Target ‘Completion —. yes                                                                               .
Figure G:29: Formatted :dataset.example for'BoolQ:

Cotitext —: ‘The ‘treid toward lower rents. may seem stirprising: giver. that. some
COMMUHITLeSs In NéW York. are Bemoaninigi the Toss: of favorite local
businesses to high rents. But, despitethe mecent softening, fox many’
of these retailers there’s still ‘been too big a jump from the rental, rates
of, the: late 1970s, when, their leases were signed: Certainly; ‘the recent
‘drop in pricés doesn’t meat Manhattan: comes cheap.

GUsstion: Manhattall comes! Cledp. trie, false, of Neither?’
Tayeet Coipletion > farse
Figure'G.30: Formatted datasetiexaniple:for CB
58


--------------------------------------------------


--- 第 59 页 ---

.            Context -—> ‘The bets ‘which won him dinner for four, ‘was regarding the existence iand
mass’ of the top: quark; an ‘elementary particle discovered in 1995
quéstion: The Top Quark is the last of six flavors of quarks predicted by’
‘thé standard. tiodel ‘theory of ‘particle ‘physies. True or False?

Target ‘Completion. False
Figure G31: Formattedidataset:example for RTE:
Cottext — An oitiitter provided, everything weeded. for the safari.
B6f6ré Nis Livst Walking holiday, hé went ‘to a specialist: outfitter vo. buy
some: boots,
‘question: Is the word ‘outfitter’ used, im the, same way in the! two:
sentences! above?
answer:
Target ‘Completion, no;                                                                                ;
Figuré G32: Formatted dataset:example for WiG
Context —> (Final Exam,with Answer Key’
Instructions: Please carefully read the following;‘passages: (For ‘each
‘passage, ‘Vou. WUSt. identify which, wolin the prondii marked in. xbold¥ refers
Passage: Mr, Moncrieff visited, :Chesten”s luxurious New York: apartment,
‘thinking ‘that! it belonged to his son Edward: ‘The result was that Mr.
‘Moncrieff has: decided, to cancel Edward#s allowance, ow the: ground that
Hé 06 longer TequUixes *hiS¥ LinanCiadl. siipport.
Quéstion: In thé passage above, what: doés the pronoun. "*his*" refer to?
Answer:
Tavget Complétion. mr. monériett
Figure:G.33: Formatted dataset example.for WSC
Context — Q: ‘Nadé Descending A. ‘Staincase® is’ perhaps thé most famous. painting ‘by
which 20th, century artist?           /     :                            ~

‘Target ‘Completion, 3. MARCEL DUCHAMP

Target ‘Completion. x mutt:

Target Completion. duchamp

Target Completion. ‘Mmarcél, ditcnamp

Target Completion R.Mutt:

Target Completion — Marcel, duChamp

Target ‘Completion —. Henri=Robert=Marcel, Duchamp

Target Completion. Marcel, dw Chanip,

Taxget Completion.  herti repért marcel. diichamp

Target Completion. ‘Duchampian

Target Completion, Duchamp

‘Target ‘Completion, -duchampian,

Target ‘Completion. —. ‘marcel, du champ.

Tafget Compléticn — Maresl. Dwehanp

Target Complétion.<} ‘MARCEL. DUCHAMP

Figure G.34; Formatted dataset example‘for TriviaQA. TriviaQA. allows for multiplevalid completions.

59


--------------------------------------------------


--- 第 60 页 ---

Context =) Qi What School did biume hogarth. establish?
Tagget Completion. School. of Vistal Arts
-Figure’G.35: Formatted dataset example:for WebQA:
Context ». Keinesfalls diirfen diese fiir den. kommerziellen Gébrauch verwendet werden:
Target ‘Completion... In no case may they be used for commercial purposes                           ;
Figure G.36:| Formatted dataset example for De-+En. This is:the-format for one- and few-shot learning; for-this:and.
other langauige tasks,, the: format-for zero-shot.Jearning is ““Q? What is’ the. qlanguage} ‘translation.of {sentence} Az
{translation }:”
Context —» In no case may they be used for commercial purposes: =                       .
‘Target Completion, Keinesfalls diirfen diese: fiir den. kommerzi¢éllen Gébrauch verwendet werden:
Figure'G.37: Formattéd dataset exaiiple for En De
Context > Analysis. Of instar distributions of Larval. T.. verticalis collected from
a, series. of, ponds. also indicated, ‘that. males were in more advanced, instars
‘than females: =
“Target ‘Completion,  Leanalyse de a distribution, de fréquence des istades danvaines d’Is.          ;
,              verticalis dans ‘une série d’étangs a également: démontré que les larves
males €taient: a des stades; plus avarnicés,-que les larves ferielles:
Figure G.38: Formatted dataset:example for En—$Fr
.            Context —> <:L?analyse de la distnibution, de fréquence ‘des istades Jarvaires d’L.         .
verticalis ‘dans:‘une série d’étangs a également: démontré que les larves
malés étaient: & des stades plies avaricés; Gis les lanves femelles. =
Target ‘Completion. Analysis: of, instar distributions of larval I» verticalis ‘collected from     ;
a, Series, of ponds; also indicated. that: males were, in more advaticed instars!
Eigure G.39: Formatted.dataset:example for Fr En.
Context —. ‘The truth is ‘that: you. wants, @t any price; and. j@painst: the) wishes! of ‘the
peoples of Europe, to continue ‘the negotiations for Turkeys accession
to: thé: Eitopeam Union, déspits Turkeys. continwitg réfiisal to irecogiise
Cyprus. and.despite the fact that thé démocratiée reforms’ Ave: at a
standstill, =<
Target Completion. AdévSrul. ésté cd va doriti,, Cu oricé pret gi. Inmpotriiva dorintei
,                     europenilor, isa continuati negocierile de aderare a Turciei, la Uniunea,
Europeana, in ciuda ‘refuzului continuu al Turciei de a, recunoaste ‘Ciprul,
gi, in ciuda faptului ca. reformele deniocratice au ajuns fntrcun punct mort...
Figure:G.40: Formatted dataset:example:for.En— Ro


--------------------------------------------------


--- 第 61 页 ---

Context —» Adevarul, este ci va doritis; ‘cu, orice pret gi, Impotriva, dorintel
suropenilor,; sé continuati, negocierile de aderare ‘a Turciei la Uniunea,
Buropesia, in citida xefizilui continu al Tiirciei de a.ecuioagte Ciprul.
gi. I Cilida PApPtilii C4, Peforiiéle democrvatice aw Ajiins sero pict. mort.
TAvgst Completion. ‘Thé tHitl G8 that, You. watt), @t any price, And. Agpaitise the Wishes of the
‘peoples of Europe, to continue ‘the negotiations for Turkey’s accession,
‘to, the: European’ Union, despite Turkey’s: continuing ‘refusal. to: recognise
‘Cyprus; and, despite the fact that the democratic reforms) are: at a
standstill.
Figure:G.41: Formatted dataset:example:for.Ro—En
Context — Q: What 4s (2 * 4) ¥ 62
Target Completion. + 43
Figure G.42: Formatted.datasetexainple for Arithmetic 1DC
Context — Q: What a8 17 minus 142
Target Completion. —> 3
Figure G.43: Formatted:dataset example for-Arithmetic 2D-
:          Context “+ Q: What as 98 plus 457
Ag
Figure'G.44: Formatted dataset example:for Arithmetic 2D+
. Target ‘Completion, 4275,
Figure:G.45: Formatted dataset example:for Arithmetic; 2Dx
.             Context —. Q: What as 509 minus 488?
Ad
Target Completion. 21,                                                                                .
Figure G.46: Formatted:dataset.example for Arithmetic 3D--
Context — Q: What is 556 plus 4977’
As
Target Completion.» 1053,
Figure:G.47: Formatted dataset:example:for.Arithmetic:3D+
Context — Qi What 4s 6209 mints 33657
Target Completion. 2844:
Figure G.48: Formatted dataset example for Arithiietic 4D-


--------------------------------------------------


--- 第 62 页 ---

.      Context 3. Q: What ds 9923 plus 6177
Ad
Figure‘G.49 Formatted dataset example for Arithmeti¢4D+
Target Completion, -38097
Figure G.50: Formatted.dataset exariple for Arithiietic 5D—
As            /
‘Figure'G.51: Formatted dataset example.for.Arithmetic:SD+


--------------------------------------------------


--- 第 63 页 ---

H. Results on Alli Tasks for All Model! Sizes
Zevo-Shot                          ‘One’Show                        ‘Few:Shot!

Fine-tune      -                             “S                             “*                               175B:
Namé;         Méitic: Split SOTA’ K°     Small:Med‘Varge:XL: 2:7B 6.7B I3B°175B: Small. Med Large:X1: '2:7B'6.7B' 13B 175B Small Med-Large XL. 2°7B'6-7B!13B. 175B. (test'servet),
HellaSwag:     ace     rdey) 85'6 20    33:7, 43:6 51.0 [54.7 62.8 67:4 70.9:78:9    83.0, 42:9 50.5; 53:5:619266:5 "70-0;781, (33:5 43.1513 54:9'62.9 67:3 “71.3. 79.3;
LAMBADA; ce     ‘fest, 68:0 15    42:7; (343 60-4 (63-6 67:1, 70:3. 72:5, 76,2    22.0, 471 32:6 38:3°611 '65.4809.0172:5 (22.0 -40.4'63.2» 57.0: 78:1 79:1 81:3 86.4:
LAMBADA: _ppl     ‘test? 8:63 15    18'6 9.09 6.53 5.44 4:60 4.00 3563.00    165.0 ‘11-6 8:29; '6:4615:53 14.61 14.0693.35.    165.0. 27.66.63 7:45.2:89 2:56 2.56 1,92:
StoryCloze    ace     ‘test; 91.8 70    ‘63/3 (68:5 7214 "73:4 97.2: 77 99:5'83:2    62.3) 68:7, 72,3) 74:2°77-3 °78.7°79.7 84:7;   {62:3 70.2)73.9: 76.1 80:2 81,2 83.0 87.7:
NOs       ace    ‘rests 445 64, = 0.64 1.75 D771 4.40 6:01. 5:79; 7:84°14.6   ANS; 307 4°79; 5:43:873 9.78, 13:7°23.0,   1.72 .446°7°89: 9.921132 17-0 B10 29.9;
TrividQa    ace    dev 68.0 64   4.15, “7.61 14:0 19.7 31.3 38.7 41.8:64:3   419 12:9 20.5; 26:5.35:9'44:4:..513 68-0 16.96 16.3,265 32.1423 51-6 575 71.2; “212
WebQs      ace     test? 45:5 64, 1977, 8.20 433 14.63 7.92, 7:73) 8.22144   2:56: 6:20 8.51: 9:15: 14:5 1541 19.0:253 5.46 12:6 15.9 19.6:24.8 27:7, 33:5.41.5:
Ro-En 16    BLEU-inb ‘test; 39.9. 64 2.08 (2:71 3.09 3:15 16:3, 8.34 20:2 19.9    O55) 15,4 23.0; 263'30.6.33.2:35.6 386    1:25 20:7:25:8 29.2/3371 34:8, 37,0 39:5:
RoEn 16   BLEU-sb’ ‘testi      64; 239 3.08 3:49. 3.56 16:8 8:75 20.8:20.9    0.65; 115.9, 23.6 26.8:3153 134:2 336.7 40.0    140° 21.3226.6. 30.1134:3. 36.2 3814.41:
En-3Ro 16    BLEU-mb ‘est; 38:5 64 214 22,65 2.53 :2.50 3:46 4/24) 53327141    035; 3230, 7:80; 8°72 13:2. 15.1 1773 20.6    1.25 5:9010:33 10:7114%3 16:3 <18'0 21°0;
En Ro 16    BLEU-sb “test;    64    Bor Bll 3:07, 3.09 4:26. 5:31, 6:43°18.0    O55. 3.90 9:15, 10.3 15:7' 18:2..20.8 24.9.    1,64 7,40: 10.9" 120.172 19-6 21:8 25,8:
Fre+En 14    BLEU:mb ‘test: 3550 64, 1181. 2.53 347, 13.13 20.6 15.1, 21.821.2    1.28) 15:9 23.7) 26329-09305 302.539, 4.98 25.5.98:5 311.3347 34.9 3616 3922)
Fr-¥En 14    BEEU-sb :testi 64" 2.29. (2:09 3.90 13.60 21.2; 15.5; 22.4. 219    150 16:3 24.4) 27.0°30.0'316:31435-6 5.30: 26.2°29'5 32.2351 36-4 383-414
En Fr 14    BLEU-mb ‘test! 45:6 64. 1474 2.16 2:73 12:15 15:1: 8.82. 12.0:25:2   0:49; 8.00, 14.8) 15.9:20:3 1933 124.9:283 = 4.08 14.5119 21.5124.9 27:3 29.5 32:6)
En-¥Fr-14    BLEUssb «test; 45.9 64: 244 0.75 354 0.82 19.8 11.4 15.3°313    O81. 10-0 18.2) 1932247983 40.1349 = 5. BT 18:0923-6; 26.1303 333 35'S 39:9:
De—-En 16   BLEU-sb’ ‘test!      64:   239. 3.27 3:85 14.04 22.5. 18.2: 24.4286   0.93) (171: 23.4. 2518:29:2.231.9934:5 82:1.   3.60 23.8:27:5 30.5°34:1. 36.5 39.1:.43.0;
En-+De 16    BLEU-mb ‘fest: 41.2 64   1:70, 2.27 9°31. 12:43 12.9! 8166, 10-4046    0:50. 7100, 12.9. 13/1: 18°3 120.9422:5 260°   53.42 12.3) 154; 17.1,20°9 23°0 26:6 20:7:
En—,Dé 16    BEEU-56 ‘test’ 41:2 64> 2.09, 2:65 2.75, 2:92 13:7: 9:36, 11.0;25:3    0:54; 37:40 13.4) 13-4 1818217233 27:3 378 12:9: 16.1 17:7 21:7, 24:1 27.7, 30.9:
Winograd’     ace     ‘test; O38 7:    ‘66:3 72:9 74:7, "76:9 82.4 85.7: 87.9883    63:4! 68:5 72,9; 76:9'82:4/84:6'86.1 89:7; 63-4 67:4°73'6: 76918493 854 (82/4 88:6)
Winogrande! = ate     dey’ 84.6 50! = 52.0 1521 37.4 (58.7 62:3! 64.5! 67.9702    31:3) 33.0 58:3) 59:1°61:7 "65:8 °66,9'73.2, (313 52:6157:5 59:1 62:6 674 70.0 77:7
PIOA. 4 Be    devi 77.1 50: G46 10.2 72.9. “75:1 75.6, 78.0) 78:5:81-0   64.3, 69:3 71.8) 744743 °76:3°77:8'805, 643 69:4°72.0: 74.3°75.4 77.8 79.9 82.3, (82:8
‘ARE (Challenge) ace     test? 78:5 50!   26:6 1295 31.8 35:5 38.0 41.4. 43.7514    25.5. 30.2 31.6. 36.4:38:4.41:5 431532 95.5 28.4532 36,7'39.5 43-7 44.8 51,5:
‘ARC (Easy): ace     test; 92:0 50: A316 A465 5310 53/8 58.2 60.2 63.8.68'8   42.7) A482 54:6 55:9:60-3262.6266.8.71.2 A272 SL0358.1 59.1 62:1 65.8 691 70.1
OPEMBSGKQA ace     ‘test, 87:2, 100. 85:6 43:2 45:2 «46-8 53.0, 50.4, 55,6576    37.0; 39.8 46:2, 46:4°53.4.53:0155.8:58:8 (37.0 -43:6348:0' 50.6755:6 55:2 60.8 65:4:
Quac;       fl,     rdeyv: 744 5)    21.2 (2658 31.0 730.1 34:7: 36;1, 38.4-4175   21.1, 269; 31.9; 32!9737:4:39:0'40.6743.4   21:6 27.6°32:9: 34'2°38'2 39°9 40:9 44:3)
RACE-h      ate     “test; 90.0 10) = 35.2 37:9 40.1 40.9 42.4 44.1. 44.6'45:5    34:3) 37:7. 40.0. 42.0:43:8.443 44.0°45.9 B43 37,040.40 4442.8 44.7, 45-1 46.8:
RACE-m:     ace     ‘testi; 931, 10" 42:1 A72524 $23 54.7 544 56:7:58:4    42.3) 47:3 51.7 55.2:56.1.54.7,.569°574 423 .47.0352.7 53.0°55.6 55.4 58.1, 58.1
SQuADv2.    em.     dev? 90.7 16    22.6 (32.8 33:9, 43-1 43.6 45.4 49.0'52.6    25.1. 37:5, 37.9: 47.9:47-9' 51.1 56-0601, 27:5 +40.5.39.2: 53.5 50.0 56.6 62,6 64.9"
SQUADV2:    fl:     dey 93.0 16    28:3 402 41:4 503 51:0 52.7) 56:3759.5    30:1; 43:6 44.1: 54.0:54.1 571 (61:8 :65.4   9321 .45.5%44.9° 58:7'55.9 16221 (67:7, 69.8!
CoQa,       fi,     idev) 907 5    345 55.0 618 65:3 71.1, 72.8 763.8155    30.6 S21 61.6 66:1°718'75.1°77.984.0 = BT 52.0627 66.8173.2 773 79.9 85.0:
PROB!      fl,     rdey? (89:1, 20:   9:40 13:6 14:4 16:4 19:7: 17.0; 24:0:23°6    ILE 18:1, 20,9: 23°0'26:4.27°3 (29.2,34:3,    12.9 18,7724.0 25.6;29.7, 29:7, 323 36.5:
BoolQ;      ace     dev 91.0 32. 49-7 603589 62:4 67:1. 65.4 66:2:60:5    52,6) 61.7; 60.4) 63/7:68.47.68.7°69.0:76.7 43-1 60.6:62'0° 64.1°703 700 70.27.51 764
CB       ace    dey 96.9 32. 0.00 32:1 8:93 19-6 19.6 28.6 19.6 46.4   Dodi 93-6 53:6, 48.2.57:1 (33:9) 55.4643   42:9 58:9'53-6: 69.6:67.9 60,7, 66:1 82:1. “75:6
cB        fl.     dev: 93:9 32    0.00 129.3 114 17:4 22.4. 25.1, 20:3:42:8    60-1, 39:8 45.6 37.5:45:7 3285 44.6525 26-1 .A0.4232:6. .48.3745,7 44.6 46.0 57:2: 52.0
Copa        ace     rdev: 94:8 32    66.0 {68:0 73:0 "77:0 76.0, 80.0; 84.0;91'0    62:0; 64:0, 66.0: .74.0°76:0!82:0'86:0'87.0 {67:0 64.0°72.0: 77:0:83°0 ‘83:0 86:0 92:0: 92:0
RTE        ate     dey’ 92:5, 32 ATT. 49.8 48.4 (56.0 46.6, 55.2; 62.8: 63.5    d3iL, 47:3 49:5) 49:5°54.94934.97563 70.4 19283 48.4.46.9" 50,9'568 495 60.6 72:9! 69.0
WiC        ace     devi 761, 32. 0.00. 10.00 0.00 0.00 0.00. 6:00. 0.60°0.00    50.0° 503 50.3) 492:49:4..503 50.0486 49:8 55.0:53.0; 53.0.51.6 53:1, 511, 55.3: 94914
WS...      ace     dev? 93:8 32    59-6, 56:7 65:4 (61:3 66.3, 60.6, 64:4°65.4    58.7) 58:7, 60.6, 62:5.66:3 60:6766:3 69.2, 158.7. 60,61548 -49.0 62:5 67:3.'75.0 75:0, 80.1
MultiRG     ace     dey 623, 32, 4472 9.65 12.3 13:6 14.3, 18.4, 24.2:97.6    4.72: 9.65. 12.3; 13.6 1433 18:42249.27.6 6.09 11.8168 20.8°24.7 23.8 25.0 32.5, 30.5
MultiRC      fla     idey) 88:9 32. 570 5977 604 599 60.0 64:5) 71.4729    57.0, 59:7; 60.4) 59.9:60.0264:5 7149729, (45.10 55.91.6412, 65:4/69.5 (66.4 69:3 74.8, 75.4
ReCoRD     ace     dey’ 925 32. 70.8 78-5 82:1" '84°1 86.2: 88.6, 89.0,90:2    69:8) 77.0 80.7! 83:0/85.9'88.0°88.8:90.2, 69-8 77:2781:3 83:1 86.6 87.9 88.9 89.0 90-2
Re€oRD:     fl;     idevy 93:3 32   719 °7922 82.8 185.2 87.3) 89:5, 90.4:91.0   10:7’ 77.8 81.6 $3:9:86.8188:8:89.7 91.2 "70.2 77.958281 84.0:87:5 88.8 89.8 90.1. 911
SuperGLUE!   average: dey ‘89:0       ‘40:6 -47:4 46:8 -49.6 50/1, 52.3) 54:4°58°2    54:4) 5571, 56:7' 57:8:61°2.,59.7 64:3 68.9,   50:2 56:2:56'8 60.0:64:3 63°6 66.9 73.2) “71:8
ANLERI     ace     ‘test; 738 50: «©: 33'd 84 B3'd 1534 34.9 32.3) 33.2946    32.1. 31.6 31.9, 34.6:30.6991:6932.752°0 = BOT 3.573019: 32.5°3315 3311, 33:3 36.8:
ANETR2,     acc     ‘tes 50.7 50: 33.2 23139 333’ (33'3 33.8) 33.5) 93.5°35.4    35.7) 3307; 33,2) 32:7:32.7'33.9'33.933.9 (35.7 33.8132-1 31.4°32'6 33:3 32.6 34:0,
ANETR3     ate    ‘test; 483° 30' = 33.6 34:0 33:8 83:4 35.3" 34:8: 34.4345   33.0 32:6 33.0, 33.9:34:1 33:1 9325 35:1 835.0 34.4/35:1 36.0:32.7, 33:9. 34'540.2:
2D#        ace     nla       50: 0-70 0.65 0.70 0:85 1°10 2:54 15.476:9    2:00, 055 3:15; 4:00 12.1 19'67°73,0:99°6 2.00: -4°10;3.50! -4:50;8-90 11.9 35:5, 100.0
2D-       ace    ava      501 1225: 1:25 125 1.25 1:60: 7:60. 12.6158.0   115. 0.95. 1:45: 1.9573.85 11:5 .44:6:86.4   115 14532.25 2:70°7:35 +13.6 524 98.9:
aD+        ace     na       50: (0.10 (0:10 0:05 0:10 0:10: 0.25) 1:40-342    O15! 0.00, O10: 0:30:045 0.95 15.4655 (0.15 0145.10.30" 0°5570.75 0.90 8:40 80.4;
4De       ace    n/a      50: 0.05 0.05 0.00. 0.00 0.05; 0.05, 0:15:4.00   0.00' 0.00. 0:10 0.00.0.00:0:1020:80:14.0 0.00 0.0510.05 0.00:0.15 0.15 0:40 25.5)
4D-         ace     nia       50    0°00 :0.00 0.00 ,0.00 0-00, 0:00, 0°10°7.50    0,00: 0:00, 0-00: '0.00:0.05 ‘0-00°0'50'14:0   10.00 0:05:0.00: 0:00:0.10 (0.05 (0.40 26.8)
5D:         ace     n/a       50: 0.00 70.00 0.00. 70.00 0.00: 0-00. 0.00°0.80    0.00' (0:00 0.60. .0°00:0.00:0.000.05 3:75, {0.00 0.00%0.00, 0.00:6.00 .0.00 0.00 9.50:
2Dxi        ace     nla       50: 2.20 (2.25 2.65 (2.10 2:55: 5.80 6:.15'19'8    1,35; 2:35 3:35; 2.35475 19:15 11,0274    1.35 2'90;2.70 2:85°4:25 (6.10 7:05, 29:2!
IDE        ace     nia       50! 1:25 *2:95 2:75 0.05 0:30 2:85: 0.75.9.75    1,90: 2.80 2:85: 3.65:6:45 915 820.143.    170° 2515%3.90: 5.75:6:20 7.60 9:95 21:31
Cytled Tetters! ace     nla       100, 0,62 (0.71 2°85. 0.00 0.63, 1°35; 2.58 3.66    LOT 436 35.68) 6.46'6:25 19.41 15.1 21,7.    4:63 927107 14.5169 21:9 27.2. 37:9:
‘Anagrams*L   ace     n/a      100. 0.10 014 0.40 -0.00 0.27’ 0:69, 1:16:2.28   O21. O61 1:12) 1:27, 1.6092772:3:72.8:62 0.50 1.279.113 3.0513:81. 5:49 8:38 15.1.
‘Anagrams'2, ace    nia      100 = O'81. 1.21 2.69 {0.01 171, 3:75; 4353°8.91   119; 2162 4:70: 4°77:6:97' 10:2, 14:6:25°9.   1.94 -4:80°7'59: 918721276 189 '25°6 39:72
Reversed: Words: acc     n/a       100. 0.00, {0.01 0.01, ‘0.01 0.02' 0.03; 0.03'0.09    0.02! 0.01. 0.01, 0.00.0.05 10.07 :0/11 0.48 = {0.00 0.05.0.00; 0:17:0.24 0:30 0.42 0.44
SAT -Atialogies acc     n/a       20: 85.6 39.0 45.2 44:1 50.0, 49.2; 52:7)53.7.    30:5: 412 43:1. 46:5:55:1 943 535 39:1 80.5 -40.4-42:8 -40.6°48'4 51:9 53'5 65.2:

‘Table H.1: Scores for‘évery task, setting and model that'we investigate in this paper,

163,


--------------------------------------------------


--- 第 64 页 ---

Boold                                         CB (accuracy)                                         CB (EA)
go bine tins SOTAT i ae =    H              te Ce TS ee                   SRiRectne SOWA: sce Soi Soe aa ae cps bow SOE
7                       a Oeste               eid ||       ee              |e) | | ||
SSF Few, Shot (K232):                    £80|                    80 BERT Larges Teepe penn
4         |     | Ae                 Bag a RE eee              || Slee         ye       _
E                                                          of—_1—_| 4              [riot RewsShet (K=39)                 we         |                   2 Few-shot 32)
OB  O45! 08B 138 268. 67B. 138         sb          fae  f4B; G88 138! Zeb: 678; 138         is          we   O48 (OSBiT3B 7268 "678 [138         ASB:
*Pardtetsis'ih LM (Billions)                                                                                                        Parariitets| LM (Biliongy                                                                                                          Parahietefs'in| LM (Biliché):
we nee,         COPA’                                 bye se ses         IRTE:                                  oo    ns pease         Wwic
15 0 Ef a                      1S T= fg        z        =                        1 FING-TUNS SOTA cndnanatinbenpe ripe ngeannesnneesnies
JOS OB SOT. eeachonen    lonornsestrernont   -   !        agg pe STAT et   a a          Hanes  an  cromenennnesrtees
| 7            eee                 |      |   |    |   | [= seeste: cosa              |    |   || [
i     =           ;                                                   ses                60,|/—_— p_
29|———     hy        ee                        |        |    |                               2g aaa, a     nal
Zero Shot:                  sh.    Leal Ale         i                           fol   1               .
50 |S RORIBIM CURBING conse ales | eas Rowe ShOl ESA) |                    ca a |     ai                               0
OBE   048; 088 138: 26  67B, 138.          4756           0B   048. OBB 138: 268] 678! 198           1758,           OB,   DAB .08B18B 1268 1678 "3B          *A75B?
1Parameters in eM (eilions)                                   Parameters in LM (Bilons)                                   Parameters nL (liens),
_—             ‘wse                   ;                       es      MultiRC (cciiracyy!                               a         MultRC (Ea)
—4    |        ims         ee Tre rn
2         |                              iit | |i                         |        td eet
%       |    |             7              I          ea.  = zed Shatl               i            4           ae  RERERRI aassas   cues yee    Socata
.   ~~   al         weft tee       =   =,           £30] |+e—! Few-Shot (K=32)            |} eee          se        ee    +    pee
‘   ~RenteaeeS8 eer any 7 ane           a Ceertageer ee!  ee ee             co se
Ly             i,                                                      +20}                        '*                                                      a
60 po                              +                                                               3                                                                               f-
|    n                 J Gne-Shot                  10 eee    1         (i              lee                        —e One-Shot!
O1B; (OAR) 8B 8B. 268 | S78. 138         a8          WIR GMB O88: 1.98) 268) OFS) 138,         TSB,          SNK (OA OSB ITER (268 i878 eB,         “T75B:
{Parameters i LM (Bilfons) :                                   Parameters in LM (Billions)                                   Parameters in LM (Bilions):
URECORD (accuracy)                                                                                                 iRECORD (1a)!
ag PRN Hine SOTA Ths aici cnet ie rehcis netnncan eeood                      [--Fine-tind SOTA Too Escacc sh saecrencde nec coo sense ae
Fe ee ee ee |             a  Has. | l  | = _l |   a
ae TOP   erm:               panne ee               099 FE ee ee   =    sat
ag!                                    ero Shot:                                                           —+— Zero Shot
a0 Tq     a        —                   §50,)       |
ee         |                *  er |   rT
Rendem Guessing nee              Random Guessing: ee epeeeeeneenen
SO4Bs  048) 0.8B 3B 2.68  67B: 13B          A75B          °04B   O4B  O8B. 1:3B: - 2.6B:  678! 138          175B:
{Parameters in. LM (Billignis} ;                                                                                                     Parameters in,LM (Billions),
Figure: H.1:: All résults for all:SuperGLUE tasks.
SAT Analogies?                                             Winogrande:                                         Winograd’
=         7                                J                             Amat ote te ope ee a ee]                   se Fie ANT SOTA on hee Eases ee ce
Sse zet0 Shot!      |    |   |        -                    .            i   i       i                             i   ia   |           °
eo lat roneshety, ff                    $90;            F    |          90; SJ aS
50. —————                  80. Fine‘liined ROBERTS Larges            &         | IT
s       |  eee Ly                                   [ee zen                 :               &       |  a"            sa Zero-Shols —
a       :   |    |                                  8 | |e:One'Shot!        | ZS            BO) as    ae |               =s= One'Shot?
§.40/——_ 2    po         ‘i                     ;                      98 20) |e. Few: Snot 280) | —?°-_—__              B10)      |       oe: Few:Shot (K=7)
a an    |      |                           aa ee a a el        8    |        aa Fee)
of   |      :        1                7                   88      =  a        :                         180)      —j
ioe  i048: (088 438 268, 67Bi 13B        4758:               0.18.  48 j088%138 266  (7B 438;        i758         018  7048 0.88) 1:38) $26B: {6.78.. “13B:        F758)
Parameters in LM. (Billions)                                                                                                                                       [Parameters in LM (Bilions) ;                                                                                                                    Parameters in LM (Billions):
Figure H.2: Results:for SAT ‘task.                                        Figure H:3; All;results forall Winograd tasks.


--------------------------------------------------


--- 第 65 页 ---

wae                   :Arithmetic (few-shot)                    .                                         “Two Digit Addition                    :                                        Two. Digit Multiplication,
"Two Digit Adaition                     a atl            100 | [—5—Szero-Shat! *—                       mae               fol |e Zeto-Bhot                            fe
—— "Two bit Subtraction                   Si                One-Shot                        oy                  —e-one-shat                            a
eo |(eesieepetasation [J | |)          < gg eeir Stor occo0y|            wea             285 [SS “Row: Stot eso)                 of“
** || “—"Thres Digit Subtraction:                  we of                   18"                             as              5,                                                            vf
a Four Digit Addition                  a     fo" |                                             ;      oa     Ke                20                                      Yo
a Go| [=e FourDiall Subtraction?        LE ff_|         so          |            Ge    JE,           al       ; 7 sq    ]       Ye
fol    —+—, Five Digit Addition     Z     wth               as                       fy     ye               io   |         |       SL
§     == Five Diott Subtraction !           a      YA                     5)                                vaAl       2                      SS a a i 4A
&     —— "Two Bigit Multipication           Al     SL                     & adh               |            ffi      WE       |              ie                                   an
:O1B:  048; (088 438 268° 676: 438.         175B          On   (4B, 086. 138; 268) 678) 138         1758;          O48,   048 {0SBiT3B i268 = [678 38)         T7SE)
‘Parameters n|LM (Gilians)                                                                                                 Parameters in LM (Billions)                                                                                                  Parameters in'LM (Bilions);
Two Digit'Subtraction:                                                                                                                         Three Digit Addition’                                                                                                                       Three ‘Digit’ Subtraction’
Se ohe shat:                       é           | Sonate!       |          y,               SE Gre shat                          /|
go | Eevesnotcccom                    y    j            TO): Few:Shot (#50)             a. oe             80) [2+ Few:Shot (=50):; —+——+   |,
—      oe                           =49| ———- +}          tf Se ——                                          os
==" SE               39,       ?    at   el                0.                      ——
OBE   048; 088 138: 26  67B, 138.           4756           0B   048. OBB 138: 268] 678! 198           1758,            OB,   DAB .08B18B 1268 1678 "3B          *A75B?
‘Parameters in LM (Bifons)                                        Parameters in Li (Bilions)                                         Parameters nL (liens),
Fouif Digit’ Addition                              .                                                            Four Digit Subtractién                                                                                           Fivé Digit Addition’
25|[— zeosney |         :         vii         agg een     | | |                      =a Peps                     .
HHH onesstts                           yA               eI one shot              i                              sg | ORES a                           fe
<#—. Few-Shot (K=50)                       Lf                 je Few-Shot (K=50)          :                 B||— ‘Few-Shat (K=50)    7     7              of
—                                   fe                 “                                      f                 sew BI                          a)
BS                          ZL   Pal         BN     |                    fs    y          z                          ff
|_|        po   |           Pt gee           |     [| | | ye.
O1B; (OAR) 8B 8B. 268 | S78. 138         a8          WIR GMB O88: 1.98) 268) OFS) 138,         TSB,          SNK (OA OSB ITER (268 i878 eB,         “T75B:
{Parameters i LM (Bilfons) :                                        Parameters in LM (Billions)                                         Parameters in LM (Bilions):
«              Five Digit Subtraction,                                         Single Disit Three\Ops
10 |= wae Ghat    —        T            7,             cone| (os oro Shh       I     al            Z
3 == Few-Siiot (R505                          fr                 Ie FawsShot (K=50),             |        Aa
=                                aft                 eT                        Le ee
Bs                               7.                   370}.     +       ;        ee ee a
OABt    048: “08B 2:38’ 2.66.    67B. 13B                1758                101B     O48. O8B: 1.38% - 26B.   67B? 138.                1758:
‘Parameters in| LM (Billions) |                                                                                                 Parameters in, LM (Billions).
-Bigure:H.4: -All:réesults:for all-Arithmetic tasks.
.                HellaSwag,                               .         -       tambada                 -               7      -          iStorycloze                .
Ba ee                    ae a a a ae ea                     “preted SOL
Gg ee | fT             90               soy          4__t._t     4
g|___!                             i           =        '                   180) fT    |    ~—                                                 188)                               i      —                  ‘
76        | ee ||                  $70) |Zere‘Shot SOTA}       see a                   eee |             |    |     |       ve                —e
e       i   i ae              s       i     a                       s              ;          —
1B 60 __|         gor  + One-Shot                Fo; —— tt    —             g   |            l,  ee
4       | a |          Feo Few shot W220)            Lo     xe |     |                        J val!        LAA
a Reds Glessind) succes searches eats H              20" fa    |       | |S Few-Shat (K215)                   me         |      |=02. Few-Shot (270)
f    ie a     i                              joo ff __._j            fu                                                                 L
‘OBE  048: (088 3B 268  67B 138         4758          {918  i048. O88: 138! 268) 678) 138         1758:          OB   O48 108B°13B 268  67B “138i         “4758
‘Parameters in, LM (Billions                                                                                                          Parameters in LM (Billions)                                                                                                          Parameters.in LM (Billions};
Figure'H.5: .All:results:for, all Clozeiand;Completion tasks.


--------------------------------------------------


--- 第 66 页 ---

PHYSICA.                ;              _     _     IARC Chaargs!                               a      OBaBSSRIAY
|                              2: Few-Shot (K=50)                    870, TI          |                  _                   (80) |    |     |       |
so LARS GUESNG c l  eeclrnel               i. ae    i   |    | [se Feveshdt tcea0y                   ge |     ESE Feweshor wera)
ous] 048) GSE 138 268 OTE TB         A7s8          i918 (GAB) OeB 498) 208) Gye) ee         1758;          918, 4B (QeBy {9B (288 eye "98;         ATSB!
Parameters in LM (Bilfons) «                                                                                                         Parameters'in LM (Billions),                                                                                                          Parameters in LM (Biliens):
Figure H.6: All résults for all’ Coninon Sense: Reasoning tasks.,
NaturalQuéstionis (T5:splits)                                           Trvia@A:                                 cece       iwesas,
Be                ZO, eet SOTA oe ah                    pa ae T Bh i ia a  i? SIDES ean aE
&   [            |                      (80,     |         |    |  ae ee             |       |  |   |    |     et
&. 23 {== Zereshot             =     Fh            —_                OE    “a                        ee                   | eet               7s
6      eee                             aaa     ||     |_| Ieee seiesnet acon                 cae | |   |  Es Few'Shot ea)
a!                                             #                 a                                                     ~                 0.     ee                  “4
Bows}   048) 088-4.3B° 26B° 6.7" 12.88.         514.68          O48 "O48 088° 138) 268) 687B. 138          1758.          ‘o-1B:   Q4B. 8088138 208 678 138         e178:
—         °     ‘Parameters LM (illens) ,             -                                 ‘Patameters'i4 LM (Billions)                                                Parameteis if’ LM (Billions)
Figure H.7: All results forall QA tasks,.
‘oiiac!                                                 SRACEZh!                                  a        RACEzin’
i fefine thited $016 eae             oo) nifine: Wed SOA oe eee ee Pe                Fieaned SOA ae aaa 2c orgs fe pe oes Seen
|               + {1           rtf
| 4A         j—[-s52er0 Shot!                 ;          :         —     -    =     7              ei | Bae |    [ea Zeresiot
ae                  [-+—"One-Shet                0 |$—e     jp]               lo                  = One-Shot: |
ape |                      a3 Few-Shot (K=5)                  a                                                  rae                        Se=) Féwestiot (K=10)
i a        i            |     FEW-Stiot (K=5)                 S|                                              aol
918; O4B (088 Y.98 268 87B. 138         a8          QIB (BABY 988 198) 268" SFB: 138         5B.         “O18 048 “OSB? 798 (208 BTR +198)         “T75B;
-                                ‘Batameters in LM (Bilions)                                                                                                    Parameters in LM (Billions),                                                                                                     Pafamieters in LM (Billens)
Squad                                                 ICOQA!                                 os                Drop’
TAPIRG-HN 6G SOTA = ats remec ne oe             reg LR SO SS             Jog" Ribleted)$ OTA sya sacs bccn ane Bc Lau EL
% as ie a ns an t              180) nn          r |    |     |
|      Lr      g    | wpa | ||                |     [|_| [fe =e
so\— |               ee et              ngg| —_|_ fF |               |                             $880], —— } toe ne shot, |
5.     ra    ta       T                           {86| —.-—_—             ———__.              ll         r|    i     i    oe        4
x7 x   rT     TTT Ee Ree Shit 18) |            390      L       [| [|  IE Fey Shot (K8)              ‘to se  SS§o_
OMB! «048: 088 3B 2B CB 138         41798         Tore   O48. 088: 138: 268! 67B? 138.         1788          Oe   048 (08B513B 1268 [67B i138)         waive!
“Pafanieters ih LM (BImGHS) ;                                                                                                                                   PTERIBtETS ie LM (BINGE),                                                                                                                                    PSRAITSLETS ILM (BING);
Figure H.82 All results foriall Reading:Comprehension’tasks..
7                              :ANLI Round                                -                             *                               TANLI Round?                               -                               JANLIRound3:                               -
“ Fine-tuned SOSBRTatange TT           v(-Hine-tuned SOTA" TE          gg BEUNEG SOT a  e
ge}                a pe tuned Beatie TT eee                 "ig                {|          |
|     |    |   |   |    |   |                         a8          |                                     vgp[ifnetneanonerratage: J. |
8              Et    |          i                                 450       t             {          t                                  44 [ Pine-tuned BERT Late on wo neeeeaceepene-teesqees oan sEaeeae
MO eine BERT og lagccel aloe ogee                        |    |       |         |               :                ee eee
=                                     Heo vere shat                   gael | | | |          ree                    ap ® | Sos
on |     |    |   |    |     |   | FE ‘One-Shot    |            oe       |    |       |     i   | Fe& one-shot                 38 gp || one Shot        |         |
Li ty       2    |   :    a =
al    TTT.      |       a          a0 PS         =           ee          tal           :
0.1B:     O4B* 0887138 268: °67B {138                 4758;                  0.18.    104B 108B-4.3B. 26B    678; 133                SA75BE                 “QIBy    O48 20.88 1.3B: $268    67B? 13B                “178B 7
"Palatietses WLM (Billo                                                                                                           Paldimotare WLM @ilions)                                                                                                           ‘patameloe’ i LAN (BHR)
Figure. H.9:: All results:for all. ANLI rounds:


--------------------------------------------------


--- 第 67 页 ---

foyele letters}                                                         ss                         Wordscrarnbie’ (feweshot)                                                                                 mid word:4 ahaigrarhs ;
vex | [= Zero-Shot                  |           atl              Se) (OT veycielaiters:    |             we               on | [F= Geto Shot    5    |     |                  a
35 |e One-Shot                   |)          ago] {Ter ymidwoid'tanagrams| 2 OK          4 [=e One-Shot;             [oe
En          —            =   ee            250 {2 eto lnserton:   |     |          a                fa;         7      +     +         >
$8. 20 |_|] — |] 9 ee]           8                           a     2               £ 6,         ——   =    — a
8    Let         ;t~—S           °       |         |    lz                        “ & a | Se
<i ee:      ia    |                    90) pe      Se A nc           ol      ket |   aii        -
CS                                     7       =                                             o         =e      *    =F    fp
aye, O4B! "088 98 288 BB SB         wee          O18 3048! (088138) 268i S.7B: 138         U8.          SHB $48. FOBB19B 20B  iG7B 193:         ATS,
Parameters'in LM (Bilfons)                                 °                                                                       Parameters iri LM (Bilions)                                                                                                          Parameters in'tM (Billions)
.                    mid word’? anagrams:                                      Se,                     tandom insertion;                    .                   ae                      reversed words
3g |[Tt One sist:    |        |       xi             phe A               == Ohesshot,              ae      Sean
#2 ||=6= Few:shot (2100)                    ee                        os! FeweShot (K=100)/]      |          »                     OF [LHe Few-Shot («21007            — ff
g                         wD           Zz              se4of:_ —- Pf} 4, FG           oS                  oe         wi
a             > rn                 Bt      i         i    i   ae    ye             i               Pad          y
fas |             ia                         a Py | | LA 7 aa           © 92)§ ——p joy     ir s/n
co    |    lot                            a           L   va                     ,          Lvl |    a a
ex   [ee                 |         ‘      !        a ae   Le"                 on        A
:    =                                      ‘s         ee et              oS PLA
:01B' O48; Oak 3B eB GTB 3B.         A758          tone  (4B, 086. 138; 268) 678) 138         1758,          ‘Ov   048 1OSBIT3B i26B  f67B "13B:         ‘W75E!
‘Parameters In LM (Bilions)                                                                                                 Parameters in LM (Billions)                                                                                                  Paramheters LM (Billions):
Figure'H:10: Allresults for:all Scramble tasks:
Getivian’> English (SacteBLEU)                                                                              iEnglish’S:Gérman (SacreBLELY'                                                                               English’, Frénich’(SacreBLEU)
an | [== ZemStok    |               |      eee!               §0)/ [+ Zero-shot| |       !      !    l      ——                      10, [Zero Shot         |      |              TE
Ae Few-siai (Ke6djl] | at  lo.          |            495 | A= Few-Shot (K=64) |    |     |   |     edt               $85.1) “- Peweshot (E64) |i   les  cet T   _.    al
20, et  Se (id Re                      |       |    =  ae         wor               ao   ae     aed          a
q 2     SLA aii        BS. LA I                              pe       | aS  L     |    fp    “a                       LB      “1 nen damm         le foe
tA          :                                     sc ley aa     ON              «|| AOU | | fl Ne
4A   Lf’ |        :                                        Ke                                              gl ~A At A,          po
—    —|           :             |           £9,                                     .            af                                    '
‘QBy 048" 088 YB" 268! 8 7B. jsB                                  wee                                    OB” "O4BF O8B 138) 260) 8.78) 198                                  UBB.                                    SAB SAB SOSB138 268 6 YB 138.                                 “788.
‘Patameters'in LM (Bilions) ,                                    .                                                                              ‘Patameters'i4 LM (Billions)                                                                                                                     Parameters i'M (Bilions):
[French => Engligh’(SacreBLEU):                                                                                        :English >. Romanian (SacreBLEU):                                                                                      ‘Romanian’-> "English (SacreBLEU):
ao [=n zesho ‘|                       aia]              ¢95"| [—s— fero Shat       {     1 tt            oes                «go, [= Zeto-Shat         i     i        a      =i
TE, Qreshets              ea      |             Oo") eras Shot                [ee                    va | te Sneeshet   | |       .        ae
a8 || -F> Fewest e645 | a   eat              a= /Few-shol (Keod)|         ae                  B54 Ae rewrshot tog) ey
a 25           2                                  I              Seis               bz                                          3 25:        ,
Vl —— |       ease | |             a
{ae    {|    |     |   |               LC             tol a] Ty | | |,              0 ee
OB!  O48) [08B ASB 26 67: 3B         ATSB          (048  W048; O88 fab: 268: 67Bl tae         Te          os   O48 {OSBi73B (268 [67B Tsp.         eB)
‘Parameters in| LM (Billions) }                                                                                                   ‘Parameters iri LM (Billions)                                                                                                     Parametéts in LM (Billicns}:
~                    ‘German:-> English (Muti-BLEU).                                                 .                  English 2>' German (Muiti-BLEU);                   -                              7           -         English French (MultiBLEU),                    .
omens naan nae a           83g"         7      i     i   fo            a                    7       aoc enemnane nal              2
46 | [= zaestet             re    oe             190) SZ sot |     i    |) | ees           ee IL_|    ||          yy
we, || > One-Shot:           nie   Te               1 Gne-Shot             .    —     ~              330 || -—--One-Shot: —|    =    i    a
8 |e ewishet econ | — tren          ips] [Li Fewssnot eeeey]} —    A= ES            == Fewsnotecon || | ee]     es
co|   be Sy         |    |    |  J i ae    a         28:  1 oa       es
35       |  ee aie ai         |   ae    eet              320              eee     | pee              ian!          wb yee ee            ee
1B 20    Sip     So yet                              38) —__ Toate”             i ae                         3 sel       HE)      x     |        we
4    LZ Loy     “|                 2     OR t fe    Lx              at   AA | S|”    a
| LAI         va                                          i re) Aas oe any                J               10} vA ny Na
+ OABE  048: 088 138 268 67E 138         4758          0468  'G4B. O88. 138! 260. 678) 1B         1758"          (Rls!   048 10SB°13B (268 [67B “13H         7B
‘Parameters in, LM (Billions                                                                                                          Parameters in LM (Billions)                                                                                                          Parameters.in LM (Billions};
~                       JFrénich . English (Miult-BCEU)                      .                                                  English-Romanian (MulttBLEUS!                                                                     sRomanian -S. Engtish (MuIBLEU)
|= veecshot               a            sag Sewn |      l     tl          =o             90: (== -Zeroishot    [              ——————     fs
"| [= ewset cet |r| eH            = Few shel e04)]        eee                   ONY [Ee steueshal (Rees    al
8       Py       i       i             |           =              Lf                  xe             el     Fo ||    |
iB    thal    x    y         :       8        Ke |          LA          £ ||   WS         ge
aa    I] ;                                         aS          |    —                                 40 ra      [   |
O28] 048) 8B 128, 268 878) 3B         1758         118 (GHB) OBE 488) 268) GTB) 198         1758,         O18 04R [QSRIT2R i268 Ore 1B;         ATSB
Parameters in LM (Bilfons) «                                                                                                         Parameters in LM (Billions),                                                                                                          Parameters in LM (Biliens):
Figure, H.11: All-results for-all Translation tasks.,
‘67


--------------------------------------------------


--- 第 68 页 ---

References

[ADG*16] ‘Marcin Andrychowiez, Misha.Denil, Sergio Gomez, Matthew W Hoffman, David:Pfau, Tom Schaul,
Brendan. Shillingford, and-Nando;De Freitas:. Learning to‘learn by:gradient descent’by gradient descent.
In Advances in newral information processing systems, pages: 3981-3989, 2016.

[AT19] WeChat:Al. Tr-mt (ensemble), December'2019.

[AJF19] ‘Roee:Aharoni, Melvin Johnson, ‘and. Orhan.Firat., Massively ‘multilingual neural machine translation. -In,
Proceedings of the 2019-Conference of the:'North:American:Chapter of the:Association:for: Computational
Linguistics: Human Language Technologies; Volume 1 (Long and Short Papers), 2019.

[BBDIW20] Su-Lin Blodgett, Solon Barocas, Hal Daumé-IIl, and Hanna Wallach, Language (technology): is power:
Avcritical survey of “bias” innlp.. arXiv preprint arXiv:2005.14050;2020..

[BGFL13] Jonathan: Berant;, Andrew: Chou, Roy Erostig, and. Percy Liang. Semantic parsing‘on ‘freébase-from.
‘question-answer pairs. In Proceedings of the 2013 conference on: empirical methods in natural language
processing, pages: 1533-1544, 2013.

[BDD*09] Luisa.Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo:Giampiccolo, and Bernardo, Magnini. The:fifth.
PASCAL recognizing textual.entailment:challenge. '2009.,

[BES10] Stefano Baccianélla, Andréa Esuli, and Fabrizio Sebastiani, Sentiwordnet 3.0) an enhanced lexical,
resource:for sentiment analysis and opinion mining. In.L7ec;;volumie, 10, pages 2200-2204, 2010.

[BHDD*06] ‘Roy Bar-Haim, Ido:Dagan, Bill Dolan,.J:isa Ferro, Danilo Giampiccolo, Bernardo Magnini,.and-Tdan.
Szpektor; The second PASCAL recognising textual,entailment challenge: 2006:

[BHT+20] Yonatan Bisk, Ari, Holtzman; Jesse, Thomason; Jacob Andreas; Yoshua, Bengio, Joyce Chai, Mirella
Lapata,-Angeliki Lazaridou, Jonathan May, Aleksandr.Nisnevich, et al, Experience: grounds language.
arXivpreprint arXiv: 2004.10 15.1, 2020.                               7         ;         /

[BLC13] “Yoshua Bengio, Nicholas Léonard!, and.Aaron‘G. Courville. Estimating or:propagating gradients through.
stochastic neurons for‘conditional computation... Arxiv, 2013.

[BZB*19] ‘Yonatan Bisk, Rowan Zellers, Ronan-Le Bras, Jianfeng Gao,,and Yejin Choi, Piqa; Reasoning about:
‘physical commonsense in natural language. arXiv preprintiarXiv:19T 1.11641, 2019.

[Car97] ‘Rich:Caruana.. Multitask learning. Machine learning, 28(1);, 1997.

[€B78] Swsati Carey and Elsa Bartlett, Acquiring asinglé new word. Proceedings of the Stanford Child Language

[CCE*18] ‘Peter Clark; Isaac Cowhey; Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, ‘and.
‘Oyvind Tafjord.. Think youwhaveisolved question: answering? tryiarc; the ai2:reasoning challenge: ArXiv,
-abs/1803:05457, 2018.

[CGRS19] ‘Rewon Child, Scott Gray, Alec Radford, and-Ilya Sutskeyer, Generating long sequences with sparse.
‘transformers, 2019.                                                                                  BO

[CHI*18] ‘Eunsol, Choi, He: He; Mohit Tyyer,.Mark ‘Yatskar,. Wen-tau, Yih; ‘Yejin Choi, Percy Liang, and Luke:
Zettlemioyer. Quac + Question answering in context, Arxiv, 2018.

[CLE*19] ‘Christopher Clark, Kenton.Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina.
Toutanova. BoolQ: Exploring ‘the, surprising; difficulty of natural yes/no questions. arXiv! preprint
arXiv:1905.10044, 2019.

[CLY*19] ‘Yen-Chuni Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed; Zhe Gan, Yu Cherig; and,

                Jingjing-Liu. Uniter: Learning: universal image-text:representations.. a7Xiv, preprint arXiv 1909..1.1740,

[Cra17] ‘Kate:Crawford: The trouble with.bias: NIPS 2017 Keynote; 2017:

[DCLT18] Jacob Devlin; Ming-Wei Chang;. Kenton Les, and Kristina Toutatiova. BERT: Pre-training. of deep
bidirectional transformers.for language understanding. .arXivipreprint:arXiv:1810,04805, 2018.

68


--------------------------------------------------


--- 第 69 页 ---

[DGM06] :Ido Dagan, ‘Oren, Glickman, ‘and Bernardo: Magnini.. The PASCAL recognising ‘textual entailment:
‘challengé., In Machine learning challenges. evaluating predictive uncertauity, visual object Classification,
“and recognising textual entailment, pages 177-190. Springer, 2006.

[DGVt18] Mostafa Dehghani, Stephan, Gouws, Oriol, Vinyals, Jakob: Uszkoreit, and Lukasz, Kaiser. Universal,
‘transformers. Aviv, 2018:.                                            -

[DHKH14] Nadir-Durrani,- Barry: Haddow, Philipp Koehn, and-Kenneth Heafield.. Edinburgh’s phrase-based machine
‘translation systems:for wmt-14., In.Proceedings.of.the Ninth Workshop.on Statistical: Machine Translation,
pages'97—104, 2014.

[DL15] Atidrew M. Dai atid Quoc V. Le. Semi-supervised sequence learning. In Advances in-neural information
processing systems,:2015..
[DMST19] Marie-Catherine: De Mameffe, Mandy: Simons,-and Judith Tonhauser; ‘The CommitmentBank: Investigat-
‘ing:projection:in naturally: occurring discourse. 2019. To:appear‘in proceedings of Sinn’und:Bedeutung’
23; Data:can be found.at/https://github:com/medm/CommitmentBank/.,
[DSC*16] “Yan Duan, John Schulman, ‘Xi Chen; Peter L. Bartlett, Ilya, Sutskever, and Pieter Abbeel. RI*: Fast:
‘reinforcement learning Via slow reinforcement learning. ArXiv, abs/1611.02779,, 2016.

[DWD+19] Dheeri, Dua, Yizhong Warig, Pradeep Dasigi, Gabriel, Stanovsky, Sameer Singh; and, Matt Gardner.
‘Drop: A reading-comprehension benchmark:requiring discrete‘reasoning over paragraphs. arXiv preprint’
arXiv:1903:00161, 2019.

[DYY*19] ‘Zihang: Dai, Zhilin Yang,.Yiming; Yang,.Jaime;G. Carbonell, Quoc: V.:Le,, and Ruslan.Salakhutdinov.
Transformer-xl: Attentive language-models, beyond a fixed-length context: Arxiv; 2019:

[EOAG18] Sergey Edunoy,-Myle Ott, Michael Auli, and 'David:Grangier: Understanding back-translation at.scale.

arXiv preprint arXiv: 1808.09381, 2018:
[FAL17] Chelsea Finn, Piéter Abbeel, and,Sergey Levine. Model-agnostic meta-learning forfastiadaptation:of
deep:networks. ArXiv, Abs/1703.03400, 2017.
[Fyo00] Yaroslav. Fyodoroy. A natural-logie inference system, 2000,
[GG19] Hila'Gonen and Yoav:Goldberg. Lipstickon aipig: Debiasing methods coverup systematic gender biases
in word. embeddings but do:notsremove them: .arXiv:preprint.arXiv:1903:03862, 2019.
[GLT*20] :Kelvin'Guu, Kenton. Lee,. Zora’ Tung, Panupong :Pasupat;, and Ming-Wei. Chang. Realm: Retrieval-
augmented.Janguage:model pre-training:, .arXivpreprint-arXiv:2002/08909, 2020.,

[GMDD07] :Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, .and Bill Dolan. The:third PASCAL recognizing
‘textual entailment'challenge. In Proceedings of the ACL-~PASCAL workshop omtextual entailmentand
paraphrasing, pages 1-9..Association-for Computational Linguistics, 2007.

[Gral6] Alex Graves. Adaptive:computation time forrectrrent neural networks. Avxiy, 2016.
[GSL*18] Suchin Gururangan, Swabha Swayamdipta, Omer.Levy, Roy Schwartz, Samuel R. Bowman,.and Noah.A,
Smith.. Annotation artifacts:in natural languagesinference data. arXiv,preprint arXiv:1803.02324, 2018.
[GSR19] Sebastian -Gehrmann, ‘Heridrik Strobelt, arid.Aléxander'M. Rush. Gltrs Statistical .détection and viswaliza-
‘tion of generated text. arXiv. preprint. arXiv. 1906.04043, 2019.
[GWCT18] Jiatao Gu; Yong Wang; Yun Chen, Kyunghyun.Cho,:and. Victor OK.Li.. Meta-learning for low-resource
neural machine translation: arXiv preprint.arXiv:1808:08437; 2018:
[HB20] Daniel Hernandez:and Tom Brown:, Ai.and efficiency; May: 2020.
[HBFC19] Ari Holtzman, Jan: Buys, Maxwell Forbes, and.Yejin Choi. The curious:case/of neural ‘text degeneration.
‘CORR, abs/1904,09751, 2019.

[HLW*20] Dai Hendrycks, Xiaoyuan Liv, Eric Wallace, Adam Dziedzic; Rishabh Krishnan, and Dawn, Song.

‘Pretrained transformers.improve out of distribution robustness.. arXiv preprint arXiy:2004,06100, 2020.


--------------------------------------------------


--- 第 70 页 ---

[HNA*17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan’ Kianinejad, Md.

Mostofa. Ali Patwary, Yarig Yang; and: Yangi Zhou: Deep learning scalinigis-predictable,. émpitically.
[HR18] Jereniy Howard and Sebastian Ruder: Universal langtiage model, fine-tuning for text classification: arXiv
preprint arXiv:1801,06146, 2018.

[HVD15] Geoffrey Hinton, Oriol Vinyals, atid Jeff’ Dean. Distilling: the: knowledge in. anetral network. arXiv
preprint arXiv:1503.02531, 2015.

[HYCOI] Sepp Hochreiter, A Steven, Younger, and.Peter R Conwell. Leariing'to Learn Using Gradient Descent.
In hiternational Conference on Artificial Neural.Networks, pages 87-94:.Springer, 2001.

[HZJ*19] Po-Sen Huang, Huan, Zhang; Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Main,
‘Dani Yogatama, and-Pushmeet-Kohli.. Reducing sentiment 'bias in language models ‘via counterfactual.
evaluation. arXiv preprint arXiv:1911.03064,:2019.                            :

[IBGC*14] ‘Mohit Iyyer; Jordan Boyd-Graber, LeonardoClaudino, Richard Socher, and: Hal:-Daumé IIT, A neural.
network. for:factoid question answering over‘paragraphs. In:Empirical Methods in Natural Language:
Processing; 2014.

[IDEBE19] ‘Daphne Tppolito, Daniel Duckworth, Chris Callison-Burch, and. Douglas Eck. Automatic detection:of
generated text is easiest when, humans are:fooled. arXiv.preprint arXiv:1911-00650, 2019:

[JCWZ17] ‘Mandar Joshi, Eunsol, Choi, Daniel S. ‘Weld, and Luke Zettlemoyer. ‘TriviaQA: .A large scale distantly:
supervised challenge dataset for reading comprehension: arXiv preprint arXiv:1705.03551;'2017.

[JN20] ‘Zheng Junyuan:and-Gamma Lab'NYC. Numeric transformer:-:albert, March 2020.

[JVS*16] ‘Rafal Jozefowicz, Oriol Vinyals, Mike: Schuster, Noam Shazeer, and. Yonghui Wu, Exploring the limits
‘of language modeling. arXiv. preprint arXiv:.1602.02410, 2016.                                         -

[TYS*19] “Xiaogi Jiao, Yichun. Yin, Lifeng Shang, Xin. Jiang, Xiao. Chen, Linlin Li, Fang Wang, and:Qui Liv,
‘TinyBERT: Distilling: BERT‘for.natural languageunderstanding: .arXiv.preprint arXiv:L909:10351,:2019.

[IZC*19] “Ying: Ju,-Fubang Zhao, Shijie:;Chen,, Bowen Zheng; Xuefeng; Yang,/and Yunfeng:iu. Technical:report on.
‘conversational question: answering. arXiv, preprint arXiv:1909. 1 0772,.2019..

[KCR#*18] ‘Daniél-Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan:Roth. Looking beyond.
‘the surface: .A,challenge:set for reading comprehension over ‘multiple:sentences.. In. Proceedings of North.
American Chapter of the Associationgfor Computational Lingustics'(NAACL), 2018.

[KKS*20] ‘Daniel-Khashabi, Tushar‘Khot,-Ashish Sabharwal, Oyvind:Tafjord, Peter‘Clark, and Hannaneh:Hajishirzi.
Unifiedqa: Crossing format boundaries With,a singleqa system, arXiv preprint:arXiv;2005:00700; 2020.

[KMB20] Sarah E..Kreps, Miles McCain, and:Miles Brundage. .All the:news that’s fit'to fabricate: .Ai-generated,
‘text.as a tool of media misinformation, 2020.

[KMH*20] Jared Kaplan,.Sam McCandlish, Tom Henighan, Tom B.. Brown, Benjamin Chess, Rewon Child, Scott:
Gray, Alec Radford, Jeffrey Wu; and Dario Amodei. Scaling. laws ‘for netiral language models, 2020.

[KPR*19] ‘Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,.Michaeli Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, lia Polosukhin,. Matthew Keleey, Jacob Devlin; Kenton Lee, Kristitia.N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-
‘tions: a.benchmark:for question answering:research. Transactions of the.Association:of Computational
Linguistics; 2019),

[KR16] “Yoon Kimsand Alexander M. Rush. Sequence-level- knowledge  distillation.. Arxiv, 2016:

[LB02] Edward Loperand.Steven-Bird. Nitk: The:natural Janguage:toolkit,2002..

[LE19] -Guillaitne Lample and.Alexis Conneat. Cross-lingtial language model pretraining. arXiv preprint
a? Xiv3 190107291, 2019.


--------------------------------------------------


--- 第 71 页 ---

[LCGT19] ‘Zhenzhong Lan, Mingda Chen,, Sebastian, Goodman, ‘Kevin, Gimpel, Piyush Sharma, ‘and‘Radu Sori-
‘cut, ALBERT: A, lite BERT for sélf-superviséd learning-of language representations. arXiv preprint
arXiv21909,1.1942,, 2019,

[LCH* 20] ‘Xiaodong;Liu,.Hao:Cheng, Pengcheng He, Weizhu Chen, Yu Wang,,Hoifung Poon, and Jianfeng Gao.
Adversarial training for large neural language models. arXiv preprint arXiv:2004,08994, 2020.

[LDL19] ‘Zhongyang Li, Xiao Ding; and Ting Liu. Story ending prediction by transferable:bert. arXiv preprint
arXiv:1905:07504, 2019.

[LDM12] Hector Levesque; Emest Davis, and Leora Morgeristem. The Winograd schema challenge: In Thirteenth
International Conference on.thé Principles of Knowledge. Réprésentation.and. Reasoning, 2012.

[LGG*20] ‘Yinhan‘Liu; Jiatao-Gu; Naman,Goyal; Xian‘Li, Sergey Edunoy, Marjan Ghazvininejad,.Mike'Lewis,and,
Luke Zettlemoyer. Multilingual denoising pre-training for.neural machitie translation. arXiv preprint
<arXiv:2001:08210, 2020.

[I-GH*15] ‘Xiaodong Liu; Jianfeng: Gao; Xiaodong He, Li Deng, ‘Kevin Duh, and Ye-Yi Wang: Representation.
leathing wsing multi-task deep netiral Hetworks for sémanticclassification and information retrieval. In,
Proceedings of the 2015: Conference of theNorth:American:Chapter of the.Association'for Computational
Linguistics: Human Language Technologies, 2015..

[LH17] Jlya Loshchilov and. Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv l7TLO5101, 2017.
[EHCG19a] ‘Xiaodong Liu; Pengcheng ‘He, Weizhu Chen, and Jianfeng Gao. Improving: multi-task deep: neural.
networks Via knowledge distillation for natural language understanding. arXiv preprintarXx1v:1904.09482,
2019.,                            a               -       Oe
[EHCG19b] “Xiaodong’Tiu,. Pengcheng ‘He; Weizhu;Chen, and Jianfeng Gao. Multi-task:deep:neural networks for:
natural language understanding. arXiv preprint arXiv21 90111504, 2019.
[Lin20] “Tal Linzen.. How can we accelerate:progress towards human-like linguistic generalization? arXiv preprint
arXiv:2005:00955, 2020.

[LLG*19] Mike Lewis, Yinhan Liu,.Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
‘Ves Stoyanoy,, and-Luke Zettlemoyer. Bart: Denoising; sequence-to-sequence pre-training: for‘natural.
Janguage:. generation, translation,,and;comprehension.., arXiv preprint-arXiv-1910.13461,,2019..,

[LM17] .Ke Li and Jitendra Malik, Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017.

{LOG*19] ‘Yinhan Liu,-Myle Ott, Naman.Goyéal, Jingfei Du, Mandar.Joshi, Dangi Chen,;Omer:Levy;.Mike Lewis,
Luke Zettlemoyer; and, Veselin Stoyanov: RoBERTa: :A.robustly optimized. BERT retraining approach:

[LPPt 20] ‘Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir‘Karpukhin, Naman Goyal,
‘Heinrich, Kittler; Mike Lewis; Wen-tau, Yih; Tim Rocktaschel,, Sebastian, Riedel; and Kiela Douwe.
Retrieval-augmented generation for knowledge-intensive:nlp tasks. arXiv preprint arXiv:2005.11401,

[LSPt18] Peter.J. Liu, Mohammad Saleh,-Etienne Pot; Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam.
Shazeer. Generating Wikipedia by summarizing long sequences. arXiv, preprint arXiv:1801.10198,,2018.
Train large; then.compress: Rethinking model:size forefficient training:and. inference of transformers;
2020,

[LXL*17] -Guokun Lai, Qizhe ‘ie, Hanxiao: Liu, Yiming: Yang, and’ Eduard Hovy. ‘Race: Large-scale reading
‘comprehension:dataset fromexaminations: ‘arXiy preprint arXiv:1704.04683;, 2017:

[LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy
Lin. Titttackling:winogrande schemas. arXiv preprint arXiv:2003.08380, 2020.                                          :

[Mac92] David. MacKay. Information-baseéd objective functionsfor activedata selection. Neural Computation,
1992.                        7                                                        :


--------------------------------------------------


--- 第 72 页 ---

[MBXS17] ‘Bryan McCann, James Bradbury, Caiming:Xiong, and‘Richard Socher. Learned in ‘translation: 'Con-
‘textialized Word Vectors. In Advances in Neural Information Processing: Systems; pages:6294~-6305,
2017.

[MCCD13] ‘Tomas Mikolovy, KaisChen, Greg Corrado, and Jeffrey Dean. Efficient:estimation of: word.representations.
in veetor space, arXiv-preprint arXiv: 13013781, 2013.

[MCH*16] ‘Nastin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhitiv Batra, Lucy Vatiderwende,
Pushmeet:Kohili, and James Allen. -A. corpus. and. evaluation framework for'deeper understanding :of
commonsense stories. arXiv,preprint arXiv: 1604.01696, 2016.                                        ,

[MCKS18] ‘Todor. Mihaylov; Peter Clark; Tushar Khot, and-Ashish Sabharwal; Cana suitofarmor:conduct electricity?
anew dataset for open book question answering. ArXiv, abs/1809.02789, 2018.

[MKAT18] Sam. McCandlish, Jared. Kaplan; Dario Amodei, and ‘OpenAL Dota ‘Team. An empirical model of
large-batch training: 2018.

[MKM+94] ‘Mitchell Marcus, Grace Kim; Mary Ann Marcinkiewicz; Robert MacIntyre; Ann Bies, Mark Ferguson;
‘Karen Katz; and Britta Schasberger: The. penn treebank: annotating’ predicate argument structure.
In.Proceedings of the workshop. on Human Language Technology, pages 114-119: Association. for
‘Computational'Linguistics, 1994;

[MKXS18] ‘Bryan McCann, Nitish Shirish, Keskar, Caiming Xiong, ard Richard. Socher, ‘The natural, language.
decathlon: .Multitask learning as question answering. arXiw preprint GrXiv21806.08730, 2018..

[MPI:19] .R-Thomas McCoy, Ellie.Pavlick;, and Tal Linzen. Right for‘therwrong reasons: Diagnosing syntactic.
‘heuristies;in natural'language inference: arXiy preprintarXiy:1902.0L007; 2019
[MWZ*18] Margaret Mitchell, Simone. Wu,.Andrew Zaldivar, Parker Barnes; Lucy Vasserman, Ben’ Hutchinson,
Elena‘ Spitzer, Inioltiwa.Deborah: Raji, atid Timnit Gebru. Model cards for:modelreporting, 2018.
[NBR20] Moin.Nadeem, Anna Bethke,,and.Siva.Reddy. Stereoset:: Measuring stereotypical! bias in-pretrained.
‘Janguage:models: arXiv preprint arXiv:2004:09456; 2020:
[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments.
arXiv preprint GrXiw:1907-073 55, 2019.
[Nor09] ‘Peter'Norvig:. Natural language corpusdata,;2009..
[NvNvdG19] Malvina Nissim; Rik-van Noord, and-Rob:van der, Goot: Fair, is. better‘than: sensational; Man is'to doctor
ag Wonlan.is:to doctor arXiv preprint arXiv1905.09806, 2019.
INWD*19] ‘Yixin.Nie, Adina Willianis, Emily Dinan, Mohit-Bansal, Jason Weston, and Douwe.Kiela. Adversarial.
cli: A new: benchmark.for natural language:understanding. arXiv preprint arXiv: 191014599, 2019.
{oR16] University:of Regensburg: Fascha; 2016:
[PCC18] Mohammad Taher Pilehvar and Jose: Camacho-Collados. WIC: 10,000-examiple pairs ‘for evaluating
Context-sensitiverépresentations...arXiw préprint G7Xiv21808.0912 1, 2018..
[PEB18] Jason.Phang, Thibault'Févry, and SamuelR. Bowman. Sentence encoders on STILTs: Supplementary’
‘training on.intermediate’ labeled-data tasks: arXiv. preprint arXiv:1811,01088, 2018:

[PHR+18] Adam Poliak; Aparajita Haldar; Rachel Rudinger, J, Edward Hu; Ellie Pavlick;, Aaron, Steven, White; and.
‘Benjamin: Van:Durme. Collecting diverse natural.language inference problems‘for sentence:representation.
‘evaluation. In;Proceedings: of EMNLP,.2018.

[PKL*16] Denis Papemo, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi ,,Sandro:
Pezzelle, Marco Baroni,-Gemma Boleda, and Raquel- Fernandez. The:lambada-dataset: Word prediction.
‘requiring a.broad:discourseicontext. arXiv preprintiarXiv:1606.06031, 2016.                                          ,

[PNZtY.18] ‘Matthew. Peters, Mark Neumann, Take Zettlemoyer; and Wen tau.Yih. Dissecting contextual word.
embeddings: Architecture and representation, 2018.

[Pos18] Matt Post, .A call-for clarity in reporting BLEU scores... arXiv preprint arXiy:1804,08771,;2018.


--------------------------------------------------


--- 第 73 页 ---

[PSM14] Jeffrey. Pennington, Richard Socher, and.‘Christopher. Manning. GloVe: Global ‘vectors for word.
representation. In, Proceedings’ of the 2014 conference on empirical methods’ in. ndtural language
processing (EMNLP), 2014.

[QIA20] ‘QIAN XIN. Sa-net:on albert (ensemble),.-April 2020.

[QMZH19] ‘Yusii Qian, Urwa Muaz, Ben Zhang; andJae Won Hyun. Redticing gender bias in word-level langtiage.
smodels witha gender-equalizing Joss function... arXivpreprint arXiv71905.1280L, 2019.

[RBG11] ‘Melissa Roemmele; Cosmin Adrian’Bejan; and Andrew: S Gordon. Choice of plausible alternatives: An.
evaluation of Commonsense:causal reasoning. In 2017 AAAT Spring Symposium Series, 2011,

[RCM19] Siva Reddy;:Dangi Chen, and ‘Christopher. D Manning. ‘Coqa: A.conversational. question ‘answering
‘challenge. Transactions.of the Association for Computational Linguistics, 7:249-266, 2019.

[RCP+17] Scott Reed, Yutian. Chen, Thomas Paine, Aaron van. den Oord, SM Eslami, Danilo Rezende, Oriol.
Vinyals, and. Nando de:Freitas. Few-shot.autoregressive density‘estimation: Towards learning to‘learn
‘distributions: arXiv preprint arXiv: 1710.10304, 2017:

[RJL18] -Pranay Rajpurkar,,Robin.Jia,.and.Percy Liang.. Know what you don’t know? Unariswerable questions for
squad. arXiv preprint.arxiv:1806.03822, 2018.                            ;
[RL16] Sachin Ravi:and. Hugo Larochelle; ‘Optimization as a model for few-shot learning: JCLR 2017 (oral),
~ 2016.                      .                                                            .                     oe

[RLLT19] ‘Qiu Ran, Yankai.Lin, ‘Peng Li, Jie'Zhou, and Zhiyuan Liu.. NumNet: Machine reading comprehension.
With ntimerical reasoning: Ih Proceedings of EMNLP, 2019.

[RNLV.D18] ‘Rachel. Rudinger; Jason. Naradowsky, Brian Leonard, and ‘Benjamin. Van -Durme: Gender: bias in.
‘coreference resolution. arXiv preprint arXiv:1804.09301, 2018.

IRNSS18] Alec:Radford; Karthik Narasimhan, Tim Salimans, and Ilya Sutskever: Improving language understanding
‘by generative:pré- training, 2018:

[Ros12] ‘R:S: Ross: Guide for:conducting-risk assessments: NIST Special Publication; 2012:

[RRBS19] Jonathan 'S.-Rosenfeld, Amir Rosenfeld, Yonatan-Belinkoy, and Nir Shavit. A.constructive prediction’ of
‘the generalization error‘acrossiscales,;2019.

[RRS20] Adam Roberts; Colin Raffel; arid Noam Shazeer, How much knowledgecan you.pack into the parameters
of a language model? arXiv preprint arXiv:2002.08910, 2020..

[RSR*19] ‘Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee; Sharan Narang, Michael Matena, Yanqi.
Zhou, Wei, Li, and PeterJ. Lit. Exploring the limits of transfer learning, with a unified. text-to-text
‘tranistorier, 2019,

[RWCE*19] Alec Radford, Jeffrey Wu; Rewon.Child, David Luan, Dario.Amodei, and,Tlya Sutskever. Language:
Models aretiisupervised multitask leatners, 2019,

[SBBC19] :Keisuke:Sakaguchi, Ronan Le Bras,’Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial.
‘winograd schema challengeatiscale, 2019.

[SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda,Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,
‘Gretchen: Krueger, Jong Wook-Kim, Sarah.Kreps, Miles McCain,.Alex:Newhouse, Jason’ Blazakis, Kris
‘McGuffie; and Jasmine Wang.. Release’ strategies:and the: social impacts'of language models, 2019.

[SCNP19] Emily Sheng: Kai-Wei, Chang, Premkuiiar Natarajan, and Nanyun, Peng. The woman worked as a,
‘babysitter: On biases:in language generation.. ueXiv, preprint arXiv:1909.01326,.2019..

[SDEW19] Victor Sanh, Lysandre: Debut; Julien: Chaumond, and Thomas Wolf. DistiIBERT, a distilled version of
BERT: smaller, faster, cheaper’and lighter. arXiv preprint arXiv:1910.01108, 2019.

[SDSE19] ‘Roy: Schwartz:, Jesse:Dodge;, Noah A. Smith, and Oren.Etzioni. Green AI. CoRR, ‘abs/1907.10597, 2019.

[SHB15] Rico.Sennrich, Barry Haddow, and.Alexandra Birch. Improving netiral machiné translation models with,
‘monolingual data. arXiy preprint arXiv:1511.06709, 2015.


--------------------------------------------------


--- 第 74 页 ---

[SMM*'17] NoamShazeer, Azalia’Mirhoseini, Krzysztof Maziarz; Andy‘Davis, Quoc Le, Geoffrey Hinton, and Jeff’
Dean, Outrageously large netiral networks: The-sparsely-gated mixture-of-experts.layer. arXiv preprint
arXiv:1701:06538, 2017.

[SPP+:19] Mohammad:Shoeybi, MostofaPatwary, Raul Puri, Patrick‘LeGresley, Jared Casper, and Bryan Catanzaro.
‘Megatron-Im: ‘Training multi-billion-parameter language models;using model parallelism; 2019:

[SS20] ‘Timo Schick and,Hinrich Schttze. Exploiting cloze.questions for few-shor text-classification and natural,
danguagecinference. arXiv preprint arXiv:2001.07676, 2020.

[STQ*19] ‘Kaitao Song, Xu.Tan, Tao Qin, Jianfeng Lu, and. Tie-Yan.Liu.,. MASS: Masked sequence to sequence:
(pre-training'for language generation. arXiv preprint arxiv:1905.02450, 2019.

‘[TFR*17] Josh Tobin, Rachel Fong, Alex: Ray; Jonas:Schiieider, Wojciech Zaremba, and Pieter.Abbeél. Domain.
randomization for transferring deep neural:networks:from:simulation to the:real. world.. In.2017. IEEE/RSJ
uiternational conference on intelligent robots and systems (IROS); pages 23-30. IEEE, 2017.

[TL05] -Peter D, Turney’ and-Michael.L, Littman, Corpus-based learning of analogies and semantic 'elations,
‘CoRR, ‘abs/es/0508103, 2005.                          7                          .

[TL18] ‘Trieu H. Trinh and Quoe:'V. ‘Le; A  simple:method, for commonsense reasoning: arXiv preprint:
arX1V21806:02847, 2018.

[TLBS03] ‘Peter. D. Turney,,Michael.L. Littman, Jeffrey Bigham, ‘and ‘Victor Shnayder. Combining independent:
smodules’to solve multiple-choice synonym and analogyproblems. CoRR, cs:CL/0309035; ‘2003.

[Tut20] Project Turing: Microsoft research blog; Feb 2020,

[VBL*16] ‘Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et.al. Matching Networks for One.
Shot:Learning:, In Advances inneural information processing systems, pages 3630-3638), 2016.

[VSP+17] Ashish ‘Vaswani, Noary Shazeer, Niki Parmar, Jakob Uszkoreit; Llion,Jones; Aidan N. Goniez, Lukasz,
‘Kaiser, and Ilia Polosukhin, Attention isiall you need. In Advances in neural information processing
systems: 2017.                                                                   :                                                                          -                       |

[WPN*19] Alex Wang; Yada Pruksachatkun;-Nikita Nangia; Amanpreet Singh, Julian Michael, Felix Hill; Omer
Levy, and Samuel-Bowman.. Superglue: A stickier benchmark-for general-purpose language'understand-
ing:systems, In.Advances. in Neural. Information Processing Systems, pages 3261-3275, 2019.

[WXH*18] ‘Yiren Wang, Yingce Xia; Tianyu He;Fei Tian; Tao Qin, Cheng Xiang Zhai, and. Tie-Yan Liu.. Multi-agent:
dual learning. 1CLR:2019, 2018.

[XDH*19] ‘Qizhe: Xie, Zihang Dai, Eduard. Hovy, Minh-Thang ‘Luong, and Quoc V. Le. Unsupervised. data.
augmentation foriconsistency: training;:2019.

[YdE*19] Dani Yogatama;, Cyprien de Masson d’ Autume; Jerome: Connor, Tomas Kocisky, Miké:Chrzanowski;
Lingpeng Kong, Angeliki.Lazaridou,, Wang Ling, Lei. Yu, Chris Dyer, etal. earning ‘and evaluating
general linguistic.intelligence.. arXiv. preprint‘arXiv-1901.11373,2019..

[YDY+19] ‘Zhilin Yang, Zihang Dai, Yiming Yang; Jaiiie Carbonell, Ruslan Salakhitdinov,,and Quoc:V. Le. XLNet!
‘Generalized ‘autoregressive pretraining for language*understanding. arXiv preprint arXiv:1906.08237,

[ZHB*+19] ‘Rowan Zellers;:Ari Holtzman, Yonatan Bisk; Ali Farhadi; and, Yejin Choi: ‘Hellaswag: Can:a.machine-
really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

[ZHR*19] Rowan Zellers, Ari Holtzman, Hannah.Rashkin, Yonatan.Bisk, Ali Farhadi, Franziska Roesner, and. Yejin.
‘Choi. Defending against neural fake news. arXiv preprint arXiv:1909.12616, 2019.

[ZLL*18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and. Benjamin Van Duime.
-ReCoRD: Bridging the gap.between human and.niachine commionserise reading: comprehension. ar-Xiv
preprintarXiv:1810.12885, 2018.

[ZSW*19a] Daniel M. Ziegler, Nisai, Stiennon, Jeffrey Wu, Toni B. Brown, Alec Radford, Dario Amodeéi, Paul,
‘Christiano, and Geoffrey Irving. Fine-tuning language models from-human preferences, 2019.

74


--------------------------------------------------


--- 第 75 页 ---

‘tiano, and Geoffrey Irving. Fine-tuning language models from human preferences, ArXiv, abs/1909.08593;


--------------------------------------------------
