#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆarXivè®ºæ–‡æ‘˜è¦æŠ“å–å™¨
ä¸»è¦ä½¿ç”¨APIå’ŒTrafilaturaè¿›è¡Œæ•°æ®æå–å’Œæ¸…ç†
"""

import json
import requests
import time
import logging
from typing import List, Dict
import xml.etree.ElementTree as ET
import os
import sys

# å¯¼å…¥ä¾èµ–åº“
try:
    import trafilatura
except ImportError:
    print("Missing trafilatura package. Please install it:")
    print("pip install trafilatura")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleArxivScraper:
    def __init__(self, category: str = "cs.CL", max_results: int = 200):
        """
        åˆå§‹åŒ–ç®€åŒ–ç‰ˆarXivæŠ“å–å™¨
        
        Args:
            category: arXivå­ç±»åˆ« (ä¾‹å¦‚: cs.CL, cs.AI, cs.LG)
            max_results: æœ€å¤§æŠ“å–ç»“æœæ•°é‡
        """
        self.category = category
        self.max_results = max_results
        self.base_url = "http://export.arxiv.org/api/query"
        
    def query_arxiv_api(self) -> List[Dict]:
        """
        é€šè¿‡arXiv APIæŸ¥è¯¢è®ºæ–‡
        
        Returns:
            åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æŸ¥è¯¢arXiv APIï¼Œç±»åˆ«: {self.category}, æœ€å¤§ç»“æœæ•°: {self.max_results}")
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {
            'search_query': f'cat:{self.category}',
            'start': 0,
            'max_results': self.max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            root = ET.fromstring(response.content)
            entries = []
            
            # è·å–å‘½åç©ºé—´
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', ns):
                # æå–åŸºæœ¬ä¿¡æ¯
                paper_id = entry.find('atom:id', ns).text
                arxiv_id = paper_id.split('/')[-1]
                
                title = entry.find('atom:title', ns).text.strip()
                title = ' '.join(title.split())  # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
                
                summary = entry.find('atom:summary', ns).text.strip()
                summary = ' '.join(summary.split())  # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
                
                # æå–ä½œè€…
                authors = []
                for author in entry.findall('atom:author', ns):
                    name = author.find('atom:name', ns).text
                    authors.append(name)
                
                # æå–æ—¥æœŸ
                published = entry.find('atom:published', ns).text
                date = published.split('T')[0]  # åªå–æ—¥æœŸéƒ¨åˆ†
                
                entries.append({
                    'arxiv_id': arxiv_id,
                    'url': f"https://arxiv.org/abs/{arxiv_id}",
                    'title': title,
                    'abstract': summary,
                    'authors': authors,
                    'date': date
                })
                
            logger.info(f"æˆåŠŸè·å– {len(entries)} ç¯‡è®ºæ–‡ä¿¡æ¯")
            return entries
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢arXiv APIå¤±è´¥: {e}")
            return []
    
    def enhance_with_trafilatura(self, papers: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨Trafilaturaå¢å¼ºè®ºæ–‡ä¿¡æ¯
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            å¢å¼ºåçš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        enhanced_papers = []
        
        for i, paper in enumerate(papers):
            logger.info(f"å¤„ç†è®ºæ–‡ {i+1}/{len(papers)}: {paper['arxiv_id']}")
            
            try:
                # è·å–æ‘˜è¦é¡µé¢HTML
                response = requests.get(paper['url'], timeout=15)
                response.raise_for_status()
                
                # ä½¿ç”¨Trafilaturaæå–æ¸…ç†åçš„å†…å®¹
                extracted = trafilatura.extract(
                    response.text,
                    include_comments=False,
                    include_tables=True,
                    include_formatting=True,
                    include_links=False
                )
                
                if extracted and len(extracted.strip()) > len(paper['abstract']):
                    # å¦‚æœTrafilaturaæå–çš„å†…å®¹æ›´ä¸°å¯Œï¼Œä½¿ç”¨å®ƒ
                    paper['abstract'] = extracted.strip()
                    logger.debug(f"ä½¿ç”¨Trafilaturaå¢å¼ºå†…å®¹: {paper['arxiv_id']}")
                
                # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡åº¦è¯·æ±‚
                time.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"Trafilaturaå¤„ç†å¤±è´¥ {paper['url']}: {e}")
                # ç»§ç»­ä½¿ç”¨åŸå§‹æ‘˜è¦
            
            enhanced_papers.append({
                'url': paper['url'],
                'title': paper['title'],
                'abstract': paper['abstract'],
                'authors': paper['authors'],
                'date': paper['date']
            })
        
        return enhanced_papers
    
    def save_to_json(self, papers: List[Dict], filename: str = "arxiv_clean.json") -> bool:
        """
        ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONæ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶å¤§å°ä¸è¶…è¿‡1MB
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶ (1MB)
            temp_data = json.dumps(papers, ensure_ascii=False, indent=2)
            original_count = len(papers)
            
            # å¦‚æœæ•°æ®è¶…è¿‡1MBï¼Œé€æ­¥å‡å°‘è®ºæ–‡æ•°é‡
            while len(temp_data.encode('utf-8')) > 1024 * 1024 and papers:
                papers = papers[:-5]  # æ¯æ¬¡ç§»é™¤5ç¯‡è®ºæ–‡
                temp_data = json.dumps(papers, ensure_ascii=False, indent=2)
            
            if len(papers) < original_count:
                logger.warning(f"ä¸ºæ»¡è¶³1MBé™åˆ¶ï¼Œè®ºæ–‡æ•°é‡ä» {original_count} å‡å°‘åˆ° {len(papers)}")
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
            
            file_size = os.path.getsize(filename)
            logger.info(f"æˆåŠŸä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ° {filename}")
            logger.info(f"æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== arXivè®ºæ–‡æ‘˜è¦æŠ“å–å™¨ ===")
    print("æ­£åœ¨æŠ“å–cs.CLç±»åˆ«çš„æœ€æ–°200ç¯‡è®ºæ–‡...")
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
    scraper = SimpleArxivScraper(category="cs.CL", max_results=200)
    
    # æŸ¥è¯¢è®ºæ–‡
    papers = scraper.query_arxiv_api()
    
    if not papers:
        print("æœªèƒ½è·å–ä»»ä½•è®ºæ–‡æ•°æ®")
        return
    
    print(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
    print("æ­£åœ¨ä½¿ç”¨Trafilaturaå¢å¼ºå†…å®¹...")
    
    # ä½¿ç”¨Trafilaturaå¢å¼ºå†…å®¹
    enhanced_papers = scraper.enhance_with_trafilatura(papers)
    
    # ä¿å­˜ç»“æœ
    success = scraper.save_to_json(enhanced_papers)
    
    if success:
        print(f"âœ… ä»»åŠ¡å®Œæˆï¼å…±å¤„ç† {len(enhanced_papers)} ç¯‡è®ºæ–‡")
        print("ğŸ“ æ•°æ®å·²ä¿å­˜åˆ° arxiv_clean.json")
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        avg_abstract_length = sum(len(p['abstract']) for p in enhanced_papers) / len(enhanced_papers)
        print(f"ğŸ“Š å¹³å‡æ‘˜è¦é•¿åº¦: {avg_abstract_length:.0f} å­—ç¬¦")
        
        # æ˜¾ç¤ºç¬¬ä¸€ç¯‡è®ºæ–‡ä½œä¸ºç¤ºä¾‹
        if enhanced_papers:
            first_paper = enhanced_papers[0]
            print(f"\nğŸ“„ ç¤ºä¾‹è®ºæ–‡:")
            print(f"æ ‡é¢˜: {first_paper['title'][:100]}...")
            print(f"ä½œè€…: {', '.join(first_paper['authors'][:3])}{'...' if len(first_paper['authors']) > 3 else ''}")
            print(f"æ—¥æœŸ: {first_paper['date']}")
            print(f"æ‘˜è¦: {first_paper['abstract'][:200]}...")
    else:
        print("âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥")

if __name__ == "__main__":
    main()
