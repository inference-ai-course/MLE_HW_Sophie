"""
æ•°æ®æå–ä¸é¢„å¤„ç†ç»¼åˆç¤ºä¾‹
æœ¬æ–‡ä»¶æ¼”ç¤ºäº†æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­å¸¸è§çš„å¤šæ¨¡æ€æ•°æ®æå–å’Œé¢„å¤„ç†æŠ€æœ¯ï¼š
1. ç½‘é¡µæ–‡æœ¬æå– - ä»å­¦æœ¯è®ºæ–‡ç½‘ç«™æå–ç»“æ„åŒ–æ–‡æœ¬
2. å›¾åƒOCR - ä½¿ç”¨å¤šç§æ–¹æ³•ä»å›¾åƒä¸­æå–æ–‡å­—
3. éŸ³é¢‘è½¬å½• - å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬
4. æ•°æ®æ¸…ç† - å»é‡ã€å»å™ªã€éšç§ä¿æŠ¤ç­‰é¢„å¤„ç†æ­¥éª¤

é€‚ç”¨åœºæ™¯ï¼šæ„å»ºå¤šæ¨¡æ€æ•°æ®é›†ã€æ–‡æ¡£æ•°å­—åŒ–ã€å†…å®¹åˆ†æç­‰
"""

# =============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šç½‘é¡µæ–‡æœ¬æå–
# =============================================================================
# åŠŸèƒ½ï¼šä»ç½‘é¡µä¸­æå–å¹²å‡€çš„æ–‡æœ¬å†…å®¹ï¼Œå»é™¤HTMLæ ‡ç­¾å’Œæ— å…³å…ƒç´ 
# åº”ç”¨åœºæ™¯ï¼šçˆ¬å–å­¦æœ¯è®ºæ–‡æ‘˜è¦ã€æ–°é—»æ–‡ç« ã€åšå®¢å†…å®¹ç­‰ç”¨äºæ–‡æœ¬åˆ†æ

# âœ… å¦‚éœ€è¦ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–é¡¹
# !pip install trafilatura

import trafilatura
import requests

# ç¤ºä¾‹ï¼šarXivè®ºæ–‡æ‘˜è¦é¡µé¢ - è¿™æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸæœ€é‡è¦çš„é¢„å°æœ¬æœåŠ¡å™¨
url = "https://arxiv.org/abs/2404.00001"

# æ­¥éª¤1ï¼šè·å–åŸå§‹HTML - æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ç½‘é¡µ
response = requests.get(url)
html = response.text

# æ­¥éª¤2ï¼šä½¿ç”¨Trafilaturaæ™ºèƒ½æå–æ–‡æœ¬
# Trafilaturaæ˜¯ä¸“é—¨ç”¨äºç½‘é¡µæ–‡æœ¬æå–çš„åº“ï¼Œèƒ½è‡ªåŠ¨è¯†åˆ«ä¸»è¦å†…å®¹åŒºåŸŸ
# æ’é™¤å¯¼èˆªæ ã€å¹¿å‘Šã€è¯„è®ºç­‰å¹²æ‰°ä¿¡æ¯ï¼Œä¿ç•™æ ¸å¿ƒæ–‡æœ¬å†…å®¹
downloaded_text = trafilatura.extract(html, include_comments=False, include_tables=False)

# æ­¥éª¤3ï¼šæ˜¾ç¤ºæå–ç»“æœ
print("ğŸ“„ æå–çš„æ–‡æœ¬é¢„è§ˆï¼š\n")
print(downloaded_text[:1000])  # æ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦ç”¨äºé¢„è§ˆ



# =============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šä¼ ç»ŸOCRæ–‡å­—è¯†åˆ«
# =============================================================================
# åŠŸèƒ½ï¼šä½¿ç”¨Tesseractå¼•æ“ä»å›¾åƒä¸­è¯†åˆ«æ–‡å­—
# ç‰¹ç‚¹ï¼šå¼€æºå…è´¹ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼Œé€‚åˆå¤„ç†æ‰«ææ–‡æ¡£ã€æˆªå›¾ç­‰
# åº”ç”¨åœºæ™¯ï¼šæ–‡æ¡£æ•°å­—åŒ–ã€è¡¨å•å¤„ç†ã€å›¾ä¹¦æ‰«æç­‰

# å®‰è£…ï¼šsudo apt install tesseract-ocr æˆ– !pip install pytesseract Pillow
import pytesseract
from PIL import Image

# å›¾åƒé¢„å¤„ç†ï¼šè½¬æ¢ä¸ºç°åº¦å›¾å¯ä»¥æé«˜OCRè¯†åˆ«å‡†ç¡®ç‡
# ç°åº¦å›¾å‡å°‘äº†é¢œè‰²å¹²æ‰°ï¼Œè®©æ–‡å­—è½®å»“æ›´æ¸…æ™°
image = Image.open("/Users/pc/Documents/cursor/ml course/MLE_in_Gen_AI-Course/Class3/test_data/image/image.png").convert("L")  # ç°åº¦å›¾
text = pytesseract.image_to_string(image)

print("ğŸ“„ Tesseract OCRè¾“å‡ºï¼ˆå‰500ä¸ªå­—ç¬¦ï¼‰ï¼š")
print(text[:500])





# =============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ OCR - Surya
# =============================================================================
# åŠŸèƒ½ï¼šä½¿ç”¨ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒOCRï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å‡†ç¡®
# ç‰¹ç‚¹ï¼šæä¾›æ–‡å­—ä½ç½®åæ ‡å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼Œæ”¯æŒå¤æ‚å¸ƒå±€è¯†åˆ«
# åº”ç”¨åœºæ™¯ï¼šå¤æ‚æ–‡æ¡£åˆ†æã€ç‰ˆé¢ç†è§£ã€å¤šè¯­è¨€æ··åˆæ–‡æœ¬è¯†åˆ«

from PIL import Image
from surya.detection import DetectionPredictor
from surya.recognition import RecognitionPredictor

# åŠ è½½å¾…å¤„ç†å›¾åƒ
image = Image.open("/Users/pc/Documents/cursor/ml course/MLE_in_Gen_AI-Course/Class3/test_data/image/image.png")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å›¾åƒè·¯å¾„
langs = ["en"]  # æŒ‡å®šè¯†åˆ«è¯­è¨€ï¼Œå¯æ”¯æŒå¤šè¯­è¨€

# åˆå§‹åŒ–æ·±åº¦å­¦ä¹ é¢„æµ‹å™¨
# DetectionPredictorï¼šæ£€æµ‹æ–‡å­—åŒºåŸŸä½ç½®
# RecognitionPredictorï¼šè¯†åˆ«æ–‡å­—å†…å®¹
detection_predictor = DetectionPredictor()
recognition_predictor = RecognitionPredictor()

# æ‰§è¡ŒOCRè¯†åˆ«ï¼Œè·å–å¸¦è¾¹ç•Œæ¡†çš„æ–‡å­—ä¿¡æ¯
predictions = recognition_predictor([image], ["ocr_with_boxes"], detection_predictor)



# è¾“å‡ºè¯¦ç»†è¯†åˆ«ç»“æœï¼ŒåŒ…å«æ–‡å­—å†…å®¹ã€ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡æ¯
# è¿™äº›ä¿¡æ¯å¯¹äºæ–‡æ¡£ç»“æ„åˆ†æå’Œç‰ˆé¢é‡å»ºéå¸¸é‡è¦
for page in predictions:
    for line in page.text_lines:
        print(f"æ–‡æœ¬: {line.text}")
        print(f"ç½®ä¿¡åº¦: {line.confidence}")  # AIæ¨¡å‹å¯¹è¯†åˆ«ç»“æœçš„ç¡®ä¿¡ç¨‹åº¦
        print(f"å¤šè¾¹å½¢: {line.polygon}\n")    # æ–‡å­—åŒºåŸŸçš„ç²¾ç¡®åæ ‡




# =============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šAIè§†è§‰æ¨¡å‹OCR - GPT-4 Vision
# =============================================================================
# åŠŸèƒ½ï¼šåˆ©ç”¨æœ€å…ˆè¿›çš„å¤šæ¨¡æ€AIæ¨¡å‹è¿›è¡Œå›¾åƒæ–‡å­—è¯†åˆ«å’Œç†è§£
# ä¼˜åŠ¿ï¼šèƒ½ç†è§£å›¾åƒä¸Šä¸‹æ–‡ã€å¤„ç†å¤æ‚å¸ƒå±€ã€æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢
# åº”ç”¨åœºæ™¯ï¼šæ™ºèƒ½æ–‡æ¡£ç†è§£ã€è¡¨æ ¼æ•°æ®æå–ã€æ‰‹å†™æ–‡å­—è¯†åˆ«

import base64
import requests

def vision_extract(b64_image, prompt, api_key):
    """
    ä½¿ç”¨GPT-4è§†è§‰æ¨¡å‹ä»å›¾åƒä¸­æå–æ–‡æœ¬
    
    è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼š
    1. ç†è§£å›¾åƒè¯­ä¹‰å’Œä¸Šä¸‹æ–‡
    2. å¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå®šåˆ¶åŒ–æå–
    3. å¤„ç†å¤æ‚å¸ƒå±€å’Œéæ ‡å‡†æ ¼å¼
    4. æ”¯æŒå¤šè¯­è¨€æ··åˆè¯†åˆ«
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    payload = {
        "model": "gpt-4o-mini",          # ä½¿ç”¨è½»é‡çº§ä½†é«˜æ•ˆçš„æ¨¡å‹
        "temperature": 0.0,              # è®¾ä¸º0ç¡®ä¿ç»“æœä¸€è‡´æ€§
        "messages": [
            {"role": "user", "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_image}"}}
            ]}
        ],
        "max_tokens": 3000               # é™åˆ¶å“åº”é•¿åº¦é¿å…è¶…é¢ä½¿ç”¨
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
    return response.json()

# å›¾åƒç¼–ç ï¼šå°†å›¾åƒè½¬æ¢ä¸ºbase64æ ¼å¼ä¾›APIä½¿ç”¨
with open("/Users/pc/Documents/cursor/ml course/MLE_in_Gen_AI-Course/Class3/test_data/image/image.png", "rb") as f:
    b64_img = base64.b64encode(f.read()).decode("utf-8")

# è°ƒç”¨AIè§†è§‰æ¨¡å‹è¿›è¡Œæ–‡å­—æå–
# æ³¨æ„ï¼šAPIå¯†é’¥ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œç¡®ä¿å®‰å…¨æ€§
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')# Use your actual API key here

if not api_key:
    raise ValueError("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")

result = vision_extract(b64_img, "Extract all the readable text from this document.", api_key=api_key)

print(result["choices"][0]["message"]["content"])



# =============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šéŸ³é¢‘è½¬å½• - OpenAI Whisper
# =============================================================================
# åŠŸèƒ½ï¼šå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæ”¯æŒå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«
# ç‰¹ç‚¹ï¼šOpenAIå¼€æºçš„æœ€å…ˆè¿›è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜
# åº”ç”¨åœºæ™¯ï¼šä¼šè®®è®°å½•ã€æ’­å®¢è½¬å½•ã€å¤šåª’ä½“å†…å®¹åˆ†æ

# å®‰è£…ï¼špip install openai-whisper
import whisper

# åŠ è½½Whisperæ¨¡å‹
# æ¨¡å‹å¤§å°é€‰æ‹©ï¼štiny < base < small < medium < large
# æ›´å¤§çš„æ¨¡å‹å‡†ç¡®ç‡æ›´é«˜ä½†å¤„ç†é€Ÿåº¦æ›´æ…¢
model = whisper.load_model("base")  # å¹³è¡¡å‡†ç¡®ç‡å’Œé€Ÿåº¦çš„é€‰æ‹©

# æ‰§è¡ŒéŸ³é¢‘è½¬å½•
# Whisperèƒ½è‡ªåŠ¨æ£€æµ‹è¯­è¨€ã€å¤„ç†å™ªéŸ³ã€è¯†åˆ«å¤šç§å£éŸ³
result = model.transcribe("/Users/pc/Documents/cursor/ml course/practice/week3/test_data/audio/sample-1.mp3")
print("ğŸ“„ Whisperè½¬å½•ç»“æœï¼š")
print(result["text"])




# =============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šé«˜æ•ˆéŸ³é¢‘è½¬å½• - Faster Whisper
# =============================================================================
# åŠŸèƒ½ï¼šWhisperçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«ä¸”å†…å­˜å ç”¨æ›´å°‘
# ç‰¹ç‚¹ï¼šä½¿ç”¨é‡åŒ–æŠ€æœ¯å’Œä¼˜åŒ–æ¨ç†ï¼Œæä¾›æ—¶é—´æˆ³ä¿¡æ¯
# é€‚ç”¨ï¼šå¤§æ‰¹é‡éŸ³é¢‘å¤„ç†ã€å®æ—¶è½¬å½•éœ€æ±‚

from faster_whisper import WhisperModel

# æ¨¡å‹ä¼˜åŒ–é…ç½®ï¼šä½¿ç”¨int8é‡åŒ–å‡å°‘å†…å­˜å ç”¨å¹¶æå‡é€Ÿåº¦
# compute_type="int8" å°†æ¨¡å‹æƒé‡ä»32ä½å‹ç¼©åˆ°8ä½ï¼Œé€Ÿåº¦æå‡4-8å€
model = WhisperModel("base", device="cpu", compute_type="int8")  # é€‚ç”¨äºCPUç¯å¢ƒ

# åˆ†æ®µè½¬å½•ï¼šè¿”å›å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬ç‰‡æ®µ
# è¿™å¯¹äºè§†é¢‘å­—å¹•ç”Ÿæˆã€ä¼šè®®è®°å½•åˆ†æéå¸¸æœ‰ç”¨
segments, _ = model.transcribe("/Users/pc/Documents/cursor/ml course/practice/week3/test_data/audio/sample-1.mp3")

print("ğŸ“„ Faster-Whisperè½¬å½•ç»“æœï¼š")
for segment in segments:
    print(f"[{segment.start:.2f} - {segment.end:.2f}] {segment.text}")



# =============================================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ•°æ®æ¸…ç†ä¸é¢„å¤„ç†
# =============================================================================
# ç›®æ ‡ï¼šå°†åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºé«˜è´¨é‡çš„æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®
# é‡è¦æ€§ï¼šæ•°æ®è´¨é‡ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ï¼Œæ¸…ç†ä¸å½“ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ é”™è¯¯æ¨¡å¼

from datasketch import MinHash, MinHashLSH

def minhash_deduplication(texts, threshold=0.7):
    """
    ä½¿ç”¨MinHash LSHç®—æ³•è¿›è¡Œé«˜æ•ˆæ–‡æœ¬å»é‡
    
    å·¥ä½œåŸç†ï¼š
    1. å°†æ¯ä¸ªæ–‡æ¡£è½¬æ¢ä¸ºMinHashç­¾åï¼ˆé™ç»´è¡¨ç¤ºï¼‰
    2. ä½¿ç”¨LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰å¿«é€Ÿæ‰¾åˆ°ç›¸ä¼¼æ–‡æ¡£
    3. åªä¿ç•™å”¯ä¸€æ–‡æ¡£ï¼Œå»é™¤é‡å¤å’Œè¿‘ä¼¼é‡å¤å†…å®¹
    
    ä¼˜åŠ¿ï¼šå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«æ•°ç™¾å€
    åº”ç”¨ï¼šé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆé‡å¤å†…å®¹ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
    """
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    unique_texts = []
    for i, doc in enumerate(texts):
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”ŸæˆMinHashç­¾å
        m = MinHash(num_perm=128)
        for word in set(doc.split()):
            m.update(word.encode('utf8'))
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸ä¼¼æ–‡æ¡£
        if not lsh.query(m):
            lsh.insert(f"doc{i}", m)
            unique_texts.append(doc)
    return unique_texts



from langdetect import detect
from bs4 import BeautifulSoup

def clean_html_and_filter_lang(texts, lang='en'):
    """
    HTMLæ¸…ç†å’Œè¯­è¨€è¿‡æ»¤åŠŸèƒ½
    
    åŠŸèƒ½è¯¦è§£ï¼š
    1. ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œæå–çº¯æ–‡æœ¬å†…å®¹
    2. ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ã€CSSæ ·å¼ã€JavaScriptä»£ç 
    3. ä½¿ç”¨langdetectåº“æ£€æµ‹æ–‡æœ¬è¯­è¨€
    4. åªä¿ç•™æŒ‡å®šè¯­è¨€çš„æ–‡æœ¬ï¼Œç¡®ä¿æ•°æ®é›†è¯­è¨€ä¸€è‡´æ€§
    
    é‡è¦æ€§ï¼šç½‘ç»œçˆ¬å–çš„æ•°æ®å¾€å¾€åŒ…å«å¤§é‡HTMLå™ªéŸ³å’Œå¤šè¯­è¨€æ··åˆ
    è¿‡æ»¤åçš„æ•°æ®æ›´é€‚åˆè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡
    """
    filtered = []
    for txt in texts:
        # è§£æHTMLå¹¶æå–çº¯æ–‡æœ¬
        txt = BeautifulSoup(txt, 'html.parser').get_text()
        try:
            # æ£€æµ‹è¯­è¨€å¹¶è¿‡æ»¤
            if detect(txt.strip()) == lang:
                filtered.append(txt.strip())
        except:
            # è¯­è¨€æ£€æµ‹å¤±è´¥çš„æ–‡æœ¬è·³è¿‡ï¼ˆé€šå¸¸æ˜¯å†…å®¹å¤ªçŸ­æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰
            continue
    return filtered



import re

def strip_pii(text):
    """
    ç§»é™¤ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ä¿æŠ¤éšç§
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - è‡ªåŠ¨è¯†åˆ«å¹¶æ›¿æ¢æ•æ„Ÿä¿¡æ¯ï¼Œé˜²æ­¢éšç§æ³„éœ²
    - ä½¿ç”¨å ä½ç¬¦æ›¿æ¢è€Œéåˆ é™¤ï¼Œä¿æŒæ–‡æœ¬ç»“æ„å®Œæ•´
    - è¿™æ˜¯æ•°æ®å¤„ç†çš„é‡è¦åˆè§„è¦æ±‚
    
    å¤„ç†çš„ä¿¡æ¯ç±»å‹ï¼š
    1. é‚®ç®±åœ°å€ â†’ [EMAIL]
    2. ä¿¡ç”¨å¡å· â†’ [CREDIT_CARD]  
    3. ç”µè¯å·ç  â†’ [PHONE]
    
    æ‰©å±•å»ºè®®ï¼šå¯æ·»åŠ èº«ä»½è¯å·ã€åœ°å€ã€å§“åç­‰å…¶ä»–æ•æ„Ÿä¿¡æ¯çš„è¯†åˆ«
    """
    text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)  # åŒ¹é…é‚®ç®±æ ¼å¼
    text = re.sub(r'\b\d{12,19}\b', '[CREDIT_CARD]', text)  # åŒ¹é…12-19ä½æ•°å­—ï¼ˆä¿¡ç”¨å¡å·é•¿åº¦ï¼‰
    text = re.sub(r'\b(?:\d{3}-){2}\d{4}\b', '[PHONE]', text)  # åŒ¹é…XXX-XXX-XXXXæ ¼å¼ç”µè¯
    return text



import re
from collections import Counter

def remove_repetitive_ngrams(text, n=3, threshold=3):
    """
    ç§»é™¤é‡å¤çš„n-gramçŸ­è¯­ï¼Œæé«˜æ–‡æœ¬è´¨é‡
    
    èƒŒæ™¯é—®é¢˜ï¼š
    - ç½‘ç»œçˆ¬å–çš„æ–‡æœ¬å¸¸åŒ…å«é‡å¤çš„å¹¿å‘Šè¯­ã€å…è´£å£°æ˜ç­‰
    - é‡å¤å†…å®¹ä¼šå½±å“æ¨¡å‹å­¦ä¹ ï¼Œå¯¼è‡´ç”Ÿæˆé‡å¤æ–‡æœ¬
    - éœ€è¦æ™ºèƒ½è¯†åˆ«å¹¶æ¸…ç†è¿™äº›é‡å¤æ¨¡å¼
    
    ç®—æ³•æ­¥éª¤ï¼š
    1. å°†æ–‡æœ¬åˆ†å‰²ä¸ºn-gramï¼ˆé»˜è®¤3ä¸ªè¯çš„ç»„åˆï¼‰
    2. ç»Ÿè®¡æ¯ä¸ªn-gramçš„å‡ºç°é¢‘æ¬¡
    3. è¯†åˆ«é«˜é¢‘é‡å¤çš„çŸ­è¯­ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰
    4. å°†è¿ç»­é‡å¤çš„çŸ­è¯­å‹ç¼©ä¸ºå•æ¬¡å‡ºç°
    
    å‚æ•°è°ƒèŠ‚ï¼š
    - n=3: æ£€æµ‹3ä¸ªè¯çš„ç»„åˆï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
    - threshold=3: å‡ºç°3æ¬¡ä»¥ä¸Šè®¤ä¸ºæ˜¯é‡å¤ï¼Œå¯è°ƒæ•´æ•æ„Ÿåº¦
    """
    words = text.split()
    # ç”Ÿæˆæ‰€æœ‰n-gramç»„åˆ
    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]

    # ç»Ÿè®¡n-gramé¢‘æ¬¡ï¼Œè¯†åˆ«é‡å¤æ¨¡å¼
    counts = Counter(ngrams)
    repetitive = [ngram for ngram, count in counts.items() if count >= threshold]

    # æ¸…ç†é‡å¤çŸ­è¯­
    for phrase in repetitive:
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ä»¥å®‰å…¨ç”¨äºæ­£åˆ™è¡¨è¾¾å¼
        escaped_phrase = re.escape(phrase)
        # å°†è¿ç»­é‡å¤çš„çŸ­è¯­æ›¿æ¢ä¸ºå•æ¬¡å‡ºç°
        text = re.sub(rf'(?:{escaped_phrase}\s*){{{threshold},}}', phrase + ' ', text)

    # æ ‡å‡†åŒ–ç©ºæ ¼
    text = re.sub(r'\s{2,}', ' ', text).strip()
    return text




# =============================================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šå®Œæ•´æ•°æ®æ¸…ç†æµæ°´çº¿æ¼”ç¤º
# =============================================================================
# ç›®æ ‡ï¼šå±•ç¤ºå¦‚ä½•å°†åŸå§‹ã€æ··ä¹±çš„æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºé«˜è´¨é‡è®­ç»ƒæ•°æ®
# è¿™æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœ€é‡è¦ä½†ç»å¸¸è¢«å¿½è§†çš„æ­¥éª¤

import pandas as pd
# åŠ è½½ç¤ºä¾‹æ•°æ®é›†ï¼šæ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­çš„åŸå§‹æ–‡æœ¬æ•°æ®
# è¿™äº›æ•°æ®é€šå¸¸åŒ…å«HTMLæ ‡ç­¾ã€é‡å¤å†…å®¹ã€å¤šè¯­è¨€æ··åˆã€éšç§ä¿¡æ¯ç­‰é—®é¢˜
fake_texts = pd.read_csv("/Users/pc/Documents/cursor/ml course/practice/week3/test_data/data/Fake_Pretraining_Texts.csv")
raw_dataset = fake_texts["Raw Text"]
print("åŸå§‹æ•°æ®é¢„è§ˆ:")
print(raw_dataset)

from IPython.display import display

# æ•°æ®æ¸…ç†æµæ°´çº¿ï¼šæ¯ä¸€æ­¥éƒ½è§£å†³ç‰¹å®šçš„æ•°æ®è´¨é‡é—®é¢˜
print("\n=== å¼€å§‹æ•°æ®æ¸…ç†æµæ°´çº¿ ===")

# æ­¥éª¤1ï¼šHTMLæ¸…ç† + è¯­è¨€è¿‡æ»¤
# ç›®çš„ï¼šå»é™¤ç½‘é¡µæ ‡ç­¾å™ªéŸ³ï¼Œç¡®ä¿è¯­è¨€ä¸€è‡´æ€§
print("\næ­¥éª¤1: HTMLæ¸…ç†å’Œè¯­è¨€è¿‡æ»¤")
step1 = clean_html_and_filter_lang(raw_dataset)
display(step1)

# æ­¥éª¤2ï¼šæ™ºèƒ½å»é‡
# ç›®çš„ï¼šç§»é™¤é‡å¤å’Œç›¸ä¼¼å†…å®¹ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆ
print("\næ­¥éª¤2: æ™ºèƒ½å»é‡å¤„ç†")
step2 = minhash_deduplication(step1)
display(step2)

# æ­¥éª¤3ï¼šéšç§ä¿æŠ¤
# ç›®çš„ï¼šç§»é™¤æ•æ„Ÿä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®ä½¿ç”¨åˆè§„
print("\næ­¥éª¤3: éšç§ä¿¡æ¯ä¿æŠ¤")
step3 = [strip_pii(t) for t in step2]
display(step3)

# æ­¥éª¤4ï¼šå»é™¤é‡å¤æ¨¡å¼
# ç›®çš„ï¼šæ¸…ç†é‡å¤çš„å¹¿å‘Šè¯­ã€æ¨¡æ¿æ–‡æœ¬ç­‰å™ªéŸ³
print("\næ­¥éª¤4: é‡å¤æ¨¡å¼æ¸…ç†")
cleaned_data = [remove_repetitive_ngrams(t) for t in step3]
display(cleaned_data)

# æµæ°´çº¿å®Œæˆï¼šå±•ç¤ºæœ€ç»ˆæ¸…ç†ç»“æœ
print("\n" + "="*50)
print("âœ… æ•°æ®æ¸…ç†æµæ°´çº¿å®Œæˆï¼")
print("æ¸…ç†åçš„é«˜è´¨é‡æ•°æ®é›†ï¼š")
for idx, text in enumerate(cleaned_data):
    print(f"\n--- æ–‡ç«  {idx + 1} ---")
    print(text)

print(f"\næ•°æ®ç»Ÿè®¡:")
print(f"åŸå§‹æ•°æ®é‡: {len(raw_dataset)}")
print(f"æ¸…ç†åæ•°æ®é‡: {len(cleaned_data)}")
print(f"æ•°æ®ä¿ç•™ç‡: {len(cleaned_data)/len(raw_dataset)*100:.1f}%")
